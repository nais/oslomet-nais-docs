{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00a7 A top level overview of NAIS describing the larger moving parts and concepts. \u00a7 Warning: Readers may experience both tounge & cheek in this document (5 minute read) (or you can fast forward to the nuts & bolts) \u23e9 Why NAIS exists \u00a7 When you have a large development organisation, providing the developers with turn-key solutions for their most common needs is a good investment. \"Swiss Army Knife\" \u00a7 At the core of NAIS lies Kubernetes , which can be described as a Swiss army knife of tools. Each of these tools comes with its own set of instructions and in sum this adds up and makes for a bit of a mess in terms of choices and considerations. Our goal is to lift this burden from our developers, and in order to do so we've condensed the essential parts into a single configuration file that we use to generate all the underlying Kubernetes resources. In nais terms this file is most commonly referred to as nais.yaml The services included are (but not limited to) logging, metrics, alerts, deployment, operators and a runtime environment (across on-premise and Google Cloud Platform). To make this all happen we leverage open source projects best suited to our needs and provide them with usable abstractions, sane defaults and the required security hardening. Clusters \u00a7 In ye olde days of yore, NAVs security model was based on network segmentation and zones. (And if we are being honest this still holds true for parts of our operation). Our goal (and strategy) however is to move everything to \"Public Cloud\". So anything described as on-prem is scheduled to be taken out behind the barn at some point and new apps, services or products are destined to exist in GCP (for now). FSS \u00a7 Many of our older systems, and systems with a high degree of sensitive content were placed in a dedicated zone with very strict restrictions: Fagsystemsone (FSS). Connectivity to and from this zone is very limited - no connectivity to the internet, inbound connections have to go through security gateways. However - connectivity inside this zone is not restricted in any way, shape or form. Everyone can connect with everyone else - like a true hippie community. (As many of these applications were written in the seventies, it sort of makes sense) SBS \u00a7 There came a day when NAV discovered this thing called \"the Internet\", and that this was something we could use to provide Norwegian citizens with services directly. This created the demand for a separate zone, as allowing traffic from the internet directly in to our hippie commune called FSS seemed unwise. Thus Selvbetjeningssonen (SBS) was born. SBS is less restricted than FSS and applications have access to the internet (almost) - and can be exposed to the internet as well. However - since most of NAVs data reside in FSS, most applications in SBS rely on data from FSS to be able to do anything meaningful. In order to get the data they need they have to jump through several burning rings of fire, pray to the almighty DataPower-gods and perform several ancient rites and rituals. NAIS on-premises \u00a7 When we started building NAIS, we built it to exist in this world, and have separate clusters in each of these two zones. We further divided our clusters in to development and production clusters to maintain a healthy separation. Thus the four clusters we've got on-premises are: dev-fss , dev-sbs , prod-fss and prod-sbs (and a fifth called nais-ci, but that's just for us to test stuff) NAIS GCP \u00a7 Luckily the world has moved on from zones and segmentation. When we built NAIS in GCP we wanted the applications to be able to communicate without jumping over hurdles and figured we'd adopt a zero-trust-model , where applications specify with whom they want to communicate, and who is allowed to communicate with them in their application manifest. And so the two GCP clusters dev-gcp and prod-gcp were born. There are a couple of additional clusters in GCP as well, though - ci-gcp for us to test changes and labs-gcp where the teams can experiment. Teams \u00a7 NAIS is for the development teams. We believe that a team should be able to take full responsibility for what they build - in the entirety of its lifecycle. This creates better software and happier teams. To achieve this, we ensure that each team has their own space where they can experiment, develop and host the stuff they build. Here they have the full set of permissions, and are only limited by their own imagination. Details about the technology to logically group users, their access and permissions is described here . Access from laptop \u00a7 In the extended NAIS universe we also have a component called naisdevice. This is a cross plattform mechanism that provices access to NAIS services. The product adheres to the same principles as are decribed here and takes aim at securing our operations without getting in our devs way. Contact the NAIS team \u00a7 The team can be found on Slack . Also, follow us on Twitter @nais_io !","title":"Introduction to NAIS"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#a-top-level-overview-of-nais-describing-the-larger-moving-parts-and-concepts","text":"Warning: Readers may experience both tounge & cheek in this document (5 minute read) (or you can fast forward to the nuts & bolts) \u23e9","title":"A top level overview of NAIS describing the larger moving parts and concepts."},{"location":"#why-nais-exists","text":"When you have a large development organisation, providing the developers with turn-key solutions for their most common needs is a good investment.","title":"Why NAIS exists"},{"location":"#swiss-army-knife","text":"At the core of NAIS lies Kubernetes , which can be described as a Swiss army knife of tools. Each of these tools comes with its own set of instructions and in sum this adds up and makes for a bit of a mess in terms of choices and considerations. Our goal is to lift this burden from our developers, and in order to do so we've condensed the essential parts into a single configuration file that we use to generate all the underlying Kubernetes resources. In nais terms this file is most commonly referred to as nais.yaml The services included are (but not limited to) logging, metrics, alerts, deployment, operators and a runtime environment (across on-premise and Google Cloud Platform). To make this all happen we leverage open source projects best suited to our needs and provide them with usable abstractions, sane defaults and the required security hardening.","title":"\"Swiss Army Knife\""},{"location":"#clusters","text":"In ye olde days of yore, NAVs security model was based on network segmentation and zones. (And if we are being honest this still holds true for parts of our operation). Our goal (and strategy) however is to move everything to \"Public Cloud\". So anything described as on-prem is scheduled to be taken out behind the barn at some point and new apps, services or products are destined to exist in GCP (for now).","title":"Clusters"},{"location":"#fss","text":"Many of our older systems, and systems with a high degree of sensitive content were placed in a dedicated zone with very strict restrictions: Fagsystemsone (FSS). Connectivity to and from this zone is very limited - no connectivity to the internet, inbound connections have to go through security gateways. However - connectivity inside this zone is not restricted in any way, shape or form. Everyone can connect with everyone else - like a true hippie community. (As many of these applications were written in the seventies, it sort of makes sense)","title":"FSS"},{"location":"#sbs","text":"There came a day when NAV discovered this thing called \"the Internet\", and that this was something we could use to provide Norwegian citizens with services directly. This created the demand for a separate zone, as allowing traffic from the internet directly in to our hippie commune called FSS seemed unwise. Thus Selvbetjeningssonen (SBS) was born. SBS is less restricted than FSS and applications have access to the internet (almost) - and can be exposed to the internet as well. However - since most of NAVs data reside in FSS, most applications in SBS rely on data from FSS to be able to do anything meaningful. In order to get the data they need they have to jump through several burning rings of fire, pray to the almighty DataPower-gods and perform several ancient rites and rituals.","title":"SBS"},{"location":"#nais-on-premises","text":"When we started building NAIS, we built it to exist in this world, and have separate clusters in each of these two zones. We further divided our clusters in to development and production clusters to maintain a healthy separation. Thus the four clusters we've got on-premises are: dev-fss , dev-sbs , prod-fss and prod-sbs (and a fifth called nais-ci, but that's just for us to test stuff)","title":"NAIS on-premises"},{"location":"#nais-gcp","text":"Luckily the world has moved on from zones and segmentation. When we built NAIS in GCP we wanted the applications to be able to communicate without jumping over hurdles and figured we'd adopt a zero-trust-model , where applications specify with whom they want to communicate, and who is allowed to communicate with them in their application manifest. And so the two GCP clusters dev-gcp and prod-gcp were born. There are a couple of additional clusters in GCP as well, though - ci-gcp for us to test changes and labs-gcp where the teams can experiment.","title":"NAIS GCP"},{"location":"#teams","text":"NAIS is for the development teams. We believe that a team should be able to take full responsibility for what they build - in the entirety of its lifecycle. This creates better software and happier teams. To achieve this, we ensure that each team has their own space where they can experiment, develop and host the stuff they build. Here they have the full set of permissions, and are only limited by their own imagination. Details about the technology to logically group users, their access and permissions is described here .","title":"Teams"},{"location":"#access-from-laptop","text":"In the extended NAIS universe we also have a component called naisdevice. This is a cross plattform mechanism that provices access to NAIS services. The product adheres to the same principles as are decribed here and takes aim at securing our operations without getting in our devs way.","title":"Access from laptop"},{"location":"#contact-the-nais-team","text":"The team can be found on Slack . Also, follow us on Twitter @nais_io !","title":"Contact the NAIS team"},{"location":"support/","text":"Contact the NAIS team \u00a7 The team can be found on Slack . On-call services \u00a7 NAIS has a 24/7 on-call service for operation and troubleshooting of the on-premises and GCP platforms, as well as third-party suppliers Aiven. During regular working hours, you can mention @nais-team on Slack. The on-call service is to be used for production environment events outside regular working hours only. The on-call service is rotated, the person on-call can be found in the Slack user group \"nais-vakt\". The on-call phone number can be found in the header of the NAIS Slack . Response time for a request should be no more than 30 minutes for Slack mentions (@nais-vakt). Would your team like to work more closely with NAIS? \u00a7 If your team would like closer follow-up for a period (or permanently), we would happily dedicate someone from NAIS as your NAM(s) - short for \"NAIS Account Managers\". This means we will create a dedicated Slack channel for NAIS/team-communication and have meetings if required to discuss your concerns, issues, or wishes for the platform.","title":"Support"},{"location":"support/#contact-the-nais-team","text":"The team can be found on Slack .","title":"Contact the NAIS team"},{"location":"support/#on-call-services","text":"NAIS has a 24/7 on-call service for operation and troubleshooting of the on-premises and GCP platforms, as well as third-party suppliers Aiven. During regular working hours, you can mention @nais-team on Slack. The on-call service is to be used for production environment events outside regular working hours only. The on-call service is rotated, the person on-call can be found in the Slack user group \"nais-vakt\". The on-call phone number can be found in the header of the NAIS Slack . Response time for a request should be no more than 30 minutes for Slack mentions (@nais-vakt).","title":"On-call services"},{"location":"support/#would-your-team-like-to-work-more-closely-with-nais","text":"If your team would like closer follow-up for a period (or permanently), we would happily dedicate someone from NAIS as your NAM(s) - short for \"NAIS Account Managers\". This means we will create a dedicated Slack channel for NAIS/team-communication and have meetings if required to discuss your concerns, issues, or wishes for the platform.","title":"Would your team like to work more closely with NAIS?"},{"location":"addons/leader-election/","text":"Leader Election \u00a7 With leader election it is possible to have one responsible pod. This can be used to control that only one pod runs a batch-job or similar tasks. This is done by asking the elector container which pod is the current leader, and comparing that to the pod's hostname. The leader election configuration does not control which pod the external service requests will be routed to. Enable leader election \u00a7 Enabling leader election in a pod is done by adding the line leaderElection: true to your nais.yaml -file. With that setting enabled, NAIS will sidecar an elector container into your pod. When you have the elector container running in your pod, you can make a HTTP GET to the URL set in environment variable $ELECTOR_PATH to see which pod is the leader. This will return a JSON object with the name of the leader, which you can now compare with your hostname. Issues \u00a7 We changed leader election implementation in January 2022. The old implementation had some issues, which we have attempted to address. In doing so, we have made other trade-offs, which results in these issues: A leader is elected for life. When a leader is elected, it will continue to be the sole leader until the pod is deleted from the cluster. This remains the case even if the leader crashes, hangs, or is otherwise unable to make progress. The only way for a leader to lose leadership is to be deleted from the cluster. We recommend that you create alerts to detect when your leader is not doing its job, so that you can intervene. Participants in an election need to poll the $ELECTOR_PATH to be informed of changes in leadership. There is no push mechanism in place to inform your application about changes. This means your application needs to check at reasonable intervals for changes in leadership. (This was also the case in the old implementation). Examples \u00a7 Java example \u00a7 // Implementation of getJSONFromUrl is left as an exercise for the reader class Leader { public static boolean isLeader () { String electorPath = System . getenv ( \"ELECTOR_PATH\" ); JSONObject leaderJson = getJSONFromUrl ( electorPath ); String leader = leaderJson . getString ( \"name\" ); String hostname = InetAddress . getLocalHost (). getHostname (); return hostname . equals ( leader ); } } cURL example \u00a7 $ kubectl exec -it elector-sidecar-755b7c5795-7k2qn -c debug bash root@elector-sidecar-755b7c5795-7k2qn:/# curl $ELECTOR_PATH { \"name\" : \"elector-sidecar-755b7c5795-2kcm7\" }","title":"Leader Election"},{"location":"addons/leader-election/#leader-election","text":"With leader election it is possible to have one responsible pod. This can be used to control that only one pod runs a batch-job or similar tasks. This is done by asking the elector container which pod is the current leader, and comparing that to the pod's hostname. The leader election configuration does not control which pod the external service requests will be routed to.","title":"Leader Election"},{"location":"addons/leader-election/#enable-leader-election","text":"Enabling leader election in a pod is done by adding the line leaderElection: true to your nais.yaml -file. With that setting enabled, NAIS will sidecar an elector container into your pod. When you have the elector container running in your pod, you can make a HTTP GET to the URL set in environment variable $ELECTOR_PATH to see which pod is the leader. This will return a JSON object with the name of the leader, which you can now compare with your hostname.","title":"Enable leader election"},{"location":"addons/leader-election/#issues","text":"We changed leader election implementation in January 2022. The old implementation had some issues, which we have attempted to address. In doing so, we have made other trade-offs, which results in these issues: A leader is elected for life. When a leader is elected, it will continue to be the sole leader until the pod is deleted from the cluster. This remains the case even if the leader crashes, hangs, or is otherwise unable to make progress. The only way for a leader to lose leadership is to be deleted from the cluster. We recommend that you create alerts to detect when your leader is not doing its job, so that you can intervene. Participants in an election need to poll the $ELECTOR_PATH to be informed of changes in leadership. There is no push mechanism in place to inform your application about changes. This means your application needs to check at reasonable intervals for changes in leadership. (This was also the case in the old implementation).","title":"Issues"},{"location":"addons/leader-election/#examples","text":"","title":"Examples"},{"location":"addons/leader-election/#java-example","text":"// Implementation of getJSONFromUrl is left as an exercise for the reader class Leader { public static boolean isLeader () { String electorPath = System . getenv ( \"ELECTOR_PATH\" ); JSONObject leaderJson = getJSONFromUrl ( electorPath ); String leader = leaderJson . getString ( \"name\" ); String hostname = InetAddress . getLocalHost (). getHostname (); return hostname . equals ( leader ); } }","title":"Java example"},{"location":"addons/leader-election/#curl-example","text":"$ kubectl exec -it elector-sidecar-755b7c5795-7k2qn -c debug bash root@elector-sidecar-755b7c5795-7k2qn:/# curl $ELECTOR_PATH { \"name\" : \"elector-sidecar-755b7c5795-2kcm7\" }","title":"cURL example"},{"location":"addons/snorlax/","text":"Snorlax \u00a7 Snorlax helps reduce resource usage by tracking requests on ingresses and scaling apps up/down based on timeouts and requests. It will scale up apps when they've got 0 replicas and receives traffic, and will set replicas to 0 after a given idle time. Enable Snorlax for you application \u00a7 Add the following annotations to your nais.yaml : snorlax.nais.io/enabled: \"true\" A more complete example: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : snorlax.nais.io/enabled : \"true\" name : myapplication namespace : myteam spec : ...","title":"Snorlax"},{"location":"addons/snorlax/#snorlax","text":"Snorlax helps reduce resource usage by tracking requests on ingresses and scaling apps up/down based on timeouts and requests. It will scale up apps when they've got 0 replicas and receives traffic, and will set replicas to 0 after a given idle time.","title":"Snorlax"},{"location":"addons/snorlax/#enable-snorlax-for-you-application","text":"Add the following annotations to your nais.yaml : snorlax.nais.io/enabled: \"true\" A more complete example: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : snorlax.nais.io/enabled : \"true\" name : myapplication namespace : myteam spec : ...","title":"Enable Snorlax for you application"},{"location":"addons/unleash/","text":"Unleash \u00a7 Unleash is a feature toggle system, that gives you a great overview of all feature toggles across all your applications and services. It comes with official client implementations for Java, Node.js, Go, Ruby, Python and .Net. We use Unleash for feature toggling in NAIS and NAV. Get started at unleash.nais.io , and you can look at tips for your feature toggles available here . We've only got one supported environment - but if you want to test it out you can use (https://)unleash.dev.intern.nav.no . NAV Recommended best practices \u00a7 Notice The below tips are just a simplified digest of tips listed on their official page . Ensure that; Defaults are set Your app doesn't die/freak out if it doesn't reach NAV IT's unleash server Feature toggles have expiration date (and are short-lived) If you don't want to remove the \" if feature -> toggle \" code/functionality from the feature toggled in your application, consider adding it as an configuration setting in your nais.yaml instead. Metrics \u00a7 Unleash supports metrics of usage of features. These can either be viewed on the web page, or in Grafana . Support \u00a7 Is the Unleash server down or unavailable? Ask for help in #unleash ! If you're wondering about best practices #unleash is a good place to ask where volunteers will help as best as they can! PS: You might be able to find the answers you seek by checking out NAV Recommended best practices and it's linked source!","title":"Unleash (feature toggling)"},{"location":"addons/unleash/#unleash","text":"Unleash is a feature toggle system, that gives you a great overview of all feature toggles across all your applications and services. It comes with official client implementations for Java, Node.js, Go, Ruby, Python and .Net. We use Unleash for feature toggling in NAIS and NAV. Get started at unleash.nais.io , and you can look at tips for your feature toggles available here . We've only got one supported environment - but if you want to test it out you can use (https://)unleash.dev.intern.nav.no .","title":"Unleash"},{"location":"addons/unleash/#nav-recommended-best-practices","text":"Notice The below tips are just a simplified digest of tips listed on their official page . Ensure that; Defaults are set Your app doesn't die/freak out if it doesn't reach NAV IT's unleash server Feature toggles have expiration date (and are short-lived) If you don't want to remove the \" if feature -> toggle \" code/functionality from the feature toggled in your application, consider adding it as an configuration setting in your nais.yaml instead.","title":"NAV Recommended best practices"},{"location":"addons/unleash/#metrics","text":"Unleash supports metrics of usage of features. These can either be viewed on the web page, or in Grafana .","title":"Metrics"},{"location":"addons/unleash/#support","text":"Is the Unleash server down or unavailable? Ask for help in #unleash ! If you're wondering about best practices #unleash is a good place to ask where volunteers will help as best as they can! PS: You might be able to find the answers you seek by checking out NAV Recommended best practices and it's linked source!","title":"Support"},{"location":"addons/velero-backup-and-restore/","text":"Velero backup and restore \u00a7 This feature is installed on all on-prem and GCP clusters. Velero is a product that includes features for backing up and restoring resources in kubernetes clusters. The backups are run every second hour on all clusters by the Velero schedule. Velero saves backups to gcp buckets and supports restores of specific resources or namespaces. All namespaces are backed up. The solution also creates snapshots of the persistent volumes in kubernetes. See also Velero documentation If you need something restored or have any questions about velero please contact the nais team on the nais slack channel or contact @Sten.Ivar.R\u00f8kke for more information. Metrics \u00a7 General velero status matrics dashboard is available in grafana: Grafana Velero stats","title":"Velero backup and restore"},{"location":"addons/velero-backup-and-restore/#velero-backup-and-restore","text":"This feature is installed on all on-prem and GCP clusters. Velero is a product that includes features for backing up and restoring resources in kubernetes clusters. The backups are run every second hour on all clusters by the Velero schedule. Velero saves backups to gcp buckets and supports restores of specific resources or namespaces. All namespaces are backed up. The solution also creates snapshots of the persistent volumes in kubernetes. See also Velero documentation If you need something restored or have any questions about velero please contact the nais team on the nais slack channel or contact @Sten.Ivar.R\u00f8kke for more information.","title":"Velero backup and restore"},{"location":"addons/velero-backup-and-restore/#metrics","text":"General velero status matrics dashboard is available in grafana: Grafana Velero stats","title":"Metrics"},{"location":"appendix/ingress-dns/","text":"Ingress DNS setup \u00a7 For most domains we point a wildcard DNS record to a loadbalancer in front of our ingress-controller. Example: *.dev-gcp.nais.io -> 35.201.69.142 This means that all entries under this domain, e.g. foo.dev-gcp.nais.io, will resolve to this address automatically. Shared domains \u00a7 When domains are shared between clusters, such as dev.nav.no which exists both in dev-gcp and dev-sbs, we have a wildcard entry pointing to dev-gcp. In addition, to be able to use the same domain in dev-sbs, we have to create explicit records in DNS to be able to direct the traffic to another cluster. These records are automatically created based on the ingresses defined in the cluster. Warning You cannot have a application using the same ingress in both clusters, as the explicit record created will always override the wildcard ingress.","title":"Ingress DNS setup"},{"location":"appendix/ingress-dns/#ingress-dns-setup","text":"For most domains we point a wildcard DNS record to a loadbalancer in front of our ingress-controller. Example: *.dev-gcp.nais.io -> 35.201.69.142 This means that all entries under this domain, e.g. foo.dev-gcp.nais.io, will resolve to this address automatically.","title":"Ingress DNS setup"},{"location":"appendix/ingress-dns/#shared-domains","text":"When domains are shared between clusters, such as dev.nav.no which exists both in dev-gcp and dev-sbs, we have a wildcard entry pointing to dev-gcp. In addition, to be able to use the same domain in dev-sbs, we have to create explicit records in DNS to be able to direct the traffic to another cluster. These records are automatically created based on the ingresses defined in the cluster. Warning You cannot have a application using the same ingress in both clusters, as the explicit record created will always override the wildcard ingress.","title":"Shared domains"},{"location":"appendix/json-schema/","text":"Validation and autocompletion in editors \u00a7 We expose two JSON schemas intended for use with editors to help the developer experience. These can be used for validation, autocompletion and documentation in supported editors. Available schemas: The following is for all nais and default kubernetes resources available. https://storage.googleapis.com/nais-json-schema-2c91/nais-k8s-all.json The following is only for nais resources. https://storage.googleapis.com/nais-json-schema-2c91/nais-all.json VSCode, VSCodium and other VSCode flavours \u00a7 Install the YAML extension from Visual Studio Marketplace. Install for non-offical distributions Visit the marketplace and find \"Download Extension\" in the right-hand menu, under \"Resources\". Then in your editor, open the Extensions page and click the ... in the top right of the sidebar, then \"Install from VSIX\". Alternatively, CTRL/CMD+Shift+P and search for VSIX . Configure \u00a7 Open settings.json by pressing CTRL/CMD+, and search for Preferences: Open Settings(JSON) . Within the root object, add the following: \"yaml.schemas\" : { \"https://storage.googleapis.com/nais-json-schema-2c91/nais-k8s-all.json\" : [ \"nais.yaml\" , \"nais/*\" , \".nais/*\" ], }, It will enable the nais-k8s-all.json schema for all yaml files with the name nais.yaml , or in the nais or .nais directory. See the extension documentations for more ways to associate schemas. IntelliJ \u00a7 See the documentation at Jetbrains . Unfortunately, you will have to set this up individually per project. Known limitations \u00a7 Templating \u00a7 One of the limitations is that the templating language used by e.g. nais-deploy isn't valid YAML. In documents with limited templating, e.g. just having and {{image}} , wrapping it in quotes is usually enough when the value is a string. For other types, there's currently no workaround. So instead of having: spec : image : {{ image }} Try: spec : image : \"{{image}}\"","title":"Validation and autocompletion"},{"location":"appendix/json-schema/#validation-and-autocompletion-in-editors","text":"We expose two JSON schemas intended for use with editors to help the developer experience. These can be used for validation, autocompletion and documentation in supported editors. Available schemas: The following is for all nais and default kubernetes resources available. https://storage.googleapis.com/nais-json-schema-2c91/nais-k8s-all.json The following is only for nais resources. https://storage.googleapis.com/nais-json-schema-2c91/nais-all.json","title":"Validation and autocompletion in editors"},{"location":"appendix/json-schema/#vscode-vscodium-and-other-vscode-flavours","text":"Install the YAML extension from Visual Studio Marketplace. Install for non-offical distributions Visit the marketplace and find \"Download Extension\" in the right-hand menu, under \"Resources\". Then in your editor, open the Extensions page and click the ... in the top right of the sidebar, then \"Install from VSIX\". Alternatively, CTRL/CMD+Shift+P and search for VSIX .","title":"VSCode, VSCodium and other VSCode flavours"},{"location":"appendix/json-schema/#configure","text":"Open settings.json by pressing CTRL/CMD+, and search for Preferences: Open Settings(JSON) . Within the root object, add the following: \"yaml.schemas\" : { \"https://storage.googleapis.com/nais-json-schema-2c91/nais-k8s-all.json\" : [ \"nais.yaml\" , \"nais/*\" , \".nais/*\" ], }, It will enable the nais-k8s-all.json schema for all yaml files with the name nais.yaml , or in the nais or .nais directory. See the extension documentations for more ways to associate schemas.","title":"Configure"},{"location":"appendix/json-schema/#intellij","text":"See the documentation at Jetbrains . Unfortunately, you will have to set this up individually per project.","title":"IntelliJ"},{"location":"appendix/json-schema/#known-limitations","text":"","title":"Known limitations"},{"location":"appendix/json-schema/#templating","text":"One of the limitations is that the templating language used by e.g. nais-deploy isn't valid YAML. In documents with limited templating, e.g. just having and {{image}} , wrapping it in quotes is usually enough when the value is a string. For other types, there's currently no workaround. So instead of having: spec : image : {{ image }} Try: spec : image : \"{{image}}\"","title":"Templating"},{"location":"appendix/zero-trust/","text":"Zero-Trust Network Architecture \u00a7 Firewalls and zones have been our primary defense mechanism for years. With this model we have defined a perimeter around our applications; keeping potential attackers on the inside, and to have control of what our applications are able to communicate with on the outside. The challenge with this model in a containerized world is that our application portfolio has become more distributed, which leaves us with more components and attack vectors. Additionally, the attack methods have become more sophisticated. Our safety planning and solutions must be able to address the following: What happens if an attacker is able to breach our perimeter? Since our application's architecture is based primarily on an outer defense layer, it would be a relatively simple task for an attacker that is already on the inside to compromise other applications within the same perimeter. Most applications have implemented further safety mechanisms, but those who rely solely on the safety perimeter are extremely vulnerable. This problem is addressed using network segmentation; applications with the same safety level and affiliation are grouped together behind separate firewalls. The challenge remains the same, though; a compromised application could mean a compromised zone. The next level of security using this methodology is micro-segmentation and a zone model where applications and services are grouped in even smaller and more specific zones, givig a potential attacker an even smaller attack surface given a successful attack. Continuing this methodology, the inevitable conclusion will be a perimeter around each individual application. Once each application has its own perimeter, the next thing to address is: What if the network itself is compromised? Are there attackers on the inside that can listen to, or spoof traffic? This is the case on unsafe networks, like the Internet, but here other safety mechanisms are being used ensuring that sensitive information like bank and health data can be transferred. It is no longer a safe assumption that there are no attackers in our own data centers, our private cloud or in the public cloud, so we have to implement mechanisms to secure the communication between our applications even here. We need to base our transportation security on authentication and authorization between all services, so that we can be cryptographically certain that both the sender and the receiver are who they claim they are. Each endpoint is given a cryptographic identity in form of a certificate proving their identity. This gives us the ability to make policies and control service to service communication based on identity.","title":"Zero-Trust Network Architecture"},{"location":"appendix/zero-trust/#zero-trust-network-architecture","text":"Firewalls and zones have been our primary defense mechanism for years. With this model we have defined a perimeter around our applications; keeping potential attackers on the inside, and to have control of what our applications are able to communicate with on the outside. The challenge with this model in a containerized world is that our application portfolio has become more distributed, which leaves us with more components and attack vectors. Additionally, the attack methods have become more sophisticated. Our safety planning and solutions must be able to address the following: What happens if an attacker is able to breach our perimeter? Since our application's architecture is based primarily on an outer defense layer, it would be a relatively simple task for an attacker that is already on the inside to compromise other applications within the same perimeter. Most applications have implemented further safety mechanisms, but those who rely solely on the safety perimeter are extremely vulnerable. This problem is addressed using network segmentation; applications with the same safety level and affiliation are grouped together behind separate firewalls. The challenge remains the same, though; a compromised application could mean a compromised zone. The next level of security using this methodology is micro-segmentation and a zone model where applications and services are grouped in even smaller and more specific zones, givig a potential attacker an even smaller attack surface given a successful attack. Continuing this methodology, the inevitable conclusion will be a perimeter around each individual application. Once each application has its own perimeter, the next thing to address is: What if the network itself is compromised? Are there attackers on the inside that can listen to, or spoof traffic? This is the case on unsafe networks, like the Internet, but here other safety mechanisms are being used ensuring that sensitive information like bank and health data can be transferred. It is no longer a safe assumption that there are no attackers in our own data centers, our private cloud or in the public cloud, so we have to implement mechanisms to secure the communication between our applications even here. We need to base our transportation security on authentication and authorization between all services, so that we can be cryptographically certain that both the sender and the receiver are who they claim they are. Each endpoint is given a cryptographic identity in form of a certificate proving their identity. This gives us the ability to make policies and control service to service communication based on identity.","title":"Zero-Trust Network Architecture"},{"location":"basics/access/","text":"Access from laptop \u00a7 This guide will take you through the required tools and permissions that need to be in place for you to be able to operate your own NAIS application directly from your laptop. Set up a team \u00a7 The primary unit of access is a team , whose origin is a group in Azure AD. Each team is given its own namespace with the same name as the team. The team will have unrestricted access to all Kubernetes assets in that namespace. See creating a new team to get started with teams. After creating a new team, you should have access to all clusters. You're probably part of an existing team If this is your first time here, chances are that you're already part of a team in the context of NAIS. There is currently no simple way to verify this, though you can look through the AAD-groups that you are part of and see if there's any overlap with navikt/teams . Install naisdevice \u00a7 naisdevice ensures that your laptop meets NAVs requirements before allowing access to internal resources such as our NAIS clusters. Install by following the naisdevice installation guide . Install kubectl \u00a7 kubectl is a command-line tool used to manage your Kubernetes resources. Check out the official documentation for instructions on how to install the binaries. Remember that kubectl is supported within one minor version (older or newer) of kube-apiserver . This is called version skew . You can see our on-prem version over at naisible/group_vars . Using brew to manage kubectl will make it troublesome to be within the version skew, as it's hard to downgrade kubectl to older versions. Therefor we recommend installing kubectl manually, or through tools like asdf . Setup your kubeconfig \u00a7 The kubectl tool uses a kubeconfig file to get the information it needs in order to connect to a cluster. We provide a pre-made kubeconfig file with NAV's clusters. Info If you use utviklerimage or connect to NAV through BigIP VPN you need to use the kubeconfig under the Git tag utviklerimage . Go to the directory where you cloned kubeconfig and run git fetch --all --tags --prune git checkout tags/utviklerimage kubectl will by default look for a file named config in the $HOME/.kube/ folder. This can be overriden by having the absolute path of the file in the environment variable KUBECONFIG . export KUBECONFIG = \"<path to your navikt/kubeconfigs repo>/config\" The above example can also be added to something like ~/.bash_profile , or the equivalent in your preferred shell. Warning If you use cygwin, you need the KUBECONFIG to be in Windows style paths rather than unix style paths (e.g. C:\\dev\\kubeconfigs\\config instead of /cygdrive/c/dev/kubeconfigs/config ) Authenticate kubectl \u00a7 Google Cloud Platform (GCP) \u00a7 Before following these steps, make sure your team is enabled for Google Cloud Platform, check out team access for more information. You will also need to perform a self-service step to synchronize your user from Azure AD to Google Cloud Platform. This can be done by following these steps: Login to https://myapps.microsoft.com using your NAV user Click on \"Add app\" at the top of the page Locate \"Google Cloud Platform\", and click on the icon After you have done this your user will be synced to Google Cloud Platform. The sync is not instantaneous, but usually does not take more than a few minutes. First you need to install gcloud following the instructions for your platform. Once installed, you need to authenticate with Google using your NAV e-mail. $ gcloud auth login Make sure you are connected to the right cluster, and verify that it works. $ kubectl config use-context prod-gcp Switched to context \"prod-gcp\" . $ kubectl cluster-info gcp-terraform $ k cluster-info Kubernetes master is running at https://127.0.0.1:14131 ... On-premise \u00a7 When connecting to on-premise clusters, you need to authenticate with Azure AD. $ kubectl config use-context prod-fss Switched to context \"prod-fss\" . $ kubectl get pods To sign in , use a web browser to open the page https://microsoft.com/devicelogin and enter the code CR69DPQQZ to authenticate. When prompted like above, go to the address and enter the code. You then log in with your NAV e-mail and password. When done, kubectl will update your kubeconfig -file with the tokens needed to gain access to the cluster. Recommended tools \u00a7 kubectx - Simplifies changing cluster and namespace context. kubeaware - Visualize which cluster and namespace is currently active. emacs-kubectx-mode - Switch kubectl context and namespace in Emacs and display current setting in mode line.","title":"Access from laptop"},{"location":"basics/access/#access-from-laptop","text":"This guide will take you through the required tools and permissions that need to be in place for you to be able to operate your own NAIS application directly from your laptop.","title":"Access from laptop"},{"location":"basics/access/#set-up-a-team","text":"The primary unit of access is a team , whose origin is a group in Azure AD. Each team is given its own namespace with the same name as the team. The team will have unrestricted access to all Kubernetes assets in that namespace. See creating a new team to get started with teams. After creating a new team, you should have access to all clusters. You're probably part of an existing team If this is your first time here, chances are that you're already part of a team in the context of NAIS. There is currently no simple way to verify this, though you can look through the AAD-groups that you are part of and see if there's any overlap with navikt/teams .","title":"Set up a team"},{"location":"basics/access/#install-naisdevice","text":"naisdevice ensures that your laptop meets NAVs requirements before allowing access to internal resources such as our NAIS clusters. Install by following the naisdevice installation guide .","title":"Install naisdevice"},{"location":"basics/access/#install-kubectl","text":"kubectl is a command-line tool used to manage your Kubernetes resources. Check out the official documentation for instructions on how to install the binaries. Remember that kubectl is supported within one minor version (older or newer) of kube-apiserver . This is called version skew . You can see our on-prem version over at naisible/group_vars . Using brew to manage kubectl will make it troublesome to be within the version skew, as it's hard to downgrade kubectl to older versions. Therefor we recommend installing kubectl manually, or through tools like asdf .","title":"Install kubectl"},{"location":"basics/access/#setup-your-kubeconfig","text":"The kubectl tool uses a kubeconfig file to get the information it needs in order to connect to a cluster. We provide a pre-made kubeconfig file with NAV's clusters. Info If you use utviklerimage or connect to NAV through BigIP VPN you need to use the kubeconfig under the Git tag utviklerimage . Go to the directory where you cloned kubeconfig and run git fetch --all --tags --prune git checkout tags/utviklerimage kubectl will by default look for a file named config in the $HOME/.kube/ folder. This can be overriden by having the absolute path of the file in the environment variable KUBECONFIG . export KUBECONFIG = \"<path to your navikt/kubeconfigs repo>/config\" The above example can also be added to something like ~/.bash_profile , or the equivalent in your preferred shell. Warning If you use cygwin, you need the KUBECONFIG to be in Windows style paths rather than unix style paths (e.g. C:\\dev\\kubeconfigs\\config instead of /cygdrive/c/dev/kubeconfigs/config )","title":"Setup your kubeconfig"},{"location":"basics/access/#authenticate-kubectl","text":"","title":"Authenticate kubectl"},{"location":"basics/access/#google-cloud-platform-gcp","text":"Before following these steps, make sure your team is enabled for Google Cloud Platform, check out team access for more information. You will also need to perform a self-service step to synchronize your user from Azure AD to Google Cloud Platform. This can be done by following these steps: Login to https://myapps.microsoft.com using your NAV user Click on \"Add app\" at the top of the page Locate \"Google Cloud Platform\", and click on the icon After you have done this your user will be synced to Google Cloud Platform. The sync is not instantaneous, but usually does not take more than a few minutes. First you need to install gcloud following the instructions for your platform. Once installed, you need to authenticate with Google using your NAV e-mail. $ gcloud auth login Make sure you are connected to the right cluster, and verify that it works. $ kubectl config use-context prod-gcp Switched to context \"prod-gcp\" . $ kubectl cluster-info gcp-terraform $ k cluster-info Kubernetes master is running at https://127.0.0.1:14131 ...","title":"Google Cloud Platform (GCP)"},{"location":"basics/access/#on-premise","text":"When connecting to on-premise clusters, you need to authenticate with Azure AD. $ kubectl config use-context prod-fss Switched to context \"prod-fss\" . $ kubectl get pods To sign in , use a web browser to open the page https://microsoft.com/devicelogin and enter the code CR69DPQQZ to authenticate. When prompted like above, go to the address and enter the code. You then log in with your NAV e-mail and password. When done, kubectl will update your kubeconfig -file with the tokens needed to gain access to the cluster.","title":"On-premise"},{"location":"basics/access/#recommended-tools","text":"kubectx - Simplifies changing cluster and namespace context. kubeaware - Visualize which cluster and namespace is currently active. emacs-kubectx-mode - Switch kubectl context and namespace in Emacs and display current setting in mode line.","title":"Recommended tools"},{"location":"basics/application/","text":"Your first NAIS application \u00a7 To run an application on NAIS, a manifest file must be created for it. This file is typically named nais.yaml and in this documentation it is referred to as such. Technically the manifest file can be named anything, but it is recommended to name it nais.yaml . Not thrilled by the prospect of editing yaml manually? To kickstart your nais.yaml and GitHub workflow, head over to the app starter or check out our plugins for IntelliJ IDEA or Visual Studio Code Such a nais.yaml file provides NAIS with the necessary information to run your application. If you are starting out for the first time, the minimal nais.yaml example below is a good starting point. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : appname namespace : teamname labels : team : teamname spec : image : ghcr.io/navikt/imagename:tag See https://github.com/features/packages for more info on how to publish packages to ghcr.io. For more information about the nais.yaml specification, see Manifest . Now that you've created your application, it's time to deploy to Kubernetes .","title":"Your first NAIS application"},{"location":"basics/application/#your-first-nais-application","text":"To run an application on NAIS, a manifest file must be created for it. This file is typically named nais.yaml and in this documentation it is referred to as such. Technically the manifest file can be named anything, but it is recommended to name it nais.yaml . Not thrilled by the prospect of editing yaml manually? To kickstart your nais.yaml and GitHub workflow, head over to the app starter or check out our plugins for IntelliJ IDEA or Visual Studio Code Such a nais.yaml file provides NAIS with the necessary information to run your application. If you are starting out for the first time, the minimal nais.yaml example below is a good starting point. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : appname namespace : teamname labels : team : teamname spec : image : ghcr.io/navikt/imagename:tag See https://github.com/features/packages for more info on how to publish packages to ghcr.io. For more information about the nais.yaml specification, see Manifest . Now that you've created your application, it's time to deploy to Kubernetes .","title":"Your first NAIS application"},{"location":"basics/teams/","text":"Teams \u00a7 Access to a resource in NAIS is based on a label set on the resource called team . In the context of Azure Active Directory a group is the same as a team, and you may already be part of a team that has applications on NAIS. Every group in AAD has a so-called mailnick/tag, this is what NAIS generally uses to identify teams. When viewing a group, the mailnick is the value before the @ in the email field. Creating a new team \u00a7 To create a new team, make a pull request to the teams repository on Github The group's owners can manage the group using either outlook or AAD The following resources will be generated for the new team: An Azure AD group is created, and can be viewed in the My Groups portal A GitHub team is created. Deploy keys are created, and can be obtained in the NAIS deploy frontend . GCP users are provisioned, and users can log in to the Google Cloud Console using their NAV e-mail address. Two GCP projects are provisioned, one for development and one for production. See https://console.cloud.google.com/home/dashboard?project=<(dev|prod)-yourteamname> . Namespaces are provisioned in all Kubernetes clusters. Managing your team \u00a7 Team members are managed by managing the group in My Groups portal Warning It is the responsibility of each team to keep the group member roster up to date. This includes removing former team members in a timely fashion. Access to API keys \u00a7 In order to access team API keys , go to deploy.nais.io . Here you will find API keys for all teams you are a member of. Rotate API key \u00a7 Go to deploy.nais.io and click on the \"Rotate key\" button, a pop-up will ask you to confirm the rotation. Team namespaces \u00a7 Team namespaces are supported in both on-prem and in GCP. Refer to the team namespaces documentation for details. GCP Team projects \u00a7 Each team has their own Google Cloud Platform project , which consist of: a set of users, a set of APIs, in addition to authentication and monitoring settings of those APIs. So, for example, all of your Cloud Storage buckets and objects, along with user permissions for accessing them, reside in a project. In general every member of the team has the possibility to add the necessary permissions they need via IAM role managment . There is no general limitation as to which features/products that can be used in a project, but everything needs to have been ROS'd . While we encourage the teams to base their ROS(s) and PVK(s) on the ones done by the NAIS team, each team is responsible to do their own necessary ROS and PVK aimed for their usage/feature not covered by existing ROS. Remember to clean up after yourself, so that NAV doesn't unnecessarily pay for resources. We have a dashboard showing what each team is using, plus a dashboard for everything in GCP. Nais recommends that teams use Terraform, og similar technologies, to control the lifecycle of resources created in team-projects. This is more important in production, for services that will run for a longer period of time, than ad-hoc experimentation in dev. Access management \u00a7 To promote autonomous teams, teams control access to their own projects completely by themselves. This can be done either by pipeline, or manually adding access when needed. There are different scenarios for when and how to give access to users, and the official Google Cloud Docs is definitively the best source for information. Google Docs has a list of possible predefined roles that we recommend using. These roles can also be listed our with gcloud iam roles list --filter $resource . Running the command without the --filter argument will return a very long list. Temporary access \u00a7 On a general basis, access should not be permanent. A good habit is to only grant your self or other a temporary access. Using the following gcloud -cli command will grant a user 1 hours of access to roles/cloudsql.instanceUser : gcloud projects add-iam-policy-binding <PROJECT_ID> --member = user:<FIRSTNAME>.<LASTNAME>@nav.no --role = roles/cloudsql.instanceUser --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" There is more information over at Google Cloud Docs . Examples \u00a7 Access to Postgres We have a separate guide for how to give your self temporary access to a Postgres database. Head on over to persistence/postgres . Underneath we have outlined two typical use cases for when you need to give your self or someone else access. Do get a better understanding of how to give and revoke access with gcloud , you should read Granting, changing, and revoking access to resources . How to give your self access \u00a7 To be able to run this commando, you first need to find your PROJECT_ID , and the specific ROLE_NAME you want access to. To adhere to the principle of using temporary access, the example below will give access to ROLE_NAME for 1 hour. gcloud projects add-iam-policy-binding <PROJECT_ID> --member = user:<FIRSTNAME>.<LASTNAME>@nav.no --role = <ROLE_NAME> --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" As an example, if you'd want to view your team's Storage buckets, you'll need the roles/storage.objectViewer role. It's always smart to only give out temporary access . This can also be leveraged to give a user belonging to a different team access to your team's resources. How to give a service account access \u00a7 A service account is also a --member of a project, but instead of running the command mentioned in How to give your self access , you change --member=user:<email> to --member=serviceAccount:<email> . GCP IAM recommender \u00a7 Sometimes you might end up giving more (a wider) access than strictly necessary - but fear not! GCP has an IAM recommender which will monitor each access permission given, and compare it to accesses actually asked for (or required). After which the IAM recommender will recommend a more granular access, that probably fits you and your usage better! Read more about the IAM recommender over at Google Cloud Docs .","title":"Teams"},{"location":"basics/teams/#teams","text":"Access to a resource in NAIS is based on a label set on the resource called team . In the context of Azure Active Directory a group is the same as a team, and you may already be part of a team that has applications on NAIS. Every group in AAD has a so-called mailnick/tag, this is what NAIS generally uses to identify teams. When viewing a group, the mailnick is the value before the @ in the email field.","title":"Teams"},{"location":"basics/teams/#creating-a-new-team","text":"To create a new team, make a pull request to the teams repository on Github The group's owners can manage the group using either outlook or AAD The following resources will be generated for the new team: An Azure AD group is created, and can be viewed in the My Groups portal A GitHub team is created. Deploy keys are created, and can be obtained in the NAIS deploy frontend . GCP users are provisioned, and users can log in to the Google Cloud Console using their NAV e-mail address. Two GCP projects are provisioned, one for development and one for production. See https://console.cloud.google.com/home/dashboard?project=<(dev|prod)-yourteamname> . Namespaces are provisioned in all Kubernetes clusters.","title":"Creating a new team"},{"location":"basics/teams/#managing-your-team","text":"Team members are managed by managing the group in My Groups portal Warning It is the responsibility of each team to keep the group member roster up to date. This includes removing former team members in a timely fashion.","title":"Managing your team"},{"location":"basics/teams/#access-to-api-keys","text":"In order to access team API keys , go to deploy.nais.io . Here you will find API keys for all teams you are a member of.","title":"Access to API keys"},{"location":"basics/teams/#rotate-api-key","text":"Go to deploy.nais.io and click on the \"Rotate key\" button, a pop-up will ask you to confirm the rotation.","title":"Rotate API key"},{"location":"basics/teams/#team-namespaces","text":"Team namespaces are supported in both on-prem and in GCP. Refer to the team namespaces documentation for details.","title":"Team namespaces"},{"location":"basics/teams/#gcp-team-projects","text":"Each team has their own Google Cloud Platform project , which consist of: a set of users, a set of APIs, in addition to authentication and monitoring settings of those APIs. So, for example, all of your Cloud Storage buckets and objects, along with user permissions for accessing them, reside in a project. In general every member of the team has the possibility to add the necessary permissions they need via IAM role managment . There is no general limitation as to which features/products that can be used in a project, but everything needs to have been ROS'd . While we encourage the teams to base their ROS(s) and PVK(s) on the ones done by the NAIS team, each team is responsible to do their own necessary ROS and PVK aimed for their usage/feature not covered by existing ROS. Remember to clean up after yourself, so that NAV doesn't unnecessarily pay for resources. We have a dashboard showing what each team is using, plus a dashboard for everything in GCP. Nais recommends that teams use Terraform, og similar technologies, to control the lifecycle of resources created in team-projects. This is more important in production, for services that will run for a longer period of time, than ad-hoc experimentation in dev.","title":"GCP Team projects"},{"location":"basics/teams/#access-management","text":"To promote autonomous teams, teams control access to their own projects completely by themselves. This can be done either by pipeline, or manually adding access when needed. There are different scenarios for when and how to give access to users, and the official Google Cloud Docs is definitively the best source for information. Google Docs has a list of possible predefined roles that we recommend using. These roles can also be listed our with gcloud iam roles list --filter $resource . Running the command without the --filter argument will return a very long list.","title":"Access management"},{"location":"basics/teams/#temporary-access","text":"On a general basis, access should not be permanent. A good habit is to only grant your self or other a temporary access. Using the following gcloud -cli command will grant a user 1 hours of access to roles/cloudsql.instanceUser : gcloud projects add-iam-policy-binding <PROJECT_ID> --member = user:<FIRSTNAME>.<LASTNAME>@nav.no --role = roles/cloudsql.instanceUser --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" There is more information over at Google Cloud Docs .","title":"Temporary access"},{"location":"basics/teams/#examples","text":"Access to Postgres We have a separate guide for how to give your self temporary access to a Postgres database. Head on over to persistence/postgres . Underneath we have outlined two typical use cases for when you need to give your self or someone else access. Do get a better understanding of how to give and revoke access with gcloud , you should read Granting, changing, and revoking access to resources .","title":"Examples"},{"location":"basics/teams/#how-to-give-your-self-access","text":"To be able to run this commando, you first need to find your PROJECT_ID , and the specific ROLE_NAME you want access to. To adhere to the principle of using temporary access, the example below will give access to ROLE_NAME for 1 hour. gcloud projects add-iam-policy-binding <PROJECT_ID> --member = user:<FIRSTNAME>.<LASTNAME>@nav.no --role = <ROLE_NAME> --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" As an example, if you'd want to view your team's Storage buckets, you'll need the roles/storage.objectViewer role. It's always smart to only give out temporary access . This can also be leveraged to give a user belonging to a different team access to your team's resources.","title":"How to give your self access"},{"location":"basics/teams/#how-to-give-a-service-account-access","text":"A service account is also a --member of a project, but instead of running the command mentioned in How to give your self access , you change --member=user:<email> to --member=serviceAccount:<email> .","title":"How to give a service account access"},{"location":"basics/teams/#gcp-iam-recommender","text":"Sometimes you might end up giving more (a wider) access than strictly necessary - but fear not! GCP has an IAM recommender which will monitor each access permission given, and compare it to accesses actually asked for (or required). After which the IAM recommender will recommend a more granular access, that probably fits you and your usage better! Read more about the IAM recommender over at Google Cloud Docs .","title":"GCP IAM recommender"},{"location":"cli/","text":"nais-cli \u00a7 nais-cli is a simple CLI application that developers in NAV can use. If you want to know more about the tool internals, please visit and read the github repository documentation. Prerequisites \u00a7 If you haven't already... Install and authenticate with naisdevice . Make sure you have configured the NAIS cluster you want to use. Use \u00a7 See the chapters about installation and commands .","title":"Overview"},{"location":"cli/#nais-cli","text":"nais-cli is a simple CLI application that developers in NAV can use. If you want to know more about the tool internals, please visit and read the github repository documentation.","title":"nais-cli"},{"location":"cli/#prerequisites","text":"If you haven't already... Install and authenticate with naisdevice . Make sure you have configured the NAIS cluster you want to use.","title":"Prerequisites"},{"location":"cli/#use","text":"See the chapters about installation and commands .","title":"Use"},{"location":"cli/install/","text":"Installation \u00a7 macOS Installation \u00a7 Install Homebrew unless you already have it Add the nais tap brew tap nais/tap Install the nais-cli brew install nais Windows Installation \u00a7 Download the archive for windows and unpack the tool to a directory on your $PATH . Install nais-cli Ubuntu Installation \u00a7 Add the nais PPA repo: NAIS_GPG_KEY=\"/usr/local/share/keyrings/nav_nais.gpg\" sudo mkdir -p \"$(dirname \"$NAIS_GPG_KEY\")\" curl -sfSL \"https://ppa.nais.io/KEY.gpg\" | gpg --dearmor | sudo dd of=\"$NAIS_GPG_KEY\" echo \"deb [signed-by=$NAIS_GPG_KEY] https://ppa.nais.io/ ./\" | sudo tee /etc/apt/sources.list.d/nav_nais.list sudo apt update # Now you can apt install nais Install the nais package: sudo apt install nais","title":"Installation"},{"location":"cli/install/#installation","text":"","title":"Installation"},{"location":"cli/install/#macos-installation","text":"Install Homebrew unless you already have it Add the nais tap brew tap nais/tap Install the nais-cli brew install nais","title":"macOS Installation"},{"location":"cli/install/#windows-installation","text":"Download the archive for windows and unpack the tool to a directory on your $PATH . Install nais-cli","title":"Windows Installation"},{"location":"cli/install/#ubuntu-installation","text":"Add the nais PPA repo: NAIS_GPG_KEY=\"/usr/local/share/keyrings/nav_nais.gpg\" sudo mkdir -p \"$(dirname \"$NAIS_GPG_KEY\")\" curl -sfSL \"https://ppa.nais.io/KEY.gpg\" | gpg --dearmor | sudo dd of=\"$NAIS_GPG_KEY\" echo \"deb [signed-by=$NAIS_GPG_KEY] https://ppa.nais.io/ ./\" | sudo tee /etc/apt/sources.list.d/nav_nais.list sudo apt update # Now you can apt install nais Install the nais package: sudo apt install nais","title":"Ubuntu Installation"},{"location":"cli/commands/","text":"Commands \u00a7 aiven device","title":"Commands"},{"location":"cli/commands/#commands","text":"aiven device","title":"Commands"},{"location":"cli/commands/aiven/","text":"aiven command \u00a7 The aiven command can be used to create a AivenApplication and extract credentials. The aiven create command will create a Protected & time-limited AivenApplication in your specified namespace. This uses your currently configured kubectl context, so in order for it to work you need to select a suitable context first. For instance, credentials for nav-prod can only be generated in the prod clusters. This command will give access to personal but time limited credentials. These credentials can be used to debug an Aiven hosted kafka topic. The aiven get command extracts the credentials and puts them in /tmp folder. The created AivenApplication has sane default (days-to-live) set to 1 day. To gain access to a specific topic be sure to update your topic resource and topic ACLs. Add username to acl.application field in your topic.yaml and apply to your namespace. The username is the one spescified in the next step . # topic.yml spec : pool : nav-integration-test config : retentionHours : 900 acl : - access : read team : test application : username create \u00a7 nais aiven create username namespace Argument Required Description username Yes Preferred username. namespace Yes Kubernetes namespace where AivenApplication will be created. nais aiven create username namespace -p nav-prod -s some-unique-secretname -e 10 Flag Required Short Default Description pool No -p nav-dev Kafka pool . secret-name No -s namespace-username-randomstring Preferred secret-name. expire No -e 1 Time in days the secret should be valid. get \u00a7 nais aiven get secret-name namespace Argument Required Description secret-name Yes Default secret-name or flag -s in create command. namespace Yes Kubernetes namespace for the created AivenApplication. nais aiven get secret-name namespace -c kcat Flag Required Short Default Description config No -c all Type of config to generated, supported values: java, .env, kcat, all. tidy \u00a7 Removes folders in /tmp ($TMPDIR) directory that starts with aiven-secret- . nais aiven tidy Available output \u00a7 After Successful nais aiven create and nais aiven get commands, a set of files wil be available. Configuration \u00a7 You can specify a configuration flag to generate all | kcat | .env | java . Default is all all \u00a7 client.keystore.p12 client.truststore.jks kafka-ca.pem kafka-certificate.crt kafka-private-key.pem kafka-secret.env kcat.conf kafka.properties .env \u00a7 client.keystore.p12 client.truststore.jks kafka-ca.pem kafka-certificate.crt kafka-private-key.pem kafka-secret.env kafka-secret.env file \u00a7 KAFKA_BROKERS = \"<broker uri>\" KAFKA_CA = \"<ca certificate>\" KAFKA_CA_PATH = \"<path to ca certificate>\" KAFKA_CERTIFICATE = \"<client certificate>\" KAFKA_CERTIFICATE_PATH = \"<path to client certificate>\" KAFKA_CREDSTORE_PASSWORD = \"<password for keystore/truststore>\" KAFKA_KEYSTORE_PATH = \"<path to keystore>\" KAFKA_PRIVATE_KEY = \"<private key>\" KAFKA_PRIVATE_KEY_PATH = \"<path to private key>\" KAFKA_SCHEMA_REGISTRY = \"<schema registry uri>\" KAFKA_SCHEMA_REGISTRY_PASSWORD = \"<schema registry password>\" KAFKA_SCHEMA_REGISTRY_USER = \"<schema registry username>\" KAFKA_TRUSTSTORE_PATH = \"<path to truststore>\" kcat \u00a7 kafka-ca.pem kafka-client-certificate.crt kafka-client-private-key.pem kcat.conf kcat.conf file \u00a7 bootstrap.servers = <broker uri> ssl.certificate.location = <path to client certificate> ssl.key.location = <path to private key> ssl.ca.location = <path to ca certificate> security.protocol = ssl The generated kcat.conf can be used with kcat to authenticate against the Aiven hosted topics in GCP. Read more about kcat.conf configurable properties . You can refer to generated config with -F flag: kcat -F path/to/kcat.conf -t namespace.your.topic Alternatively, you can specify the same settings directly on the command line: kcat \\ -b boostrap-server.aivencloud.com:26484 \\ -X security.protocol = ssl \\ -X ssl.key.location = service.key \\ -X ssl.certificate.location = service.cert \\ -X ssl.ca.location = ca.pem For more details aiven-kcat java \u00a7 client.keystore.p12 client.truststore.jks kafka.properties kafka.properties file \u00a7 # nais-cli 2021-11-16 20:26:00 +0100 CET # Usage example: kafka-console-consumer.sh --topic aura.your.topic --bootstrap-server <broker uri> --consumer.config <file path>/kafka.properties security.protocol=SSL ssl.protocol=TLS ssl.keystore.type=PKCS12 ssl.truststore.type=JKS ssl.keystore.location=<path to keystore> ssl.key.password=<password for keystore/truststore> ssl.keystore.password=<password for keystore/truststore> ssl.truststore.password=<password for keystore/truststore> ssl.truststore.location=<path to truststore> The kafka.properties file can be used with the official Kafka command-line tools included in the Kafka distribution, and with many other Java based tools/applications.","title":"Aiven"},{"location":"cli/commands/aiven/#aiven-command","text":"The aiven command can be used to create a AivenApplication and extract credentials. The aiven create command will create a Protected & time-limited AivenApplication in your specified namespace. This uses your currently configured kubectl context, so in order for it to work you need to select a suitable context first. For instance, credentials for nav-prod can only be generated in the prod clusters. This command will give access to personal but time limited credentials. These credentials can be used to debug an Aiven hosted kafka topic. The aiven get command extracts the credentials and puts them in /tmp folder. The created AivenApplication has sane default (days-to-live) set to 1 day. To gain access to a specific topic be sure to update your topic resource and topic ACLs. Add username to acl.application field in your topic.yaml and apply to your namespace. The username is the one spescified in the next step . # topic.yml spec : pool : nav-integration-test config : retentionHours : 900 acl : - access : read team : test application : username","title":"aiven command"},{"location":"cli/commands/aiven/#create","text":"nais aiven create username namespace Argument Required Description username Yes Preferred username. namespace Yes Kubernetes namespace where AivenApplication will be created. nais aiven create username namespace -p nav-prod -s some-unique-secretname -e 10 Flag Required Short Default Description pool No -p nav-dev Kafka pool . secret-name No -s namespace-username-randomstring Preferred secret-name. expire No -e 1 Time in days the secret should be valid.","title":"create"},{"location":"cli/commands/aiven/#get","text":"nais aiven get secret-name namespace Argument Required Description secret-name Yes Default secret-name or flag -s in create command. namespace Yes Kubernetes namespace for the created AivenApplication. nais aiven get secret-name namespace -c kcat Flag Required Short Default Description config No -c all Type of config to generated, supported values: java, .env, kcat, all.","title":"get"},{"location":"cli/commands/aiven/#tidy","text":"Removes folders in /tmp ($TMPDIR) directory that starts with aiven-secret- . nais aiven tidy","title":"tidy"},{"location":"cli/commands/aiven/#available-output","text":"After Successful nais aiven create and nais aiven get commands, a set of files wil be available.","title":"Available output"},{"location":"cli/commands/aiven/#configuration","text":"You can specify a configuration flag to generate all | kcat | .env | java . Default is all","title":"Configuration"},{"location":"cli/commands/aiven/#all","text":"client.keystore.p12 client.truststore.jks kafka-ca.pem kafka-certificate.crt kafka-private-key.pem kafka-secret.env kcat.conf kafka.properties","title":"all"},{"location":"cli/commands/aiven/#env","text":"client.keystore.p12 client.truststore.jks kafka-ca.pem kafka-certificate.crt kafka-private-key.pem kafka-secret.env","title":".env"},{"location":"cli/commands/aiven/#kafka-secretenv-file","text":"KAFKA_BROKERS = \"<broker uri>\" KAFKA_CA = \"<ca certificate>\" KAFKA_CA_PATH = \"<path to ca certificate>\" KAFKA_CERTIFICATE = \"<client certificate>\" KAFKA_CERTIFICATE_PATH = \"<path to client certificate>\" KAFKA_CREDSTORE_PASSWORD = \"<password for keystore/truststore>\" KAFKA_KEYSTORE_PATH = \"<path to keystore>\" KAFKA_PRIVATE_KEY = \"<private key>\" KAFKA_PRIVATE_KEY_PATH = \"<path to private key>\" KAFKA_SCHEMA_REGISTRY = \"<schema registry uri>\" KAFKA_SCHEMA_REGISTRY_PASSWORD = \"<schema registry password>\" KAFKA_SCHEMA_REGISTRY_USER = \"<schema registry username>\" KAFKA_TRUSTSTORE_PATH = \"<path to truststore>\"","title":"kafka-secret.env file"},{"location":"cli/commands/aiven/#kcat","text":"kafka-ca.pem kafka-client-certificate.crt kafka-client-private-key.pem kcat.conf","title":"kcat"},{"location":"cli/commands/aiven/#kcatconf-file","text":"bootstrap.servers = <broker uri> ssl.certificate.location = <path to client certificate> ssl.key.location = <path to private key> ssl.ca.location = <path to ca certificate> security.protocol = ssl The generated kcat.conf can be used with kcat to authenticate against the Aiven hosted topics in GCP. Read more about kcat.conf configurable properties . You can refer to generated config with -F flag: kcat -F path/to/kcat.conf -t namespace.your.topic Alternatively, you can specify the same settings directly on the command line: kcat \\ -b boostrap-server.aivencloud.com:26484 \\ -X security.protocol = ssl \\ -X ssl.key.location = service.key \\ -X ssl.certificate.location = service.cert \\ -X ssl.ca.location = ca.pem For more details aiven-kcat","title":"kcat.conf file"},{"location":"cli/commands/aiven/#java","text":"client.keystore.p12 client.truststore.jks kafka.properties","title":"java"},{"location":"cli/commands/aiven/#kafkaproperties-file","text":"# nais-cli 2021-11-16 20:26:00 +0100 CET # Usage example: kafka-console-consumer.sh --topic aura.your.topic --bootstrap-server <broker uri> --consumer.config <file path>/kafka.properties security.protocol=SSL ssl.protocol=TLS ssl.keystore.type=PKCS12 ssl.truststore.type=JKS ssl.keystore.location=<path to keystore> ssl.key.password=<password for keystore/truststore> ssl.keystore.password=<password for keystore/truststore> ssl.truststore.password=<password for keystore/truststore> ssl.truststore.location=<path to truststore> The kafka.properties file can be used with the official Kafka command-line tools included in the Kafka distribution, and with many other Java based tools/applications.","title":"kafka.properties file"},{"location":"cli/commands/device/","text":"device command \u00a7 The device command can be used to connect to, disconnect from, and view the connection status of naisdevice . Currently, the command requires the processes naisdevice-agent and naisdevice-helper to run, both of which can be run by starting naisdevice. connect \u00a7 Requests a connection and waits for success. The expected result is \"Connected\". nais device connect disconnect \u00a7 Requests a disconnection and waits for success. The expected result is \"Disconnected\". nais device disconnect status \u00a7 Prints the current connection status of naisdevice-agent . This includes connection status, as well as gateways and their current statuses. nais device status Flag Required Short Default Description quiet No -q false Only print connection status. output No -o '' Specify output format. (yaml output format If the output format and quiet flags are specified, output takes precedence. jita \u00a7 Starts the just-in-time access flow for a named gateway. This should redirect you to a browser to submit a request for just-in-time access. nais device jita my-privileged-access-gateway Argument Required Description gateway Yes The desired gateway to establish a connection to. Which gateways require just-in-time access? You can view gateways and their JITA requirement with nais device status . This snippet explicitly shows the names of gateways with that requirement. nais device status -ojson | jq -r '.Gateways[] | select(.requiresPrivilegedAccess == true) | .name' config \u00a7 Shows and adjusts the naisdevice-agent configuration. get \u00a7 Shows the current naisdevice-agent configuration. Currently, there are only to config fields: AutoConnect and CertRenewal . nais device config get set \u00a7 Sets a naisdevice-agent configuration field to a desired value. nais device config set AutoConnect true Argument Required Description setting Yes The setting to adjust. Must be one of [autoconnect, certrenewal] , case insensitive. value Yes The value to set. Must be one of [true, false] , or anything strconv.ParseBool can parse.","title":"Device"},{"location":"cli/commands/device/#device-command","text":"The device command can be used to connect to, disconnect from, and view the connection status of naisdevice . Currently, the command requires the processes naisdevice-agent and naisdevice-helper to run, both of which can be run by starting naisdevice.","title":"device command"},{"location":"cli/commands/device/#connect","text":"Requests a connection and waits for success. The expected result is \"Connected\". nais device connect","title":"connect"},{"location":"cli/commands/device/#disconnect","text":"Requests a disconnection and waits for success. The expected result is \"Disconnected\". nais device disconnect","title":"disconnect"},{"location":"cli/commands/device/#status","text":"Prints the current connection status of naisdevice-agent . This includes connection status, as well as gateways and their current statuses. nais device status Flag Required Short Default Description quiet No -q false Only print connection status. output No -o '' Specify output format. (yaml output format If the output format and quiet flags are specified, output takes precedence.","title":"status"},{"location":"cli/commands/device/#jita","text":"Starts the just-in-time access flow for a named gateway. This should redirect you to a browser to submit a request for just-in-time access. nais device jita my-privileged-access-gateway Argument Required Description gateway Yes The desired gateway to establish a connection to. Which gateways require just-in-time access? You can view gateways and their JITA requirement with nais device status . This snippet explicitly shows the names of gateways with that requirement. nais device status -ojson | jq -r '.Gateways[] | select(.requiresPrivilegedAccess == true) | .name'","title":"jita"},{"location":"cli/commands/device/#config","text":"Shows and adjusts the naisdevice-agent configuration.","title":"config"},{"location":"cli/commands/device/#get","text":"Shows the current naisdevice-agent configuration. Currently, there are only to config fields: AutoConnect and CertRenewal . nais device config get","title":"get"},{"location":"cli/commands/device/#set","text":"Sets a naisdevice-agent configuration field to a desired value. nais device config set AutoConnect true Argument Required Description setting Yes The setting to adjust. Must be one of [autoconnect, certrenewal] , case insensitive. value Yes The value to set. Must be one of [true, false] , or anything strconv.ParseBool can parse.","title":"set"},{"location":"clusters/gcp/","text":"Google Cloud Platform \u00a7 cluster environment comment dev-gcp development selected ingresses publicly accessible prod-gcp production publicly accessible labs-gcp development publicly accessible In GCP, we do not operate with a zone model like with the on-premise clusters. Instead, we rely on a zero trust model with a service mesh. The only thing we differentiate on a cluster level is development and production. The applications running in GCP need access policy rules defined for every other service they receive requests from or sends requests to. To access the GCP clusters, see Access . Access to GCP \u00a7 In order to use GCP, a team is required to add their team in a PR to navikt/teams . This will generate a namespace for the team in each cluster, and dev and prod GCP projects will be created. The team's group is initially granted a restricted set of permissions in these projects, but have the ability to grant further permissions on demand using the GCP console Warning With the ability to grant permissions, the team has full control of the team's GCP projects, and should take care when granting further permissions or enabling features and APIs. Accessing the application \u00a7 Access is controlled in part by ingresses, which define where your application will be exposed as a HTTP endpoint. You can control where your application is reachable from by selecting the appropriate ingress domain. Warning Make sure you understand where you expose your application, taking into account the state of your application, what kind of data it exposes and how it is secured. If in doubt, ask in #nais or someone on the NAIS team. You can control from where you application is reachable by selecting the appropriate ingress domain. If no ingress is selected, the application will not be reachable from outside the cluster. dev-gcp ingresses \u00a7 domain accessible from description ekstern.dev.nav.no internet manually configured, see instructions below . URLs containing /metrics , /actuator or /internal are blocked. dev.nav.no naisdevice development ingress for nav.no applications dev.intern.nav.no naisdevice development ingress for non-public/internet-facing applications dev-gcp.nais.io naisdevice reserved for platform services dev.adeo.no deprecated replaced by dev.intern.nav.no ekstern.dev.nav.no \u00a7 In order to allow an ingress <app>.ekstern.dev.nav.no to resolve, it must be configured in our loadbalancer config found on GitHub . You might see a 404 Page not found error when visiting the GitHub repository for the load balancer config. To gain access, do the following: Go to myapps.microsoft.com . Find the GitHub.com (nais) app and add it to your account. Add a new entry to external_domains under rules for \"gw-ekstern-dev-nav-no\" : external_domains = { ... \"gw-ekstern-dev-nav-no\" = { rules = [ ... { description = \"allow access to <app>.ekstern.dev.nav.no\" priority = \"<previous value, incremented by 1>\" action = \"allow\" expr = \"request.headers['Host'] == '<app>.ekstern.dev.nav.no'\" preview = false }, ] } } Commit the changes and create a pull request. prod-gcp ingresses \u00a7 domain accessible from description nav.no internet manually configured, contact at #tech-sikkerhet. URLs containing /metrics , /actuator or /internal are blocked intern.nav.no naisdevice used by non-public/internet-facing applications (previously called adeo.no). prod-gcp.nais.io naisdevice reserved for platform services More info about how DNS is configured for these domains can be found here labs-gcp ingresses \u00a7 domain accessible from description labs.nais.io internet automatically configured ROS and PVK \u00a7 When establishing an application on GCP, it is a great time to update its Risikovurdering ( ROS ) analysis. It is required to update the application's entry in the Behandlingsoversikt when changing platforms. If both of these words are unfamiliar to your team, it's time to sit down and take a look at both of them. Every application needs to have a ROS analysis. Applications handling personal information needs a Personvernkonsekvens ( PVK ) analysis and an entry in the Behandlingsoversikt . See also additional information about ROS and PVK under Laws and regulations. Questions about ROS can be directed to Leif Tore L\u00f8vmo or Line Langlo Spongsveen or posted in #tryggnok . Questions about Behandling should be directed to #behandlinskatalogen .","title":"Google Cloud Platform"},{"location":"clusters/gcp/#google-cloud-platform","text":"cluster environment comment dev-gcp development selected ingresses publicly accessible prod-gcp production publicly accessible labs-gcp development publicly accessible In GCP, we do not operate with a zone model like with the on-premise clusters. Instead, we rely on a zero trust model with a service mesh. The only thing we differentiate on a cluster level is development and production. The applications running in GCP need access policy rules defined for every other service they receive requests from or sends requests to. To access the GCP clusters, see Access .","title":"Google Cloud Platform"},{"location":"clusters/gcp/#access-to-gcp","text":"In order to use GCP, a team is required to add their team in a PR to navikt/teams . This will generate a namespace for the team in each cluster, and dev and prod GCP projects will be created. The team's group is initially granted a restricted set of permissions in these projects, but have the ability to grant further permissions on demand using the GCP console Warning With the ability to grant permissions, the team has full control of the team's GCP projects, and should take care when granting further permissions or enabling features and APIs.","title":"Access to GCP"},{"location":"clusters/gcp/#accessing-the-application","text":"Access is controlled in part by ingresses, which define where your application will be exposed as a HTTP endpoint. You can control where your application is reachable from by selecting the appropriate ingress domain. Warning Make sure you understand where you expose your application, taking into account the state of your application, what kind of data it exposes and how it is secured. If in doubt, ask in #nais or someone on the NAIS team. You can control from where you application is reachable by selecting the appropriate ingress domain. If no ingress is selected, the application will not be reachable from outside the cluster.","title":"Accessing the application"},{"location":"clusters/gcp/#dev-gcp-ingresses","text":"domain accessible from description ekstern.dev.nav.no internet manually configured, see instructions below . URLs containing /metrics , /actuator or /internal are blocked. dev.nav.no naisdevice development ingress for nav.no applications dev.intern.nav.no naisdevice development ingress for non-public/internet-facing applications dev-gcp.nais.io naisdevice reserved for platform services dev.adeo.no deprecated replaced by dev.intern.nav.no","title":"dev-gcp ingresses"},{"location":"clusters/gcp/#eksterndevnavno","text":"In order to allow an ingress <app>.ekstern.dev.nav.no to resolve, it must be configured in our loadbalancer config found on GitHub . You might see a 404 Page not found error when visiting the GitHub repository for the load balancer config. To gain access, do the following: Go to myapps.microsoft.com . Find the GitHub.com (nais) app and add it to your account. Add a new entry to external_domains under rules for \"gw-ekstern-dev-nav-no\" : external_domains = { ... \"gw-ekstern-dev-nav-no\" = { rules = [ ... { description = \"allow access to <app>.ekstern.dev.nav.no\" priority = \"<previous value, incremented by 1>\" action = \"allow\" expr = \"request.headers['Host'] == '<app>.ekstern.dev.nav.no'\" preview = false }, ] } } Commit the changes and create a pull request.","title":"ekstern.dev.nav.no"},{"location":"clusters/gcp/#prod-gcp-ingresses","text":"domain accessible from description nav.no internet manually configured, contact at #tech-sikkerhet. URLs containing /metrics , /actuator or /internal are blocked intern.nav.no naisdevice used by non-public/internet-facing applications (previously called adeo.no). prod-gcp.nais.io naisdevice reserved for platform services More info about how DNS is configured for these domains can be found here","title":"prod-gcp ingresses"},{"location":"clusters/gcp/#labs-gcp-ingresses","text":"domain accessible from description labs.nais.io internet automatically configured","title":"labs-gcp ingresses"},{"location":"clusters/gcp/#ros-and-pvk","text":"When establishing an application on GCP, it is a great time to update its Risikovurdering ( ROS ) analysis. It is required to update the application's entry in the Behandlingsoversikt when changing platforms. If both of these words are unfamiliar to your team, it's time to sit down and take a look at both of them. Every application needs to have a ROS analysis. Applications handling personal information needs a Personvernkonsekvens ( PVK ) analysis and an entry in the Behandlingsoversikt . See also additional information about ROS and PVK under Laws and regulations. Questions about ROS can be directed to Leif Tore L\u00f8vmo or Line Langlo Spongsveen or posted in #tryggnok . Questions about Behandling should be directed to #behandlinskatalogen .","title":"ROS and PVK"},{"location":"clusters/migrating-databases-to-gcp/","text":"Migrating databases to GCP \u00a7 Migrating databases to GCP postgreSQL \u00a7 Suggested patterns for moving on-prem databases to GCP postgreSQL. Disclaimer: These are options for migrations to GCP postgreSQL. Others may work better for your team. Prerequisites \u00a7 The team needs to update their ROS and PVK analysis to migrate to GCP. Refer to the ROS and PVK section under Google Cloud Platform clusters . See database creation in GCP in Google Cloud Platform persistence . Migration paths available \u00a7 From on-premise Oracle \u00a7 Replication migration using migration application \u00a7 Create a simple migration application that supports writing data to the new database. Using requests sent to this application you can populate the new postgreSQL database. Rewrite the oracle DDL scripts to postgreSQL. If your oracle database contains specific oracle procedures or functions, that do not exist in postgreSQL, they will have to be recreated in some other way. There are tools available to help ease this rewrite, for example ora2pg( http://ora2pg.darold.net/start.html ). Create the postgreSQL database in GCP and start deploy the application to GCP with the empty database and let flyway (or other database versioning software) create the DDLs. Create migration app as a container in the same pod as the database application (this is to avoid permission issues using the same database). This migration application only handles the data transfer from the oracle database to postgreSQL in GCP. Examples: PAM Stillingsregistrering API Migration (documentation and code) PAM AD Migration (no documentation, just code) Rekrutteringsbistand migration (not using JPA for easier code and less memory-intensive data handling, but as-is only suitable on-premise migration) Trigger migration from command line (or use another form of trigger) and read the data from a feed or kafka. Pros: No downtime Live synchronization between on-premise and GCP Migration controlled entirely by team Migration can be stopped and restarted at any moment Cons: Can be slow if large amounts of data are to be transferred, if this is the case use kafka for the streaming process instead Can be tricky for complex databases Note This procedure is also valid for on-premise postgreSQL migration, and even simpler as no rewrite is necessary. From on-premise postgreSQL \u00a7 Migration using pg_dump \u00a7 This method is suitable for applications that can have the database in read-only or application that allow for some downtime. It requires that the database instance and DDLs are created up front (i.e. deploy your application in GCP and let flyway create DDLs): Use docker container image with psql and cloudsdk: GCP migration image . This image let you do all the following actions from one place. Deploy the pod into on-premise cluster that can connect to the database kubectl apply -f https://raw.githubusercontent.com/navikt/gcp-migrering/main/gcloud.yml exec into that pod kubectl exec -it gcloud /bin/bash Log in to gcloud with your own NAV-account gcloud auth login Configure the project id (find project id with gcloud projects list --filter team ) gcloud config set project <project id> Create a GCP bucket. You will need the roles/storage.admin IAM role for the required operations on the bucket. gsutil mb -l europe-north1 gs://<bucket name> Find the GCP service account e-mail (the instance id is specified in your nais.yaml file) gcloud sql instances describe <CloudSQL instance id> | yq r - serviceAccountEmailAddress Set the objectAdmin role for the bucket (with the previous e-mail) gsutil iam ch serviceAccount:<GCP service account e-mail>:objectAdmin gs://<bucket name>/ Use pg_dump to create the dump file. Notes: Make sure that you stop writes to database before running pg_dump . Get a database user from Vault . If the database in GCP already has the flyway_schema_history table, you might want to exclude the equivalent table in the dump by using the --exclude-table=flyway_schema_history option. pg_dump \\ -h <postgreSQL on-premise host name> \\ -d <database instance name> \\ -U <database user name to connect with> \\ --format = plain --no-owner --no-acl --data-only -Z 9 > dump.sql.gz Copy the dump file to GCP bucket gsutil -o GSUtil:parallel_composite_upload_threshold = 150M -h \"Content-Type:application/x-gzip\" cp dump.sql.gz gs://<bucket name>/ Import the dump into the GCP postgreSQL database. Notes: You need the roles/cloudsql.admin IAM role in order to perform the import. The user in the command below should be a GCP SQL Instance user. If the GCP Postgres database has any existing tables or sequences, make sure that the user has all required grants for these. gcloud sql import sql <Cloud SQL instance id> gs://<bucket name>/dump.sql.gz \\ --database = <database instance name> \\ --user = <database instance user name> Verify that the application is behaving as expected and that the data in the new database is correct. Finally we need to switch loadbalancer to route to the GCP application instead of the on-premise equivalent. Delete the bucket in GCP after migration is complete gsutil -m rm -r gs://<bucket name> gsutil rb gs://<bucket name> Pros: Easy and relatively fast migration path No need for separate migration application or streams to populate database Cons: Requires downtime for the application, or at least no writes to database Requires node with access to on-premise database and GCP buckets Replication migration using migration application \u00a7 Same procedure as for Oracle. Replication migration using pgbouncer \u00a7 Not available as of now.","title":"Migrating databases to GCP"},{"location":"clusters/migrating-databases-to-gcp/#migrating-databases-to-gcp","text":"","title":"Migrating databases to GCP"},{"location":"clusters/migrating-databases-to-gcp/#migrating-databases-to-gcp-postgresql","text":"Suggested patterns for moving on-prem databases to GCP postgreSQL. Disclaimer: These are options for migrations to GCP postgreSQL. Others may work better for your team.","title":"Migrating databases to GCP postgreSQL"},{"location":"clusters/migrating-databases-to-gcp/#prerequisites","text":"The team needs to update their ROS and PVK analysis to migrate to GCP. Refer to the ROS and PVK section under Google Cloud Platform clusters . See database creation in GCP in Google Cloud Platform persistence .","title":"Prerequisites"},{"location":"clusters/migrating-databases-to-gcp/#migration-paths-available","text":"","title":"Migration paths available"},{"location":"clusters/migrating-databases-to-gcp/#from-on-premise-oracle","text":"","title":"From on-premise Oracle"},{"location":"clusters/migrating-databases-to-gcp/#replication-migration-using-migration-application","text":"Create a simple migration application that supports writing data to the new database. Using requests sent to this application you can populate the new postgreSQL database. Rewrite the oracle DDL scripts to postgreSQL. If your oracle database contains specific oracle procedures or functions, that do not exist in postgreSQL, they will have to be recreated in some other way. There are tools available to help ease this rewrite, for example ora2pg( http://ora2pg.darold.net/start.html ). Create the postgreSQL database in GCP and start deploy the application to GCP with the empty database and let flyway (or other database versioning software) create the DDLs. Create migration app as a container in the same pod as the database application (this is to avoid permission issues using the same database). This migration application only handles the data transfer from the oracle database to postgreSQL in GCP. Examples: PAM Stillingsregistrering API Migration (documentation and code) PAM AD Migration (no documentation, just code) Rekrutteringsbistand migration (not using JPA for easier code and less memory-intensive data handling, but as-is only suitable on-premise migration) Trigger migration from command line (or use another form of trigger) and read the data from a feed or kafka. Pros: No downtime Live synchronization between on-premise and GCP Migration controlled entirely by team Migration can be stopped and restarted at any moment Cons: Can be slow if large amounts of data are to be transferred, if this is the case use kafka for the streaming process instead Can be tricky for complex databases Note This procedure is also valid for on-premise postgreSQL migration, and even simpler as no rewrite is necessary.","title":"Replication migration using migration application"},{"location":"clusters/migrating-databases-to-gcp/#from-on-premise-postgresql","text":"","title":"From on-premise postgreSQL"},{"location":"clusters/migrating-databases-to-gcp/#migration-using-pg_dump","text":"This method is suitable for applications that can have the database in read-only or application that allow for some downtime. It requires that the database instance and DDLs are created up front (i.e. deploy your application in GCP and let flyway create DDLs): Use docker container image with psql and cloudsdk: GCP migration image . This image let you do all the following actions from one place. Deploy the pod into on-premise cluster that can connect to the database kubectl apply -f https://raw.githubusercontent.com/navikt/gcp-migrering/main/gcloud.yml exec into that pod kubectl exec -it gcloud /bin/bash Log in to gcloud with your own NAV-account gcloud auth login Configure the project id (find project id with gcloud projects list --filter team ) gcloud config set project <project id> Create a GCP bucket. You will need the roles/storage.admin IAM role for the required operations on the bucket. gsutil mb -l europe-north1 gs://<bucket name> Find the GCP service account e-mail (the instance id is specified in your nais.yaml file) gcloud sql instances describe <CloudSQL instance id> | yq r - serviceAccountEmailAddress Set the objectAdmin role for the bucket (with the previous e-mail) gsutil iam ch serviceAccount:<GCP service account e-mail>:objectAdmin gs://<bucket name>/ Use pg_dump to create the dump file. Notes: Make sure that you stop writes to database before running pg_dump . Get a database user from Vault . If the database in GCP already has the flyway_schema_history table, you might want to exclude the equivalent table in the dump by using the --exclude-table=flyway_schema_history option. pg_dump \\ -h <postgreSQL on-premise host name> \\ -d <database instance name> \\ -U <database user name to connect with> \\ --format = plain --no-owner --no-acl --data-only -Z 9 > dump.sql.gz Copy the dump file to GCP bucket gsutil -o GSUtil:parallel_composite_upload_threshold = 150M -h \"Content-Type:application/x-gzip\" cp dump.sql.gz gs://<bucket name>/ Import the dump into the GCP postgreSQL database. Notes: You need the roles/cloudsql.admin IAM role in order to perform the import. The user in the command below should be a GCP SQL Instance user. If the GCP Postgres database has any existing tables or sequences, make sure that the user has all required grants for these. gcloud sql import sql <Cloud SQL instance id> gs://<bucket name>/dump.sql.gz \\ --database = <database instance name> \\ --user = <database instance user name> Verify that the application is behaving as expected and that the data in the new database is correct. Finally we need to switch loadbalancer to route to the GCP application instead of the on-premise equivalent. Delete the bucket in GCP after migration is complete gsutil -m rm -r gs://<bucket name> gsutil rb gs://<bucket name> Pros: Easy and relatively fast migration path No need for separate migration application or streams to populate database Cons: Requires downtime for the application, or at least no writes to database Requires node with access to on-premise database and GCP buckets","title":"Migration using pg_dump"},{"location":"clusters/migrating-databases-to-gcp/#replication-migration-using-migration-application_1","text":"Same procedure as for Oracle.","title":"Replication migration using migration application"},{"location":"clusters/migrating-databases-to-gcp/#replication-migration-using-pgbouncer","text":"Not available as of now.","title":"Replication migration using pgbouncer"},{"location":"clusters/migrating-to-gcp/","text":"Migrating to GCP \u00a7 Why migrate our application(s)? \u00a7 Access to self-service Google-managed buckets and Postgres databases . Access to Google Cloud features. Zero Trust security model instead of FSS/SBS zone model. Built-in call tracing similar to AppDynamics. Cost efficient and future proof. Prerequisites \u00a7 The team needs to update their ROS and PVK analysis to migrate to GCP. Refer to Google Cloud Platform's ROS and PVK section . Read this roles and responsibilites Basic setup \u00a7 Follow the Getting started's Access from laptop instructions, and make sure to pay attention to the GCP section. Security \u00a7 Our GCP clusters use a zero trust security model, implying that the application must specify both incoming and outgoing connections in order to receive or send traffic at all. This is expressed using the access policy spec . The access policy also enables zone traversal and cross-cluster communication. This must be implemented in both applications, by using and accepting tokens from TokenX or AAD . Deploy \u00a7 The same deployment mechanism is leveraged for both on-premise and GCP K8s clusters. See deployment section of the documentation for how to leverage the NAIS deploy tool . Ingress \u00a7 See GCP clusters . Privacy \u00a7 Google is cleared to be a data processor for personally identifiable information (PII) at NAV. However, before your team moves any applications or data to GCP the following steps should be taken: Verify that you have a valid and up-to-date PVK for your application. This document should be tech stack agnostic and as such does not need to be changed to reflect the move to GCP. If the application stores any data in GCP, update Behandlingskatalogen to reflect that Google is a data processor. ROS \u00a7 The ROS analysis for the team's applications need to be updated to reflect any changes in platform components used. For example, if your team has any specific measures implemented to mitigate risks related to \"Kode 6 / 7 users\", you should consider if these measures still apply on the new infrastructure or if you want to initiate any additional measures. When updating the ROS, please be aware that the GCP components you are most likely to use have already undergone risk assessment by the nais team and that you can refer to these ROS documents in your own risk assessment process. Roles and responsibilites \u00a7 As with applications in our on-premises clusters, the operation, security and integrity of any application is the responsibility of the team that owns that particular application. Conversely, it is the responsiblity of the nais platform team to handle the operation, security and integrity of the nais application platform and associated components. At GCP, Google is responsible for operating infrastructure underlying the nais platform, as well as any cloud services not consumed through the nais abstraction layer. Service exceptions reported by either Google or the nais team will be announced in the #nais slack channel. If your application stores personally identifiable information in any GCP data store, Google is effectively a data processor (\"databehandler\") for this data, and your documentation needs to reflect this fact. FAQ \u00a7 What do we have to change? \u00a7 Answer Cluster name: All references to cluster name. (Logs, grafana, deploy, etc.) Secrets: are now stored as native secrets in the cluster, rather than externally in Vault. Namespace: If your application is in the default namespace, you will have to move to team namespace Storage: Use GCS-buckets instead of s3 in GCP. Buckets, and access to them, are expressed in your application manifest Ingress: There are some domains that are available both on-prem and in GCP, but some differ, make sure to verify before you move. Postgres: A new database (and access to it) is automatically configured when expressing sqlInstance in your application manifest We're currently investigating the possibility of using on-prem databases during a migration window. PVK: Update your existing PVK to include cloud See this table for the differences between GCP and on-premises, and which that may apply to your application. What should we change? \u00a7 Answer Use TokenX instead of API-GW. If using automatically configured buckets or postgres , use Google APIs What do we not need to change? \u00a7 Answer You do not have to make any changes to your application code. Ingresses work the same way, although some domains overlap and others are exclusive. Logging, secure logging, metrics and alerts work the same way. What can we do now to ease migration to GCP later? \u00a7 Answer Make sure your PVK is up to date. Deploy your application to your team's namespace instead of default , as this is not available in GCP. Use a token auth flow between your applications. Either TokenX , AAD on-behalf-of or AAD client_credentials flow depending on your use case. This allows for a more seamless migration of your applications. E.g. if you have two apps in FSS, you can migrate one without the other. What about PVK? \u00a7 Answer A PVK is not a unique requirement for GCP, so all applications should already have one. See about security and privacy when using platform services for details How do we migrate our database? \u00a7 Answer See Migrating databases to GCP . Why is there no Vault in GCP? \u00a7 Answer There is native functionality in GCP that overlap with many of the use cases that Vault have covered on-prem. Using these mechanisms removes the need to deal with these secrets at all. Introducing team namespaces allows the teams to manage their own secrets in their own namespaces without the need for IAC and manual routines. For other secrets that are not used by the application during runtime, you can use the Secret Manager in each team's GCP project. How do we migrate from Vault to Secrets Manager? \u00a7 Answer See the Secrets Manager documentation . How do we migrate from filestorage to buckets? \u00a7 Answer Add a bucket to your application spec Copy the data from the filestore using s3cmd to the bucket using gsutil What are the plans for cloud migration in NAV? \u00a7 Answer We aim to shut down both sbs clusters by summer 2021 NAVs strategic goal is to shut off all on-prem datacenters by the end of 2023 What can we do in our GCP project? \u00a7 Answer The teams GCP projects are primarily used for automatically generated resources (buckets and postgres). We're working on extending the service offering. However, additional access may be granted if required by the team How long does it take to migrate? \u00a7 Answer A minimal application without any external requirements only have to change a single configuration parameter when deploying and have migrated their application in 5 minutes. See this table for the differences between GCP and on-premises, and which that may apply to your application. We have personally identifiable and/or sensitive data in our application, and we heard about the Privacy Shield invalidation. Can we still use GCP? \u00a7 Answer Yes. NAV's evaluation of our Data Processor Agreement with Google post-Schrems II is that it still protects us and is valid for use given that data is stored and processed in data centers located within the EU/EEA . If your team uses resources provisioned through NAIS, this is guaranteed by the nais team. If your team uses any other GCP services the team is responsible for ensuring that only resources within EU/EES are used (as well as for evaluating the risk of using these services). See Laws and regulations/Application PVK for details. How do I reach an application found on-premises from my application in GCP? \u00a7 Answer The application on-premises must fulfill the following requirements: Be secured with OAuth 2.0 . That is, either: a. TokenX , or b. Azure AD Exposed to GCP using a special ingress: https://<app>.dev-fss-pub.nais.io https://<app>.prod-fss-pub.nais.io Create a pull request at https://github.com/navikt/bigip-iac/tree/main/pub-fss add these ingresses which allows them be exposed to GCP. The application on-premises must then: Add the ingress created above to the list of ingresses: spec : ingresses : - https://<app>.<dev|prod>-fss-pub.nais.io The application in GCP must then: Add the above hosts to their outbound external access policies : spec : accessPolicy : outbound : external : - host : <app>.<dev|prod>-fss-pub.nais.io How do I reach an application found on GCP from my application on-premises? \u00a7 Answer The application in GCP must be exposed on a matching ingress : ingress reachable from zone <app>.dev.intern.nav.no dev-fss <app>.dev.nav.no dev-sbs <app>.intern.nav.no prod-fss <app>.nav.no prod-sbs The application on-premises should not have to use webproxy to reach these ingresses. GCP compared to on-premises \u00a7 Feature on-prem gcp Comment Deploy \u2714\ufe0f \u2714\ufe0f different clustername when deploying Logging \u2714\ufe0f \u2714\ufe0f different clustername in logs.adeo.no Metrics \u2714\ufe0f \u2714\ufe0f same mechanism, different datasource Nais app dashboard \u2714\ufe0f \u2714\ufe0f new and improved in GCP Alerts \u2714\ufe0f \u2714\ufe0f identical Secure logs \u2714\ufe0f \u2714\ufe0f different clustername in logs.adeo.no Kafka \u2714\ufe0f \u2714\ufe0f identical Secrets Vault Secret manager Team namespaces \u2714\ufe0f \u2714\ufe0f Shared namespaces \u2714\ufe0f \u2716\ufe0f Default namespace not available for teams in GCP Health checks \u2714\ufe0f \u2714\ufe0f identical Ingress \u2714\ufe0f \u2714\ufe0f see GCP and on-premises for available domains Storage Ceph Buckets Postgres \u2714\ufe0f (IAC) \u2714\ufe0f (self-service) Laptop access \u2714\ufe0f \u2714\ufe0f domain: dev.intern.nav.no (if migrating from SBS) \u2714\ufe0f (IAC) \u2714\ufe0f (Automatic) Wildcard DNS points to GCP load balancer Access to FSS services (if migrating from SBS) \u2714\ufe0f \u2714\ufe0f Identical (either API-gw or TokenX . May require a proxy app, see FAQ for details. OpenAM (ESSO) \u2714\ufe0f \u2714\ufe0f OpenAM is available for existing application, but it is EOL. We recommend migrating to TokenX NAV truststore \u2714\ufe0f \u2714\ufe0f PVK required \u2714\ufe0f \u2714\ufe0f amend to cover storage in cloud Security Zone Model zero-trust","title":"Migrating to GCP"},{"location":"clusters/migrating-to-gcp/#migrating-to-gcp","text":"","title":"Migrating to GCP"},{"location":"clusters/migrating-to-gcp/#why-migrate-our-applications","text":"Access to self-service Google-managed buckets and Postgres databases . Access to Google Cloud features. Zero Trust security model instead of FSS/SBS zone model. Built-in call tracing similar to AppDynamics. Cost efficient and future proof.","title":"Why migrate our application(s)?"},{"location":"clusters/migrating-to-gcp/#prerequisites","text":"The team needs to update their ROS and PVK analysis to migrate to GCP. Refer to Google Cloud Platform's ROS and PVK section . Read this roles and responsibilites","title":"Prerequisites"},{"location":"clusters/migrating-to-gcp/#basic-setup","text":"Follow the Getting started's Access from laptop instructions, and make sure to pay attention to the GCP section.","title":"Basic setup"},{"location":"clusters/migrating-to-gcp/#security","text":"Our GCP clusters use a zero trust security model, implying that the application must specify both incoming and outgoing connections in order to receive or send traffic at all. This is expressed using the access policy spec . The access policy also enables zone traversal and cross-cluster communication. This must be implemented in both applications, by using and accepting tokens from TokenX or AAD .","title":"Security"},{"location":"clusters/migrating-to-gcp/#deploy","text":"The same deployment mechanism is leveraged for both on-premise and GCP K8s clusters. See deployment section of the documentation for how to leverage the NAIS deploy tool .","title":"Deploy"},{"location":"clusters/migrating-to-gcp/#ingress","text":"See GCP clusters .","title":"Ingress"},{"location":"clusters/migrating-to-gcp/#privacy","text":"Google is cleared to be a data processor for personally identifiable information (PII) at NAV. However, before your team moves any applications or data to GCP the following steps should be taken: Verify that you have a valid and up-to-date PVK for your application. This document should be tech stack agnostic and as such does not need to be changed to reflect the move to GCP. If the application stores any data in GCP, update Behandlingskatalogen to reflect that Google is a data processor.","title":"Privacy"},{"location":"clusters/migrating-to-gcp/#ros","text":"The ROS analysis for the team's applications need to be updated to reflect any changes in platform components used. For example, if your team has any specific measures implemented to mitigate risks related to \"Kode 6 / 7 users\", you should consider if these measures still apply on the new infrastructure or if you want to initiate any additional measures. When updating the ROS, please be aware that the GCP components you are most likely to use have already undergone risk assessment by the nais team and that you can refer to these ROS documents in your own risk assessment process.","title":"ROS"},{"location":"clusters/migrating-to-gcp/#roles-and-responsibilites","text":"As with applications in our on-premises clusters, the operation, security and integrity of any application is the responsibility of the team that owns that particular application. Conversely, it is the responsiblity of the nais platform team to handle the operation, security and integrity of the nais application platform and associated components. At GCP, Google is responsible for operating infrastructure underlying the nais platform, as well as any cloud services not consumed through the nais abstraction layer. Service exceptions reported by either Google or the nais team will be announced in the #nais slack channel. If your application stores personally identifiable information in any GCP data store, Google is effectively a data processor (\"databehandler\") for this data, and your documentation needs to reflect this fact.","title":"Roles and responsibilites"},{"location":"clusters/migrating-to-gcp/#faq","text":"","title":"FAQ"},{"location":"clusters/migrating-to-gcp/#what-do-we-have-to-change","text":"Answer Cluster name: All references to cluster name. (Logs, grafana, deploy, etc.) Secrets: are now stored as native secrets in the cluster, rather than externally in Vault. Namespace: If your application is in the default namespace, you will have to move to team namespace Storage: Use GCS-buckets instead of s3 in GCP. Buckets, and access to them, are expressed in your application manifest Ingress: There are some domains that are available both on-prem and in GCP, but some differ, make sure to verify before you move. Postgres: A new database (and access to it) is automatically configured when expressing sqlInstance in your application manifest We're currently investigating the possibility of using on-prem databases during a migration window. PVK: Update your existing PVK to include cloud See this table for the differences between GCP and on-premises, and which that may apply to your application.","title":"What do we have to change?"},{"location":"clusters/migrating-to-gcp/#what-should-we-change","text":"Answer Use TokenX instead of API-GW. If using automatically configured buckets or postgres , use Google APIs","title":"What should we change?"},{"location":"clusters/migrating-to-gcp/#what-do-we-not-need-to-change","text":"Answer You do not have to make any changes to your application code. Ingresses work the same way, although some domains overlap and others are exclusive. Logging, secure logging, metrics and alerts work the same way.","title":"What do we not need to change?"},{"location":"clusters/migrating-to-gcp/#what-can-we-do-now-to-ease-migration-to-gcp-later","text":"Answer Make sure your PVK is up to date. Deploy your application to your team's namespace instead of default , as this is not available in GCP. Use a token auth flow between your applications. Either TokenX , AAD on-behalf-of or AAD client_credentials flow depending on your use case. This allows for a more seamless migration of your applications. E.g. if you have two apps in FSS, you can migrate one without the other.","title":"What can we do now to ease migration to GCP later?"},{"location":"clusters/migrating-to-gcp/#what-about-pvk","text":"Answer A PVK is not a unique requirement for GCP, so all applications should already have one. See about security and privacy when using platform services for details","title":"What about PVK?"},{"location":"clusters/migrating-to-gcp/#how-do-we-migrate-our-database","text":"Answer See Migrating databases to GCP .","title":"How do we migrate our database?"},{"location":"clusters/migrating-to-gcp/#why-is-there-no-vault-in-gcp","text":"Answer There is native functionality in GCP that overlap with many of the use cases that Vault have covered on-prem. Using these mechanisms removes the need to deal with these secrets at all. Introducing team namespaces allows the teams to manage their own secrets in their own namespaces without the need for IAC and manual routines. For other secrets that are not used by the application during runtime, you can use the Secret Manager in each team's GCP project.","title":"Why is there no Vault in GCP?"},{"location":"clusters/migrating-to-gcp/#how-do-we-migrate-from-vault-to-secrets-manager","text":"Answer See the Secrets Manager documentation .","title":"How do we migrate from Vault to Secrets Manager?"},{"location":"clusters/migrating-to-gcp/#how-do-we-migrate-from-filestorage-to-buckets","text":"Answer Add a bucket to your application spec Copy the data from the filestore using s3cmd to the bucket using gsutil","title":"How do we migrate from filestorage to buckets?"},{"location":"clusters/migrating-to-gcp/#what-are-the-plans-for-cloud-migration-in-nav","text":"Answer We aim to shut down both sbs clusters by summer 2021 NAVs strategic goal is to shut off all on-prem datacenters by the end of 2023","title":"What are the plans for cloud migration in NAV?"},{"location":"clusters/migrating-to-gcp/#what-can-we-do-in-our-gcp-project","text":"Answer The teams GCP projects are primarily used for automatically generated resources (buckets and postgres). We're working on extending the service offering. However, additional access may be granted if required by the team","title":"What can we do in our GCP project?"},{"location":"clusters/migrating-to-gcp/#how-long-does-it-take-to-migrate","text":"Answer A minimal application without any external requirements only have to change a single configuration parameter when deploying and have migrated their application in 5 minutes. See this table for the differences between GCP and on-premises, and which that may apply to your application.","title":"How long does it take to migrate?"},{"location":"clusters/migrating-to-gcp/#we-have-personally-identifiable-andor-sensitive-data-in-our-application-and-we-heard-about-the-privacy-shield-invalidation-can-we-still-use-gcp","text":"Answer Yes. NAV's evaluation of our Data Processor Agreement with Google post-Schrems II is that it still protects us and is valid for use given that data is stored and processed in data centers located within the EU/EEA . If your team uses resources provisioned through NAIS, this is guaranteed by the nais team. If your team uses any other GCP services the team is responsible for ensuring that only resources within EU/EES are used (as well as for evaluating the risk of using these services). See Laws and regulations/Application PVK for details.","title":"We have personally identifiable and/or sensitive data in our application, and we heard about the Privacy Shield invalidation. Can we still use GCP?"},{"location":"clusters/migrating-to-gcp/#how-do-i-reach-an-application-found-on-premises-from-my-application-in-gcp","text":"Answer The application on-premises must fulfill the following requirements: Be secured with OAuth 2.0 . That is, either: a. TokenX , or b. Azure AD Exposed to GCP using a special ingress: https://<app>.dev-fss-pub.nais.io https://<app>.prod-fss-pub.nais.io Create a pull request at https://github.com/navikt/bigip-iac/tree/main/pub-fss add these ingresses which allows them be exposed to GCP. The application on-premises must then: Add the ingress created above to the list of ingresses: spec : ingresses : - https://<app>.<dev|prod>-fss-pub.nais.io The application in GCP must then: Add the above hosts to their outbound external access policies : spec : accessPolicy : outbound : external : - host : <app>.<dev|prod>-fss-pub.nais.io","title":"How do I reach an application found on-premises from my application in GCP?"},{"location":"clusters/migrating-to-gcp/#how-do-i-reach-an-application-found-on-gcp-from-my-application-on-premises","text":"Answer The application in GCP must be exposed on a matching ingress : ingress reachable from zone <app>.dev.intern.nav.no dev-fss <app>.dev.nav.no dev-sbs <app>.intern.nav.no prod-fss <app>.nav.no prod-sbs The application on-premises should not have to use webproxy to reach these ingresses.","title":"How do I reach an application found on GCP from my application on-premises?"},{"location":"clusters/migrating-to-gcp/#gcp-compared-to-on-premises","text":"Feature on-prem gcp Comment Deploy \u2714\ufe0f \u2714\ufe0f different clustername when deploying Logging \u2714\ufe0f \u2714\ufe0f different clustername in logs.adeo.no Metrics \u2714\ufe0f \u2714\ufe0f same mechanism, different datasource Nais app dashboard \u2714\ufe0f \u2714\ufe0f new and improved in GCP Alerts \u2714\ufe0f \u2714\ufe0f identical Secure logs \u2714\ufe0f \u2714\ufe0f different clustername in logs.adeo.no Kafka \u2714\ufe0f \u2714\ufe0f identical Secrets Vault Secret manager Team namespaces \u2714\ufe0f \u2714\ufe0f Shared namespaces \u2714\ufe0f \u2716\ufe0f Default namespace not available for teams in GCP Health checks \u2714\ufe0f \u2714\ufe0f identical Ingress \u2714\ufe0f \u2714\ufe0f see GCP and on-premises for available domains Storage Ceph Buckets Postgres \u2714\ufe0f (IAC) \u2714\ufe0f (self-service) Laptop access \u2714\ufe0f \u2714\ufe0f domain: dev.intern.nav.no (if migrating from SBS) \u2714\ufe0f (IAC) \u2714\ufe0f (Automatic) Wildcard DNS points to GCP load balancer Access to FSS services (if migrating from SBS) \u2714\ufe0f \u2714\ufe0f Identical (either API-gw or TokenX . May require a proxy app, see FAQ for details. OpenAM (ESSO) \u2714\ufe0f \u2714\ufe0f OpenAM is available for existing application, but it is EOL. We recommend migrating to TokenX NAV truststore \u2714\ufe0f \u2714\ufe0f PVK required \u2714\ufe0f \u2714\ufe0f amend to cover storage in cloud Security Zone Model zero-trust","title":"GCP compared to on-premises"},{"location":"clusters/on-premises/","text":"On-premises \u00a7 The on-premise Kubernetes clusters are split across two zones and two environments: cluster zone environment dev-fss fagsystemsonen development dev-sbs selvbetjeningssonen development prod-fss fagsystemsonen production prod-sbs selvbetjeningssonen production Accessing the application \u00a7 Access is controlled in part by ingresses, which define where your application will be exposed as a HTTP endpoint. You can control where your application is reachable from by selecting the appropriate ingress domain. Warning Make sure you understand where you expose your application, taking into account the state of your application, what kind of data it exposes and how it is secured. If in doubt, ask in #nais or someone on the NAIS team. You can control from where you application is reachable by selecting the appropriate ingress domain. dev-fss \u00a7 domain accessible from description dev.intern.nav.no naisdevice development ingress for dev internal applications (supersedes dev.adeo.no). Also available in dev-gcp, use this to ease migration dev.adeo.no naisdevice deprecated development ingress for adeo.no applications (superceded by dev.intern.nav.no) intern.dev.adeo.no internal network only development ingress for adeo.no applications that should not be reached from naisdevice dev-fss.nais.io naisdevice reserved for platform services dev-fss-pub.nais.io GCP Exposing applications to GCP requires a manual entry in BigIP as well. See sonekryssing . nais.preprod.local vdi deprecated , use .dev.intern.nav.no instead dev-sbs \u00a7 domain accessible from description dev.nav.no naisdevice development ingress for nav.no applications intern.dev.nav.no internal network only development ingress for nav.no applications dev-sbs.nais.io naisdevice reserved for platform services nais.oera-q.local vdi deprecated , use {intern,}.dev.nav.no instead prod-fss \u00a7 domain accessible from description intern.nav.no naisdevice ingress for internal applications (supersedes nais.adeo.no). Also available in prod-gcp, use this to ease migration nais.adeo.no vdi automatically configured prod-fss.nais.io naisdevice reserved for platform services prod-fss-pub.nais.io GCP Exposing applications to GCP requires a manual entry in BigIP as well. See sonekryssing . prod-sbs \u00a7 domain accessible from description nav.no internet currently manually configured by #tech-sikkerhet nais.oera.no vdi automatically configured. Typically used by backend/admin apps not exposed to end-users tjenester.nav.no internet context root based routing on format tjenester.nav.no/<appname> . prod-sbs.nais.io naisdevice reserved for platform services More info about how DNS is configured for these domains can be found here","title":"On-premises"},{"location":"clusters/on-premises/#on-premises","text":"The on-premise Kubernetes clusters are split across two zones and two environments: cluster zone environment dev-fss fagsystemsonen development dev-sbs selvbetjeningssonen development prod-fss fagsystemsonen production prod-sbs selvbetjeningssonen production","title":"On-premises"},{"location":"clusters/on-premises/#accessing-the-application","text":"Access is controlled in part by ingresses, which define where your application will be exposed as a HTTP endpoint. You can control where your application is reachable from by selecting the appropriate ingress domain. Warning Make sure you understand where you expose your application, taking into account the state of your application, what kind of data it exposes and how it is secured. If in doubt, ask in #nais or someone on the NAIS team. You can control from where you application is reachable by selecting the appropriate ingress domain.","title":"Accessing the application"},{"location":"clusters/on-premises/#dev-fss","text":"domain accessible from description dev.intern.nav.no naisdevice development ingress for dev internal applications (supersedes dev.adeo.no). Also available in dev-gcp, use this to ease migration dev.adeo.no naisdevice deprecated development ingress for adeo.no applications (superceded by dev.intern.nav.no) intern.dev.adeo.no internal network only development ingress for adeo.no applications that should not be reached from naisdevice dev-fss.nais.io naisdevice reserved for platform services dev-fss-pub.nais.io GCP Exposing applications to GCP requires a manual entry in BigIP as well. See sonekryssing . nais.preprod.local vdi deprecated , use .dev.intern.nav.no instead","title":"dev-fss"},{"location":"clusters/on-premises/#dev-sbs","text":"domain accessible from description dev.nav.no naisdevice development ingress for nav.no applications intern.dev.nav.no internal network only development ingress for nav.no applications dev-sbs.nais.io naisdevice reserved for platform services nais.oera-q.local vdi deprecated , use {intern,}.dev.nav.no instead","title":"dev-sbs"},{"location":"clusters/on-premises/#prod-fss","text":"domain accessible from description intern.nav.no naisdevice ingress for internal applications (supersedes nais.adeo.no). Also available in prod-gcp, use this to ease migration nais.adeo.no vdi automatically configured prod-fss.nais.io naisdevice reserved for platform services prod-fss-pub.nais.io GCP Exposing applications to GCP requires a manual entry in BigIP as well. See sonekryssing .","title":"prod-fss"},{"location":"clusters/on-premises/#prod-sbs","text":"domain accessible from description nav.no internet currently manually configured by #tech-sikkerhet nais.oera.no vdi automatically configured. Typically used by backend/admin apps not exposed to end-users tjenester.nav.no internet context root based routing on format tjenester.nav.no/<appname> . prod-sbs.nais.io naisdevice reserved for platform services More info about how DNS is configured for these domains can be found here","title":"prod-sbs"},{"location":"clusters/service-discovery/","text":"Service Discovery in Kubernetes \u00a7 Applications deployed to Kubernetes are exposed through a service. This is an address that allows for direct communication within a Kubernetes cluster without having to go through an external ingress or load balancer. Services available can be viewed with kubectl get service or shorthand kubectl get svc . The service name is the same in both dev and prod clusters. This allows for simpler configuration. Google Cloud Platform \u00a7 Warning Ensure that you've set up proper access policies for your applications. The full hostname of a service on GCP follows this format: http://<service-name>.<namespace>.svc.cluster.local On-prem \u00a7 The full hostname of a service on-prem follows this format: http://<service-name>.<namespace>.svc.nais.local Short names \u00a7 You often won't need to use the full hostname to contact another service. If you\u2019re addressing a service in the same namespace, you can use just the service name to contact it: http://<another-service> If the service exists in a different namespace, you must add the appropriate namespace: http://<another-service>.<another-namespace> Note for on-prem If your application has webproxy enabled, you should use the full hostname for all service discovery calls. This is to ensure that your application does not attempt to perform these in-cluster calls through the proxy, as the environment variable NO_PROXY includes *.local .","title":"Service discovery"},{"location":"clusters/service-discovery/#service-discovery-in-kubernetes","text":"Applications deployed to Kubernetes are exposed through a service. This is an address that allows for direct communication within a Kubernetes cluster without having to go through an external ingress or load balancer. Services available can be viewed with kubectl get service or shorthand kubectl get svc . The service name is the same in both dev and prod clusters. This allows for simpler configuration.","title":"Service Discovery in Kubernetes"},{"location":"clusters/service-discovery/#google-cloud-platform","text":"Warning Ensure that you've set up proper access policies for your applications. The full hostname of a service on GCP follows this format: http://<service-name>.<namespace>.svc.cluster.local","title":"Google Cloud Platform"},{"location":"clusters/service-discovery/#on-prem","text":"The full hostname of a service on-prem follows this format: http://<service-name>.<namespace>.svc.nais.local","title":"On-prem"},{"location":"clusters/service-discovery/#short-names","text":"You often won't need to use the full hostname to contact another service. If you\u2019re addressing a service in the same namespace, you can use just the service name to contact it: http://<another-service> If the service exists in a different namespace, you must add the appropriate namespace: http://<another-service>.<another-namespace> Note for on-prem If your application has webproxy enabled, you should use the full hostname for all service discovery calls. This is to ensure that your application does not attempt to perform these in-cluster calls through the proxy, as the environment variable NO_PROXY includes *.local .","title":"Short names"},{"location":"clusters/team-namespaces/","text":"Team namespaces \u00a7 A namespace per team is automatically created in every NAIS cluster both on-prem and in GCP. Namespaces are created based on the contents of teams.yaml in navikt/teams repo . The namespace has the same name as the name field i teams.yaml Using team namespaces instead of shared namespaces has several advantages: Team members have full admin access in that namespace. This includes kubectl commands like scale , port-forward , exec etc. Support for Kubernetes native secrets as an alternative to Vault. People from other teams cannot read native secrets in your team's namespace People from other teams does not have access to your team's namespace. This prevents accidental changes or removal of Kubernetes resources used by your team. Google Cloud Platform (GCP) only supports team namespaces. Migrating your application to a team namespace now makes it easier to move from on-prem to GCP later. No longer forces use of the nais.io/Application abstraction. If nais.io/Application doesn't support your requirements, or you simply prefer handling these resources yourself, you are free to do so. On-prem migration to team namespaces \u00a7 Migrating an application to a team namespace is done by changing the .metadata.namespace field in the nais.yaml file and redeploying the app. For example, if you're migrating my-app from the default namespace to a namespace called my-team , your yaml previously looked like this: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : my-app namespace : default it should instead look like this: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : my-app namespace : my-team Reminder Remove the application from the old namespace when you have finished migrating. kubectl delete app <app-name> -n <old-namespace> If your application uses Vault, make sure the application is registered under the correct team in Vault IAC . There are a few more steps to consider if you are integrating with other rest- or webservices. Integration via ingress or BigIP \u00a7 If you are calling other services through a Kubernetes ingress or BigIP such as: https://myapp.nais.adeo.no https://myapp.adeo.no https://app.adeo.no/myapp https://modapp.adeo.no/myapp no changes are required. Service calls via api-gateway or service-gateway will also work without any changes. Integration via Kubernetes service discovery \u00a7 When migrating your application from either the default namespace or from a environment namespace (e.g. t1, q1 etc.), the service URL will change and consequently break your consumer integrations. This can be mitigated during a migration phase by creating an ExternalName -service in the namespace you are migrating from, that points to the new service in the team namespace. Example \u00a7 When migrating your application my-app from the default namespace to my-teamnamespace , you can create the following Service in the default namespace to keep the previous service URL, and allow for a seamless migration. apiVersion : v1 kind : Service metadata : name : my-app namespace : default spec : type : ExternalName externalName : my-app.my-teamnamespace.svc.nais.local This will create a CNAME DNS record that will resolve my-app.default.svc.nais.local as my-app.my-teamnamespace.svc.nais.local Migrating an application from default to team namespace with minimal downtime. \u00a7 First thing you need to do, is to deploy the application to your team's namespace. Create a file locally using the example above that points to the Service in your team namespace The commands below will replace the Service in default , delete the Ingress in default and create an ingress for the application in your team's namespace. kubectl replace -f service.yaml kubectl delete ingress -n default -l app = my-app kubectl -n my-teamnamespace patch app my-app -p '[{\"op\": \"remove\", \"path\": \"/status/synchronizationHash\"}]' --type = json","title":"Team namespaces"},{"location":"clusters/team-namespaces/#team-namespaces","text":"A namespace per team is automatically created in every NAIS cluster both on-prem and in GCP. Namespaces are created based on the contents of teams.yaml in navikt/teams repo . The namespace has the same name as the name field i teams.yaml Using team namespaces instead of shared namespaces has several advantages: Team members have full admin access in that namespace. This includes kubectl commands like scale , port-forward , exec etc. Support for Kubernetes native secrets as an alternative to Vault. People from other teams cannot read native secrets in your team's namespace People from other teams does not have access to your team's namespace. This prevents accidental changes or removal of Kubernetes resources used by your team. Google Cloud Platform (GCP) only supports team namespaces. Migrating your application to a team namespace now makes it easier to move from on-prem to GCP later. No longer forces use of the nais.io/Application abstraction. If nais.io/Application doesn't support your requirements, or you simply prefer handling these resources yourself, you are free to do so.","title":"Team namespaces"},{"location":"clusters/team-namespaces/#on-prem-migration-to-team-namespaces","text":"Migrating an application to a team namespace is done by changing the .metadata.namespace field in the nais.yaml file and redeploying the app. For example, if you're migrating my-app from the default namespace to a namespace called my-team , your yaml previously looked like this: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : my-app namespace : default it should instead look like this: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : my-app namespace : my-team Reminder Remove the application from the old namespace when you have finished migrating. kubectl delete app <app-name> -n <old-namespace> If your application uses Vault, make sure the application is registered under the correct team in Vault IAC . There are a few more steps to consider if you are integrating with other rest- or webservices.","title":"On-prem migration to team namespaces"},{"location":"clusters/team-namespaces/#integration-via-ingress-or-bigip","text":"If you are calling other services through a Kubernetes ingress or BigIP such as: https://myapp.nais.adeo.no https://myapp.adeo.no https://app.adeo.no/myapp https://modapp.adeo.no/myapp no changes are required. Service calls via api-gateway or service-gateway will also work without any changes.","title":"Integration via ingress or BigIP"},{"location":"clusters/team-namespaces/#integration-via-kubernetes-service-discovery","text":"When migrating your application from either the default namespace or from a environment namespace (e.g. t1, q1 etc.), the service URL will change and consequently break your consumer integrations. This can be mitigated during a migration phase by creating an ExternalName -service in the namespace you are migrating from, that points to the new service in the team namespace.","title":"Integration via Kubernetes service discovery"},{"location":"clusters/team-namespaces/#example","text":"When migrating your application my-app from the default namespace to my-teamnamespace , you can create the following Service in the default namespace to keep the previous service URL, and allow for a seamless migration. apiVersion : v1 kind : Service metadata : name : my-app namespace : default spec : type : ExternalName externalName : my-app.my-teamnamespace.svc.nais.local This will create a CNAME DNS record that will resolve my-app.default.svc.nais.local as my-app.my-teamnamespace.svc.nais.local","title":"Example"},{"location":"clusters/team-namespaces/#migrating-an-application-from-default-to-team-namespace-with-minimal-downtime","text":"First thing you need to do, is to deploy the application to your team's namespace. Create a file locally using the example above that points to the Service in your team namespace The commands below will replace the Service in default , delete the Ingress in default and create an ingress for the application in your team's namespace. kubectl replace -f service.yaml kubectl delete ingress -n default -l app = my-app kubectl -n my-teamnamespace patch app my-app -p '[{\"op\": \"remove\", \"path\": \"/status/synchronizationHash\"}]' --type = json","title":"Migrating an application from default to team namespace with minimal downtime."},{"location":"deployment/","text":"NAIS deploy \u00a7 This section will take you through the deployment of your application using NAIS deploy . NAIS deploy enables you to deploy your application from any continuous integration platform. Our primary supported platform is GitHub Actions, but you can also deploy from CircleCI, Travis CI, Jenkins, or other tools. If you experience any trouble along the way, please take a look at the troubleshooting page . How it works \u00a7 Your application is assumed to be present in the form of a Docker image when using the NAIS deploy tool . The NAIS deploy tool is used to create a deployment request. The Docker image will be deployed to Kubernetes, and the deploy tool will wait until your deployment is rolled out, gets an error, or a timeout occurs. Underway, deployment statuses are continually posted back to GitHub Deployment API . Deployment logs can be viewed on Kibana . The link to the logs will be provided by the deploy tool. Set it up \u00a7 Your application must have a repository on GitHub containing a nais.yaml and Dockerfile . Your GitHub team must have admin access on that repository. Your GitHub team's identifier must match the Kubernetes team label in your nais.yaml . There is an example file below. Retrieve your team API key . Save the key as a secret named NAIS_DEPLOY_APIKEY in your GitHub repository. Follow the guide below. When things break, see the help page . Deploy with GitHub Actions \u00a7 A GitHub Actions pipeline is called a Workflow . You can set up workflows by adding a YAML file to your application's Git repository. In this example, the workflow is set up in the file deploy.yaml . The workflow will build a Docker image and push it to GitHub Container Registry. Next, if the code was pushed to the master branch AND the build job succeeded, the application will be deployed to NAIS. Official GitHub documentation: Automating your workflow with GitHub Actions . Get started by creating the following structure in your application repository: myapplication/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 deploy.yaml \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 nais.yaml Add the example files below, then commit and push. This will trigger the workflow, and you can follow its progress under the Actions tab on your GitHub repository page. .github/workflows/deploy.yaml name : Build, push, and deploy on : [ push ] env : docker_image : ghcr.io/${{ github.repository }}:${{ github.sha }} jobs : build : name : Build and push Docker container runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : docker/login-action@v1 with : registry : ghcr.io username : ${{ github.actor }} password : ${{ secrets.GITHUB_TOKEN }} - uses : docker/build-push-action@v2 with : context : . push : true tags : ${{ env.docker_image }} deploy : name : Deploy to NAIS needs : build if : github.ref == 'refs/heads/master' runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : dev-gcp RESOURCE : nais.yaml VAR : image=${{ env.docker_image }} nais.yaml apiVersion : nais.io/v1alpha1 kind : Application metadata : name : myapplication namespace : myteam labels : team : myteam spec : image : {{ image }} #image: ghcr.io/navikt/myapplication:417dcaa2c839b9da72e0189e2cfdd4e90e9cc6fd # ^--- interpolated from the ${{ env.docker_image }} variable in the action In this nais.yaml file, {{ image }} will be replaced by the $docker_image environment variable set in the action. You can add more by using a comma separated list, or even put all your variables in a file; see action configuration below. Dockerfile FROM nginxinc/nginx-unprivileged You have to write your own Dockerfile, but if you're just trying this out, you can use the following example file. Action configuration \u00a7 Environment variable Default Description APIKEY (required) NAIS deploy API key. Obtained from https://deploy.nais.io . CLUSTER (required) Which NAIS cluster ( On-premises or GCP ) to deploy into. DRY_RUN false If true , run templating and validate input, but do not actually make any requests. ENVIRONMENT (auto-detect) The environment to be shown in GitHub Deployments. Defaults to CLUSTER:NAMESPACE for the resource to be deployed if not specified, otherwise falls back to CLUSTER if multiple namespaces exist in the given resources. OWNER (auto-detect) Owner of the repository making the request. PRINT_PAYLOAD false If true , print templated resources to standard output. QUIET false If true , suppress all informational messages. REF master (auto-detect) Commit reference of the deployment. Shown in GitHub's interface. REPOSITORY (auto-detect) Name of the repository making the request. RESOURCE (required) Comma-separated list of files containing Kubernetes resources. Must be JSON or YAML format. RETRY true Automatically retry deploying if deploy service is unavailable. TEAM (auto-detect) Team making the deployment. TIMEOUT 10m Time to wait for deployment completion, especially when using WAIT . VAR Comma-separated list of template variables in the form key=value . Will overwrite any identical template variable in the VARS file. VARS /dev/null File containing template variables. Will be interpolated with the $RESOURCE file. Must be JSON or YAML format. WAIT true Block until deployment has completed with either success , failure or error state. Note that OWNER and REPOSITORY corresponds to the two parts of a full repository identifier. If that name is navikt/myapplication , those two variables should be set to navikt and myapplication , respectively. Deploy with other CI \u00a7 You can still use NAIS deploy even if not using GitHub Actions. Our deployment command line tool supports all CI tools such as Jenkins, Travis or Circle. Use can either use a Docker image or one the binaries to make deployments. Docker usage docker run -it --rm -v $(pwd):/nais /navikt/deployment:v1 \\ /app/deploy \\ --apikey=\"$NAIS_DEPLOY_APIKEY\" \\ --cluster=\"$CLUSTER\" \\ --owner=\"$OWNER\" \\ --repository=\"$REPOSITORY\" \\ --resource=\"/nais/path/to/nais.yaml\" \\ --vars=\"/nais/path/to/resources\" \\ --wait=true \\ ; Here we use the current directory as a volume for the CLI, and we have to append /nais to the path to our manifest. So if our original manifest is under /home/cooluser/workspace/bestapp/nais.yaml , we then need to --resource=\"/nais/nais.yaml\" , and not only --resource=\"nais.yaml\" . CLI usage deploy \\ --apikey=\"$NAIS_DEPLOY_APIKEY\" \\ --cluster=\"$CLUSTER\" \\ --owner=\"$OWNER\" \\ --repository=\"$REPOSITORY\" \\ --resource=\"/path/to/nais.yaml\" \\ --vars=\"/path/to/resources\" \\ --wait=true Syntax: --apikey string NAIS Deploy API key. --cluster string NAIS cluster to deploy into. --deploy-server string URL to API server. (default \"https://deployment.prod-sbs.nais.io\") --dry-run Run templating, but don't actually make any requests. --environment string Environment for GitHub deployment. Auto-detected from nais.yaml if not specified. --owner string Owner of GitHub repository. (default \"navikt\") --print-payload Print templated resources to standard output. --quiet Suppress printing of informational messages except errors. --ref string Git commit hash, tag, or branch of the code being deployed. (default \"master\") --repository string Name of GitHub repository. --resource strings File with Kubernetes resource. Can be specified multiple times. --team string Team making the deployment. Auto-detected if possible. --var strings Template variable in the form KEY=VALUE. Can be specified multiple times. --vars string File containing template variables. --wait Block until deployment reaches final state (success, failure, error). All of these options can be set using environment variables, such as $APIKEY and $PRINT_PAYLOAD . Proxy server \u00a7 If you are running NAIS deploy from an internal Jenkins server you need to set up an HTTP proxy as deploy.nais.io is a public address. When using the CLI binary you can wrap steps in your pipeline with injected environment variables. stage('Deploy') { withEnv(['HTTPS_PROXY=http://webproxy-utvikler.nav.no:8088']) { ... } } When using NAIS deploy docker image, pass the environment variable to Docker run. sh \"docker run --env HTTPS_PROXY='http://webproxy-utvikler.nav.no:8088' ...\" ; Templating \u00a7 Templates use Handlebars 3.0 syntax. Both the template and variable file supports either YAML or JSON syntax. A practical example follows. Create a nais.yaml file: apiVersion : nais.io/v1alpha1 kind : Application metadata : name : {{ app }} namespace : {{ namespace }} labels : team : {{ team }} spec : image : {{ image }} ingresses : {{ #each ingresses as |url|}} - {{ url }} {{ /each }} Now, create a vars.yaml file containing variables for your deployment: app : myapplication namespace : myteam team : myteam image : ghcr.io/navikt/myapplication:latest ingresses : - https://myapplication.nav.no - https://tjenester.nav.no/myapplication Run the deploy tool to see the final results: $ deploy -- dry - run -- print - payload -- resource nais . yaml -- vars vars . yaml | jq \".resources[0]\" { \"apiVersion\" : \"nais.io/v1alpha1\" , \"kind\" : \"Application\" , \"metadata\" : { \"labels\" : { \"team\" : \"myteam\" }, \"name\" : \"myapplication\" , \"namespace\" : \"default\" }, \"spec\" : { \"image\" : \"ghcr.io/navikt/myapplication:417dcaa2c839b9da72e0189e2cfdd4e90e9cc6fd\" , \"ingresses\" : [ \"https://myapplication.nav.no\" , \"https://tjenester.nav.no/myapplication\" ] } } Escaping and raw resources \u00a7 If you do not specify the --vars or --var command-line flags, your resource will not be run through the templating engine, so these resources will not need templating. Handlebars content may be escaped by prefixing a mustache block with \\, such as: \\{{escaped}} Real-world example: apiVersion : nais.io/v1alpha1 kind : Alert metadata : name : {{ app }} labels : team : {{ team }} spec : alerts : - action : Se `kubectl describe pod \\{{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs \\{{ $labels.kubernetes_pod_name }}` for logger alert : {{ app }} -fails description : '\\{{ $labels.app }} er nede i \\{{ $labels.kubernetes_namespace }}' expr : up{app=~\"{{app}}\",job=\"kubernetes-pods\"} == 0 for : 2m severity : danger sla : respond within 1h, during office hours receivers : slack : channel : '#{{team}}' Will result in: deploy -- dry - run -- print - payload -- resource alert . yaml -- vars vars . yaml | jq . resources [ { \"apiVersion\" : \"nais.io/v1alpha1\" , \"kind\" : \"Alert\" , \"metadata\" : { \"labels\" : { \"team\" : \"myteam\" }, \"name\" : \"myapplication\" }, \"spec\" : { \"alerts\" : [ { \"action\" : \"Se `kubectl describe pod {{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }}` for logger\" , \"alert\" : \"myapplication-fails\" , \"description\" : \"{{ $labels.app }} er nede i {{ $labels.kubernetes_namespace }}\" , \"expr\" : \"up{app=~\\\"myapplication\\\",job=\\\"kubernetes-pods\\\"} == 0\" , \"for\" : \"2m\" , \"severity\" : \"danger\" , \"sla\" : \"respond within 1h, during office hours\" } ], \"receivers\" : { \"slack\" : { \"channel\" : \"#myteam\" } } } } ] PS: Templating will not be run if you do not use VARS and/or VAR , meaning \\{{ }} will not be handled by nais/deploy. Default environment variables \u00a7 These environment variables will be injected into your application container variable example source NAIS_APP_NAME myapp metadata.name from nais.yaml NAIS_NAMESPACE default metadata.namespace from nais.yaml NAIS_APP_IMAGE navikt/myapp:69 spec.image from nais.yaml NAIS_CLUSTER_NAME prod-fss naiserator runtime context NAIS_CLIENT_ID prod-fss:default:myapp concatenation of cluster, namespace and app name Environment variables for loading CA bundles into your application will also be injected: variable example source NAV_TRUSTSTORE_PATH /etc/ssl/certs/java/cacerts CA bundle containing the most commonly used CA root certificates NAV_TRUSTSTORE_PASSWORD xxxxx Password for the CA bundle Build status badge \u00a7 Use the following URL to create a small badge for your workflow/action. https://github.com/{github_id}/{repository}/workflows/{workflow_name}/badge.svg","title":"NAIS deploy"},{"location":"deployment/#nais-deploy","text":"This section will take you through the deployment of your application using NAIS deploy . NAIS deploy enables you to deploy your application from any continuous integration platform. Our primary supported platform is GitHub Actions, but you can also deploy from CircleCI, Travis CI, Jenkins, or other tools. If you experience any trouble along the way, please take a look at the troubleshooting page .","title":"NAIS deploy"},{"location":"deployment/#how-it-works","text":"Your application is assumed to be present in the form of a Docker image when using the NAIS deploy tool . The NAIS deploy tool is used to create a deployment request. The Docker image will be deployed to Kubernetes, and the deploy tool will wait until your deployment is rolled out, gets an error, or a timeout occurs. Underway, deployment statuses are continually posted back to GitHub Deployment API . Deployment logs can be viewed on Kibana . The link to the logs will be provided by the deploy tool.","title":"How it works"},{"location":"deployment/#set-it-up","text":"Your application must have a repository on GitHub containing a nais.yaml and Dockerfile . Your GitHub team must have admin access on that repository. Your GitHub team's identifier must match the Kubernetes team label in your nais.yaml . There is an example file below. Retrieve your team API key . Save the key as a secret named NAIS_DEPLOY_APIKEY in your GitHub repository. Follow the guide below. When things break, see the help page .","title":"Set it up"},{"location":"deployment/#deploy-with-github-actions","text":"A GitHub Actions pipeline is called a Workflow . You can set up workflows by adding a YAML file to your application's Git repository. In this example, the workflow is set up in the file deploy.yaml . The workflow will build a Docker image and push it to GitHub Container Registry. Next, if the code was pushed to the master branch AND the build job succeeded, the application will be deployed to NAIS. Official GitHub documentation: Automating your workflow with GitHub Actions . Get started by creating the following structure in your application repository: myapplication/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u2514\u2500\u2500 deploy.yaml \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 nais.yaml Add the example files below, then commit and push. This will trigger the workflow, and you can follow its progress under the Actions tab on your GitHub repository page. .github/workflows/deploy.yaml name : Build, push, and deploy on : [ push ] env : docker_image : ghcr.io/${{ github.repository }}:${{ github.sha }} jobs : build : name : Build and push Docker container runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : docker/login-action@v1 with : registry : ghcr.io username : ${{ github.actor }} password : ${{ secrets.GITHUB_TOKEN }} - uses : docker/build-push-action@v2 with : context : . push : true tags : ${{ env.docker_image }} deploy : name : Deploy to NAIS needs : build if : github.ref == 'refs/heads/master' runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : dev-gcp RESOURCE : nais.yaml VAR : image=${{ env.docker_image }} nais.yaml apiVersion : nais.io/v1alpha1 kind : Application metadata : name : myapplication namespace : myteam labels : team : myteam spec : image : {{ image }} #image: ghcr.io/navikt/myapplication:417dcaa2c839b9da72e0189e2cfdd4e90e9cc6fd # ^--- interpolated from the ${{ env.docker_image }} variable in the action In this nais.yaml file, {{ image }} will be replaced by the $docker_image environment variable set in the action. You can add more by using a comma separated list, or even put all your variables in a file; see action configuration below. Dockerfile FROM nginxinc/nginx-unprivileged You have to write your own Dockerfile, but if you're just trying this out, you can use the following example file.","title":"Deploy with GitHub Actions"},{"location":"deployment/#action-configuration","text":"Environment variable Default Description APIKEY (required) NAIS deploy API key. Obtained from https://deploy.nais.io . CLUSTER (required) Which NAIS cluster ( On-premises or GCP ) to deploy into. DRY_RUN false If true , run templating and validate input, but do not actually make any requests. ENVIRONMENT (auto-detect) The environment to be shown in GitHub Deployments. Defaults to CLUSTER:NAMESPACE for the resource to be deployed if not specified, otherwise falls back to CLUSTER if multiple namespaces exist in the given resources. OWNER (auto-detect) Owner of the repository making the request. PRINT_PAYLOAD false If true , print templated resources to standard output. QUIET false If true , suppress all informational messages. REF master (auto-detect) Commit reference of the deployment. Shown in GitHub's interface. REPOSITORY (auto-detect) Name of the repository making the request. RESOURCE (required) Comma-separated list of files containing Kubernetes resources. Must be JSON or YAML format. RETRY true Automatically retry deploying if deploy service is unavailable. TEAM (auto-detect) Team making the deployment. TIMEOUT 10m Time to wait for deployment completion, especially when using WAIT . VAR Comma-separated list of template variables in the form key=value . Will overwrite any identical template variable in the VARS file. VARS /dev/null File containing template variables. Will be interpolated with the $RESOURCE file. Must be JSON or YAML format. WAIT true Block until deployment has completed with either success , failure or error state. Note that OWNER and REPOSITORY corresponds to the two parts of a full repository identifier. If that name is navikt/myapplication , those two variables should be set to navikt and myapplication , respectively.","title":"Action configuration"},{"location":"deployment/#deploy-with-other-ci","text":"You can still use NAIS deploy even if not using GitHub Actions. Our deployment command line tool supports all CI tools such as Jenkins, Travis or Circle. Use can either use a Docker image or one the binaries to make deployments. Docker usage docker run -it --rm -v $(pwd):/nais /navikt/deployment:v1 \\ /app/deploy \\ --apikey=\"$NAIS_DEPLOY_APIKEY\" \\ --cluster=\"$CLUSTER\" \\ --owner=\"$OWNER\" \\ --repository=\"$REPOSITORY\" \\ --resource=\"/nais/path/to/nais.yaml\" \\ --vars=\"/nais/path/to/resources\" \\ --wait=true \\ ; Here we use the current directory as a volume for the CLI, and we have to append /nais to the path to our manifest. So if our original manifest is under /home/cooluser/workspace/bestapp/nais.yaml , we then need to --resource=\"/nais/nais.yaml\" , and not only --resource=\"nais.yaml\" . CLI usage deploy \\ --apikey=\"$NAIS_DEPLOY_APIKEY\" \\ --cluster=\"$CLUSTER\" \\ --owner=\"$OWNER\" \\ --repository=\"$REPOSITORY\" \\ --resource=\"/path/to/nais.yaml\" \\ --vars=\"/path/to/resources\" \\ --wait=true Syntax: --apikey string NAIS Deploy API key. --cluster string NAIS cluster to deploy into. --deploy-server string URL to API server. (default \"https://deployment.prod-sbs.nais.io\") --dry-run Run templating, but don't actually make any requests. --environment string Environment for GitHub deployment. Auto-detected from nais.yaml if not specified. --owner string Owner of GitHub repository. (default \"navikt\") --print-payload Print templated resources to standard output. --quiet Suppress printing of informational messages except errors. --ref string Git commit hash, tag, or branch of the code being deployed. (default \"master\") --repository string Name of GitHub repository. --resource strings File with Kubernetes resource. Can be specified multiple times. --team string Team making the deployment. Auto-detected if possible. --var strings Template variable in the form KEY=VALUE. Can be specified multiple times. --vars string File containing template variables. --wait Block until deployment reaches final state (success, failure, error). All of these options can be set using environment variables, such as $APIKEY and $PRINT_PAYLOAD .","title":"Deploy with other CI"},{"location":"deployment/#proxy-server","text":"If you are running NAIS deploy from an internal Jenkins server you need to set up an HTTP proxy as deploy.nais.io is a public address. When using the CLI binary you can wrap steps in your pipeline with injected environment variables. stage('Deploy') { withEnv(['HTTPS_PROXY=http://webproxy-utvikler.nav.no:8088']) { ... } } When using NAIS deploy docker image, pass the environment variable to Docker run. sh \"docker run --env HTTPS_PROXY='http://webproxy-utvikler.nav.no:8088' ...\" ;","title":"Proxy server"},{"location":"deployment/#templating","text":"Templates use Handlebars 3.0 syntax. Both the template and variable file supports either YAML or JSON syntax. A practical example follows. Create a nais.yaml file: apiVersion : nais.io/v1alpha1 kind : Application metadata : name : {{ app }} namespace : {{ namespace }} labels : team : {{ team }} spec : image : {{ image }} ingresses : {{ #each ingresses as |url|}} - {{ url }} {{ /each }} Now, create a vars.yaml file containing variables for your deployment: app : myapplication namespace : myteam team : myteam image : ghcr.io/navikt/myapplication:latest ingresses : - https://myapplication.nav.no - https://tjenester.nav.no/myapplication Run the deploy tool to see the final results: $ deploy -- dry - run -- print - payload -- resource nais . yaml -- vars vars . yaml | jq \".resources[0]\" { \"apiVersion\" : \"nais.io/v1alpha1\" , \"kind\" : \"Application\" , \"metadata\" : { \"labels\" : { \"team\" : \"myteam\" }, \"name\" : \"myapplication\" , \"namespace\" : \"default\" }, \"spec\" : { \"image\" : \"ghcr.io/navikt/myapplication:417dcaa2c839b9da72e0189e2cfdd4e90e9cc6fd\" , \"ingresses\" : [ \"https://myapplication.nav.no\" , \"https://tjenester.nav.no/myapplication\" ] } }","title":"Templating"},{"location":"deployment/#escaping-and-raw-resources","text":"If you do not specify the --vars or --var command-line flags, your resource will not be run through the templating engine, so these resources will not need templating. Handlebars content may be escaped by prefixing a mustache block with \\, such as: \\{{escaped}} Real-world example: apiVersion : nais.io/v1alpha1 kind : Alert metadata : name : {{ app }} labels : team : {{ team }} spec : alerts : - action : Se `kubectl describe pod \\{{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs \\{{ $labels.kubernetes_pod_name }}` for logger alert : {{ app }} -fails description : '\\{{ $labels.app }} er nede i \\{{ $labels.kubernetes_namespace }}' expr : up{app=~\"{{app}}\",job=\"kubernetes-pods\"} == 0 for : 2m severity : danger sla : respond within 1h, during office hours receivers : slack : channel : '#{{team}}' Will result in: deploy -- dry - run -- print - payload -- resource alert . yaml -- vars vars . yaml | jq . resources [ { \"apiVersion\" : \"nais.io/v1alpha1\" , \"kind\" : \"Alert\" , \"metadata\" : { \"labels\" : { \"team\" : \"myteam\" }, \"name\" : \"myapplication\" }, \"spec\" : { \"alerts\" : [ { \"action\" : \"Se `kubectl describe pod {{ $labels.kubernetes_pod_name }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }}` for logger\" , \"alert\" : \"myapplication-fails\" , \"description\" : \"{{ $labels.app }} er nede i {{ $labels.kubernetes_namespace }}\" , \"expr\" : \"up{app=~\\\"myapplication\\\",job=\\\"kubernetes-pods\\\"} == 0\" , \"for\" : \"2m\" , \"severity\" : \"danger\" , \"sla\" : \"respond within 1h, during office hours\" } ], \"receivers\" : { \"slack\" : { \"channel\" : \"#myteam\" } } } } ] PS: Templating will not be run if you do not use VARS and/or VAR , meaning \\{{ }} will not be handled by nais/deploy.","title":"Escaping and raw resources"},{"location":"deployment/#default-environment-variables","text":"These environment variables will be injected into your application container variable example source NAIS_APP_NAME myapp metadata.name from nais.yaml NAIS_NAMESPACE default metadata.namespace from nais.yaml NAIS_APP_IMAGE navikt/myapp:69 spec.image from nais.yaml NAIS_CLUSTER_NAME prod-fss naiserator runtime context NAIS_CLIENT_ID prod-fss:default:myapp concatenation of cluster, namespace and app name Environment variables for loading CA bundles into your application will also be injected: variable example source NAV_TRUSTSTORE_PATH /etc/ssl/certs/java/cacerts CA bundle containing the most commonly used CA root certificates NAV_TRUSTSTORE_PASSWORD xxxxx Password for the CA bundle","title":"Default environment variables"},{"location":"deployment/#build-status-badge","text":"Use the following URL to create a small badge for your workflow/action. https://github.com/{github_id}/{repository}/workflows/{workflow_name}/badge.svg","title":"Build status badge"},{"location":"deployment/allowed-registries/","text":"Allowed registries \u00a7 In an effort to minimize risk, we restrict the origin of Docker images to a set of known repositories. This ensures that if we get compromised, malicious images can't be pulled directly from an arbitrary source, but must exist at a pre-known location. How? \u00a7 When a Pod , Deployment , CronJob , or Application resource is applied to the cluster, the image tag is validated against the list of allowed sources . If the image is not in the list, the resource will not be created, and the user will receive the following error message: \"application.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by allowed-registry] image 'not_allowed/image' is not in the list of allowed sources Images from unlisted registry \u00a7 Re-tag the image and upload it to Docker Package Registry: docker pull unlisted/image:tag docker tag unlisted/image:tag ghcr.io/orgname/reponame/imagename:tag docker push ghcr.io/orgname/reponame/imagename:tag Alternatively, in special cases, submit a pull request to the list of allowed sources instead.","title":"Allowed registries"},{"location":"deployment/allowed-registries/#allowed-registries","text":"In an effort to minimize risk, we restrict the origin of Docker images to a set of known repositories. This ensures that if we get compromised, malicious images can't be pulled directly from an arbitrary source, but must exist at a pre-known location.","title":"Allowed registries"},{"location":"deployment/allowed-registries/#how","text":"When a Pod , Deployment , CronJob , or Application resource is applied to the cluster, the image tag is validated against the list of allowed sources . If the image is not in the list, the resource will not be created, and the user will receive the following error message: \"application.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by allowed-registry] image 'not_allowed/image' is not in the list of allowed sources","title":"How?"},{"location":"deployment/allowed-registries/#images-from-unlisted-registry","text":"Re-tag the image and upload it to Docker Package Registry: docker pull unlisted/image:tag docker tag unlisted/image:tag ghcr.io/orgname/reponame/imagename:tag docker push ghcr.io/orgname/reponame/imagename:tag Alternatively, in special cases, submit a pull request to the list of allowed sources instead.","title":"Images from unlisted registry"},{"location":"deployment/change-team/","text":"Changing teams \u00a7 This section applies when you get the following error when deploying: \"tobac.nais.io\" denied the request: user 'system:serviceaccount:default:serviceuser-MYTEAM' has no access to team 'OTHERTEAM' To change which team owns an application you must use kubectl and change the team label for the application. Deploying with the new team will not work if there exists an Application with the old team label set. The easiest way to do this is with this one-liner. The user running the command must be a member of both the old and new team. Warning Windows users: this one-liner will not work. Try the alternate method below. kubectl patch app MYAPPLICATION --type merge --patch '{\"metadata\":{\"labels\":{\"team\":\"MYTEAM\"}}}' Alternate version: run the following command, and change the .metadata.labels.team field to the new team. kubectl edit app MYAPPLICATION","title":"Changing teams"},{"location":"deployment/change-team/#changing-teams","text":"This section applies when you get the following error when deploying: \"tobac.nais.io\" denied the request: user 'system:serviceaccount:default:serviceuser-MYTEAM' has no access to team 'OTHERTEAM' To change which team owns an application you must use kubectl and change the team label for the application. Deploying with the new team will not work if there exists an Application with the old team label set. The easiest way to do this is with this one-liner. The user running the command must be a member of both the old and new team. Warning Windows users: this one-liner will not work. Try the alternate method below. kubectl patch app MYAPPLICATION --type merge --patch '{\"metadata\":{\"labels\":{\"team\":\"MYTEAM\"}}}' Alternate version: run the following command, and change the .metadata.labels.team field to the new team. kubectl edit app MYAPPLICATION","title":"Changing teams"},{"location":"deployment/delete-app/","text":"Delete app \u00a7 If you want to completely remove your application from a cluster, you need to have kubectl installed, and access to the cluster . Then run the following command: kubectl delete app <app-name> This will remove the application/pods from the cluster, do some minor cleaning (removing your ingress), and that's all. Other services needs to be manually removed, such as: Kafka Postgres ID-porten Maskinporten FAQ \u00a7 I deleted an application by running kubectl delete deployment <app-name> , why did it reappear? \u00a7 Answer An Application is a resource in Kubernetes that itself is the source of configuration for other resources, e.g. Deployment . By only deleting Deployment , it will be recreated whenever the parent resource Application is synchronized again. To ensure that an application is completely deleted from the cluster, delete the Application as shown above .","title":"Delete app"},{"location":"deployment/delete-app/#delete-app","text":"If you want to completely remove your application from a cluster, you need to have kubectl installed, and access to the cluster . Then run the following command: kubectl delete app <app-name> This will remove the application/pods from the cluster, do some minor cleaning (removing your ingress), and that's all. Other services needs to be manually removed, such as: Kafka Postgres ID-porten Maskinporten","title":"Delete app"},{"location":"deployment/delete-app/#faq","text":"","title":"FAQ"},{"location":"deployment/delete-app/#i-deleted-an-application-by-running-kubectl-delete-deployment-app-name-why-did-it-reappear","text":"Answer An Application is a resource in Kubernetes that itself is the source of configuration for other resources, e.g. Deployment . By only deleting Deployment , it will be recreated whenever the parent resource Application is synchronized again. To ensure that an application is completely deleted from the cluster, delete the Application as shown above .","title":"I deleted an application by running kubectl delete deployment &lt;app-name&gt;, why did it reappear?"},{"location":"deployment/manual/","text":"Manual deployment \u00a7 Performing deployments manually requires that you have access to the cluster and kubectl configured. For automated deployments, use NAIS deploy . $ kubectl apply -f nais.yaml application.nais.io/<app name> created Verify that your application is running $ kubectl get pod -l app=<myapp> NAME READY STATUS RESTARTS AGE <app name>-59cbd7c89c-8h6wp 1/1 Running 0 4s <app name>-59cbd7c89c-xpshz 1/1 Running 0 5s You can also check that the Application resource was successfully created $ kubectl describe app <my app> ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal synchronize 3s naiserator successfully synchronized application resources (hash = 13485216922060251669) Rolling back your deployment \u00a7 If your newly deployed application does not work, and you wish to quickly rollback to a previous version, you can do this manually by executing the following command: kubectl rollout undo deployment.v1.apps/<your application> Note that if your Deployment is overridden by the Application resource, so if this is resynced / redeployed, the version and settings specified there will be effectuated. A more detailed description on how this works can be found here Resync your application \u00a7 If you want to let naiserator recreate all the resources spawned by your Application , you can do so without deleting and redeploying it. You can either run kubectl edit application <your app> and delete the .status.synchronizationHash field or run the following command: kubectl patch application <your app> -p '[{\"op\": \"remove\", \"path\": \"/status/synchronizationHash\"}]' --type = json Verify that your application has been resynced: $ kubectl describe app <your app> ... Synchronization Time: 1631876997595876945 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Synchronized 69s naiserator Successfully synchronized all application resources","title":"Manual deploy"},{"location":"deployment/manual/#manual-deployment","text":"Performing deployments manually requires that you have access to the cluster and kubectl configured. For automated deployments, use NAIS deploy . $ kubectl apply -f nais.yaml application.nais.io/<app name> created Verify that your application is running $ kubectl get pod -l app=<myapp> NAME READY STATUS RESTARTS AGE <app name>-59cbd7c89c-8h6wp 1/1 Running 0 4s <app name>-59cbd7c89c-xpshz 1/1 Running 0 5s You can also check that the Application resource was successfully created $ kubectl describe app <my app> ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal synchronize 3s naiserator successfully synchronized application resources (hash = 13485216922060251669)","title":"Manual deployment"},{"location":"deployment/manual/#rolling-back-your-deployment","text":"If your newly deployed application does not work, and you wish to quickly rollback to a previous version, you can do this manually by executing the following command: kubectl rollout undo deployment.v1.apps/<your application> Note that if your Deployment is overridden by the Application resource, so if this is resynced / redeployed, the version and settings specified there will be effectuated. A more detailed description on how this works can be found here","title":"Rolling back your deployment"},{"location":"deployment/manual/#resync-your-application","text":"If you want to let naiserator recreate all the resources spawned by your Application , you can do so without deleting and redeploying it. You can either run kubectl edit application <your app> and delete the .status.synchronizationHash field or run the following command: kubectl patch application <your app> -p '[{\"op\": \"remove\", \"path\": \"/status/synchronizationHash\"}]' --type = json Verify that your application has been resynced: $ kubectl describe app <your app> ... Synchronization Time: 1631876997595876945 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Synchronized 69s naiserator Successfully synchronized all application resources","title":"Resync your application"},{"location":"deployment/troubleshooting/","text":"Troubleshooting \u00a7 Don't panic! \u00a7 Deployment status Description success Everything is fine, your application has been deployed, and is up and running. queued Deployment request accepted, waiting to be deployed. in_progress Application deployed to Kubernetes, waiting for new version to start. failure Your application failed to start. Check state with kubectl describe app <APP> . error Either an error in your request, or the deployment system has issues. Check the logs. First debugging steps \u00a7 When something is wrong with your application, these kubectl tools should be the first things you check out: Describe the pod to find statuses and messages: kubectl describe pod <podname> And view the logs for your pods with this command: kubectl logs <podname> Logs \u00a7 All deployments are logged to Kibana. You can get a direct link to your deployment logs from https://deploy.nais.io or the https://github.com/navikt/<YOUR_REPOSITORY>/deployments page. Click on the link that says deployed . This link is also printed in the console output. It looks like https://deployment.prod-sbs.nais.io/logs?delivery_id=<UUID>&ts=<TIMESTAMP> . Error messages \u00a7 Message Action You don't have access to apikey/. See Access to Vault in the Teams documentation 403 authentication failed Either you're using the wrong team API key , or if using the old version of NAIS deploy, your team is not registered in the team portal . 502 bad gateway There is some transient error with GitHub or Vault. Please try again later. If the problem persists, ask @nais-team for help. deployment failed: failed authentication Wrong team API key , please check https://deploy.nais.io/ for the correct key. applications.nais.io is forbidden: User \"...\" cannot create resource \"applications\" in API group \"nais.io\" at the cluster scope You forgot to specify the .namespace attribute in nais.yaml . failed authentication: HMAC signature error See above. \"tobac.nais.io\" denied the request: user 'system:serviceaccount:default:serviceuser-FOO' has no access to team 'BAR' The application is already deployed, and team names differ. See changing teams . \u201ctobac.nais.io\u201d denied the request: team 'FOO' on existing resource does not exist The team owning the resource may have been deleted or renamed. Ask @nais-team for help. the server could not find the requested resource (total of 1 errors) The resource is not specifying its namespace. MountVolume.SetUp failed for volume \" \" : secret \" \" not found See secret not found . Secret not found \u00a7 When describing a pod or inspecting the deployment logs, you might find the following message: Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning FailedMount 22m ( x5 over 22m ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Some secrets (such as those from Azure AD or Kafka ) are eventually consistent . Typically, the message appears on either: First-time deployments where the secret has to be provisioned, or As part of a secret rotation process Generally, the warning message should not persist or be stuck for more than a minute. Follow the steps below to verify that the secret exists: Step 1. Verify Pod events \u00a7 Success If the event log looks like this: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m22s default-scheduler Successfully assigned <namespace>/<pod> to <node> Warning FailedMount 2m20s ( x3 over 2m22s ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Normal Pulling 2m18s kubelet Pulling image \"<docker-image>\" Normal Pulled 117s kubelet Successfully pulled image \"<docker-image>\" Normal Created 61s ( x4 over 115s ) kubelet Created container <app> Normal Started 61s ( x4 over 115s ) kubelet Started container <app> Then the secret was successfully found and your application's container has been started. Failure If the event log looks like this: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m53s default-scheduler Successfully assigned <namespace>/<pod> to <node> Warning FailedMount 2m50s kubelet Unable to attach or mount volumes: unmounted volumes =[ <secret-name> ] , unattached volumes =[ <secret-name> ] : timed out waiting for the condition Warning FailedMount 43s ( x10 over 4m53s ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Warning FailedMount 35s kubelet Unable to attach or mount volumes: unmounted volumes =[ <secret-name> ] , unattached volumes =[ <secret-name> ] : timed out waiting for the condition Then you should report this to the #nais channel on Slack. Step 2. Verify that the Secret exists \u00a7 With kubectl , run the following command to verify that the secret exists: kubectl get secret <secret-name> Success \u279c kubectl get secret my-secret NAME TYPE DATA AGE my-secret Opaque 3 15h Failure \u279c kubectl get secret my-secret Error from server ( NotFound ) : secrets \"my-secret\" not found If you have followed the above checklist and verified that the secret does not exist, please contact #nais for further assistance. Asking on Slack \u00a7 If you read this entire page, and checked your logs, and checked application status with kubectl , you can ask Slack for help. Use the #nais channel and include the following information: Application Team tag Namespace Cluster name Link to logs What steps you already took to debug the error","title":"Troubleshooting"},{"location":"deployment/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"deployment/troubleshooting/#dont-panic","text":"Deployment status Description success Everything is fine, your application has been deployed, and is up and running. queued Deployment request accepted, waiting to be deployed. in_progress Application deployed to Kubernetes, waiting for new version to start. failure Your application failed to start. Check state with kubectl describe app <APP> . error Either an error in your request, or the deployment system has issues. Check the logs.","title":"Don't panic!"},{"location":"deployment/troubleshooting/#first-debugging-steps","text":"When something is wrong with your application, these kubectl tools should be the first things you check out: Describe the pod to find statuses and messages: kubectl describe pod <podname> And view the logs for your pods with this command: kubectl logs <podname>","title":"First debugging steps"},{"location":"deployment/troubleshooting/#logs","text":"All deployments are logged to Kibana. You can get a direct link to your deployment logs from https://deploy.nais.io or the https://github.com/navikt/<YOUR_REPOSITORY>/deployments page. Click on the link that says deployed . This link is also printed in the console output. It looks like https://deployment.prod-sbs.nais.io/logs?delivery_id=<UUID>&ts=<TIMESTAMP> .","title":"Logs"},{"location":"deployment/troubleshooting/#error-messages","text":"Message Action You don't have access to apikey/. See Access to Vault in the Teams documentation 403 authentication failed Either you're using the wrong team API key , or if using the old version of NAIS deploy, your team is not registered in the team portal . 502 bad gateway There is some transient error with GitHub or Vault. Please try again later. If the problem persists, ask @nais-team for help. deployment failed: failed authentication Wrong team API key , please check https://deploy.nais.io/ for the correct key. applications.nais.io is forbidden: User \"...\" cannot create resource \"applications\" in API group \"nais.io\" at the cluster scope You forgot to specify the .namespace attribute in nais.yaml . failed authentication: HMAC signature error See above. \"tobac.nais.io\" denied the request: user 'system:serviceaccount:default:serviceuser-FOO' has no access to team 'BAR' The application is already deployed, and team names differ. See changing teams . \u201ctobac.nais.io\u201d denied the request: team 'FOO' on existing resource does not exist The team owning the resource may have been deleted or renamed. Ask @nais-team for help. the server could not find the requested resource (total of 1 errors) The resource is not specifying its namespace. MountVolume.SetUp failed for volume \" \" : secret \" \" not found See secret not found .","title":"Error messages"},{"location":"deployment/troubleshooting/#secret-not-found","text":"When describing a pod or inspecting the deployment logs, you might find the following message: Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning FailedMount 22m ( x5 over 22m ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Some secrets (such as those from Azure AD or Kafka ) are eventually consistent . Typically, the message appears on either: First-time deployments where the secret has to be provisioned, or As part of a secret rotation process Generally, the warning message should not persist or be stuck for more than a minute. Follow the steps below to verify that the secret exists:","title":"Secret not found"},{"location":"deployment/troubleshooting/#step-1-verify-pod-events","text":"Success If the event log looks like this: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m22s default-scheduler Successfully assigned <namespace>/<pod> to <node> Warning FailedMount 2m20s ( x3 over 2m22s ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Normal Pulling 2m18s kubelet Pulling image \"<docker-image>\" Normal Pulled 117s kubelet Successfully pulled image \"<docker-image>\" Normal Created 61s ( x4 over 115s ) kubelet Created container <app> Normal Started 61s ( x4 over 115s ) kubelet Started container <app> Then the secret was successfully found and your application's container has been started. Failure If the event log looks like this: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m53s default-scheduler Successfully assigned <namespace>/<pod> to <node> Warning FailedMount 2m50s kubelet Unable to attach or mount volumes: unmounted volumes =[ <secret-name> ] , unattached volumes =[ <secret-name> ] : timed out waiting for the condition Warning FailedMount 43s ( x10 over 4m53s ) kubelet MountVolume.SetUp failed for volume \"<secret-name>\" : secret \"<secret-name>\" not found Warning FailedMount 35s kubelet Unable to attach or mount volumes: unmounted volumes =[ <secret-name> ] , unattached volumes =[ <secret-name> ] : timed out waiting for the condition Then you should report this to the #nais channel on Slack.","title":"Step 1. Verify Pod events"},{"location":"deployment/troubleshooting/#step-2-verify-that-the-secret-exists","text":"With kubectl , run the following command to verify that the secret exists: kubectl get secret <secret-name> Success \u279c kubectl get secret my-secret NAME TYPE DATA AGE my-secret Opaque 3 15h Failure \u279c kubectl get secret my-secret Error from server ( NotFound ) : secrets \"my-secret\" not found If you have followed the above checklist and verified that the secret does not exist, please contact #nais for further assistance.","title":"Step 2. Verify that the Secret exists"},{"location":"deployment/troubleshooting/#asking-on-slack","text":"If you read this entire page, and checked your logs, and checked application status with kubectl , you can ask Slack for help. Use the #nais channel and include the following information: Application Team tag Namespace Cluster name Link to logs What steps you already took to debug the error","title":"Asking on Slack"},{"location":"device/","text":"naisdevice \u00a7 naisdevice is the mechanism used by NAV employees to connect to internal services. It is based on WireGuard for connectivity and Kolide for ensuring that the device is in a healthy state. Until further notice naisdevice is only offered to machines that are purchased and managed by NAV. You might get som mileage off a non-NAV device, but please refrain from attempting to do so. There is a fair amount of legalese that needs interpretation before we can offer naisdevice \"in the wild\". If you want to understand more about how it works, head over to the github repository and check out the project documentation.","title":"naisdevice Overview"},{"location":"device/#naisdevice","text":"naisdevice is the mechanism used by NAV employees to connect to internal services. It is based on WireGuard for connectivity and Kolide for ensuring that the device is in a healthy state. Until further notice naisdevice is only offered to machines that are purchased and managed by NAV. You might get som mileage off a non-NAV device, but please refrain from attempting to do so. There is a fair amount of legalese that needs interpretation before we can offer naisdevice \"in the wild\". If you want to understand more about how it works, head over to the github repository and check out the project documentation.","title":"naisdevice"},{"location":"device/install/","text":"Installation \u00a7 OS-specific installation steps \u00a7 macOS Installation \u00a7 Install the Kolide agent . Install Homebrew unless you already have it. Add the nais tap brew tap nais/tap Install the naisdevice cask: brew install naisdevice (you will be prompted for your laptop-account's password to unlock sudo ). Turn on your freshly installed naisdevice app. Use <Command> + <Space> to find your naisdevice.app and press <Enter>. Follow the instructions to connect your nais device . Remember to update your kubeconfig if you need to connect to anything running in a K8s cluster. Windows Installation \u00a7 install Kolide agent install WireGuard (Note: Ignore error message regarding UI only being accessible by the Builtin Administrators group) install naisdevice (naisdevice.msi) Remember to update your kubeconfig Start naisdevice from the Start menu Ubuntu Installation \u00a7 Warn Using Gnome DE on latest Ubuntu LTS - only supported variant atm Install Kolide agent . Add the nais PPA repo: NAIS_GPG_KEY=\"/usr/local/share/keyrings/nav_nais.gpg\" sudo mkdir -p \"$(dirname \"$NAIS_GPG_KEY\")\" curl -sfSL \"https://ppa.nais.io/KEY.gpg\" | gpg --dearmor | sudo dd of=\"$NAIS_GPG_KEY\" echo \"deb [signed-by=$NAIS_GPG_KEY] https://ppa.nais.io/ ./\" | sudo tee /etc/apt/sources.list.d/nav_nais.list sudo apt update # Now you can apt install naisdevice Intall the naisdevice package: sudo apt install naisdevice Turn on your freshly installed naisdevice application. Find naisdevice in your application menu, or use the naisdevice command in a terminal to start the application. Follow the instructions to connect your nais device . Remember to update your kubeconfig . OS-agnostic install steps \u00a7 Install Kolide agent \u00a7 Send a message to the Kolide app on Slack, either by: Pasting the following command (in any message input field) in Slack: /msg @Kolide installers or by, finding the \"Kolide\" app and directly messaging it the word installers (case independent). Follow Kolide's walk-through: Select Enroll a Device Select Enroll your device Select platform and wait for Kolide to create your installer. Install the package created by Kolide in your chat with the app (named xkxp-*-kolide-launcher.{pkg,msi,deb} ). There are is no success feedback given by Kolide in Slack. No error message means that the installation was successful. Allow a couple of minutes to let Kolide initialize device state, but if you're stuck at \"Waiting for your device to connect\" just go to the next step. Check your devices status: /msg @Kolide status on Slack and fix errors if there are any. Unless Kolide reports your device as \"Ok\"/\"Healthy\", follow the instructions on how to remediate the issues. Go back to macOS , Windows or Ubuntu installations to continue. Warning The issues reported by Kolide must be addressed - these remediations have been vetted by the NAIS team and should be followed. Depending on the issue, you might lose naisdevice connectivity if an issue is left unresolved for a sufficient length of time. If a remediation required by Kolide makes you feel unsafe - feel free to ask in #naisdevice Slack channel. Connect naisdevice through task/sys -tray icon \u00a7 In your Systray (where all your small program icons are located - see above picture for how it looks on Mac): Find your naisdevice icon (pictured above - though it should not be red at first attempted connection). Can't find the icon? Make sure it is installed (See macOS , Windows or Ubuntu ) Left-click it and select Connect . Read and accept the End-User terms and agreement (The Do's and Don'ts of naisdevice ). See the picture below . Left-click the naisdevice icon again and click Connect . You might need to allow ~20 seconds to pass before clicking Connect turns your naisdevice icon green. If naisdevice gives a pop-up notification about your device being unhealthy - double check that Kolide still reports your device as healthy. (The naisdevice systray-icon should have turned into a yellow color). If not your device is not healthy - remediate the issues. Tip If Kolide reports your device to be healthy, but still naisdevice won't let you connect, try to disconnect and re-connect naisdevice . If naisdevice still won't let you connect, be aware that it may take up to 5 minutes for the naisdevice server to register that Kolide now thinks your device is okay. How to accept the \"Do's and don'ts\" of naisdevice \u00a7 Info You should be automatically sent here when connecting to naisdevice if not you've not yet accepted. Can manually be found at the following URL: https://naisdevice-approval.nais.io/ . Read through the list of \"Do's and don'ts\". If you've got any questions, you may join the #naisdevice Slack channel. Which happens to be one of the required \"Do's\" anyways ;) . If you accept the terms (they are non-negotiable); click the green \"Accept\" button at the botttom of the page! The button should turn into a red \"Reject\" button once your acceptance has been processed! Connecting to NAIS clusters \u00a7 In a terminal/shell of your choice, navigate to kubeconfigs repo . If you haven't downloaded repo already: git clone https://github.com/navikt/kubeconfigs.git If you are using SSH keys, use this command instead: git clone git@github.com:navikt/kubeconfigs.git cd kubeconfigs To navigate to the repository. git pull To ensure you've got latest & greatest. Make and set the KUBECONFIG environment variable to the path of the config -file. You can do this from the terminal with: export KUBECONFIG=\"<path-to>/kubeconfigs/config\"","title":"Installation"},{"location":"device/install/#installation","text":"","title":"Installation"},{"location":"device/install/#os-specific-installation-steps","text":"","title":"OS-specific installation steps"},{"location":"device/install/#macos-installation","text":"Install the Kolide agent . Install Homebrew unless you already have it. Add the nais tap brew tap nais/tap Install the naisdevice cask: brew install naisdevice (you will be prompted for your laptop-account's password to unlock sudo ). Turn on your freshly installed naisdevice app. Use <Command> + <Space> to find your naisdevice.app and press <Enter>. Follow the instructions to connect your nais device . Remember to update your kubeconfig if you need to connect to anything running in a K8s cluster.","title":"macOS Installation"},{"location":"device/install/#windows-installation","text":"install Kolide agent install WireGuard (Note: Ignore error message regarding UI only being accessible by the Builtin Administrators group) install naisdevice (naisdevice.msi) Remember to update your kubeconfig Start naisdevice from the Start menu","title":"Windows Installation"},{"location":"device/install/#ubuntu-installation","text":"Warn Using Gnome DE on latest Ubuntu LTS - only supported variant atm Install Kolide agent . Add the nais PPA repo: NAIS_GPG_KEY=\"/usr/local/share/keyrings/nav_nais.gpg\" sudo mkdir -p \"$(dirname \"$NAIS_GPG_KEY\")\" curl -sfSL \"https://ppa.nais.io/KEY.gpg\" | gpg --dearmor | sudo dd of=\"$NAIS_GPG_KEY\" echo \"deb [signed-by=$NAIS_GPG_KEY] https://ppa.nais.io/ ./\" | sudo tee /etc/apt/sources.list.d/nav_nais.list sudo apt update # Now you can apt install naisdevice Intall the naisdevice package: sudo apt install naisdevice Turn on your freshly installed naisdevice application. Find naisdevice in your application menu, or use the naisdevice command in a terminal to start the application. Follow the instructions to connect your nais device . Remember to update your kubeconfig .","title":"Ubuntu Installation"},{"location":"device/install/#os-agnostic-install-steps","text":"","title":"OS-agnostic install steps"},{"location":"device/install/#install-kolide-agent","text":"Send a message to the Kolide app on Slack, either by: Pasting the following command (in any message input field) in Slack: /msg @Kolide installers or by, finding the \"Kolide\" app and directly messaging it the word installers (case independent). Follow Kolide's walk-through: Select Enroll a Device Select Enroll your device Select platform and wait for Kolide to create your installer. Install the package created by Kolide in your chat with the app (named xkxp-*-kolide-launcher.{pkg,msi,deb} ). There are is no success feedback given by Kolide in Slack. No error message means that the installation was successful. Allow a couple of minutes to let Kolide initialize device state, but if you're stuck at \"Waiting for your device to connect\" just go to the next step. Check your devices status: /msg @Kolide status on Slack and fix errors if there are any. Unless Kolide reports your device as \"Ok\"/\"Healthy\", follow the instructions on how to remediate the issues. Go back to macOS , Windows or Ubuntu installations to continue. Warning The issues reported by Kolide must be addressed - these remediations have been vetted by the NAIS team and should be followed. Depending on the issue, you might lose naisdevice connectivity if an issue is left unresolved for a sufficient length of time. If a remediation required by Kolide makes you feel unsafe - feel free to ask in #naisdevice Slack channel.","title":"Install Kolide agent"},{"location":"device/install/#connect-naisdevice-through-tasksys-tray-icon","text":"In your Systray (where all your small program icons are located - see above picture for how it looks on Mac): Find your naisdevice icon (pictured above - though it should not be red at first attempted connection). Can't find the icon? Make sure it is installed (See macOS , Windows or Ubuntu ) Left-click it and select Connect . Read and accept the End-User terms and agreement (The Do's and Don'ts of naisdevice ). See the picture below . Left-click the naisdevice icon again and click Connect . You might need to allow ~20 seconds to pass before clicking Connect turns your naisdevice icon green. If naisdevice gives a pop-up notification about your device being unhealthy - double check that Kolide still reports your device as healthy. (The naisdevice systray-icon should have turned into a yellow color). If not your device is not healthy - remediate the issues. Tip If Kolide reports your device to be healthy, but still naisdevice won't let you connect, try to disconnect and re-connect naisdevice . If naisdevice still won't let you connect, be aware that it may take up to 5 minutes for the naisdevice server to register that Kolide now thinks your device is okay.","title":"Connect naisdevice through task/sys -tray icon"},{"location":"device/install/#how-to-accept-the-dos-and-donts-of-naisdevice","text":"Info You should be automatically sent here when connecting to naisdevice if not you've not yet accepted. Can manually be found at the following URL: https://naisdevice-approval.nais.io/ . Read through the list of \"Do's and don'ts\". If you've got any questions, you may join the #naisdevice Slack channel. Which happens to be one of the required \"Do's\" anyways ;) . If you accept the terms (they are non-negotiable); click the green \"Accept\" button at the botttom of the page! The button should turn into a red \"Reject\" button once your acceptance has been processed!","title":"How to accept the \"Do's and don'ts\" of naisdevice"},{"location":"device/install/#connecting-to-nais-clusters","text":"In a terminal/shell of your choice, navigate to kubeconfigs repo . If you haven't downloaded repo already: git clone https://github.com/navikt/kubeconfigs.git If you are using SSH keys, use this command instead: git clone git@github.com:navikt/kubeconfigs.git cd kubeconfigs To navigate to the repository. git pull To ensure you've got latest & greatest. Make and set the KUBECONFIG environment variable to the path of the config -file. You can do this from the terminal with: export KUBECONFIG=\"<path-to>/kubeconfigs/config\"","title":"Connecting to NAIS clusters"},{"location":"device/jita/","text":"Just In Time Access (JITA) \u00a7 Providing access to sensitive information \u00a7 When you start naisdevice you will not be automatically connected to all the available gateways. In order to reach some of the services you will need to request \"just in time access\". The purpose of this mechanism is to provide temporary access to selected production environments through self service while keeping an audit trail. JITA requires authentication through the Microsoft login portal. Info The gateways currently requiring JITA are aiven-prod , onprem-k8s-prod and postgres-prod . Access can be requested via the naisdevice menu or through the nais cli . Once authenticated you will be presented with a form where you have to supply a short reason for why access is needed and for how long. You will then be granted access for the requested amount of time instantly and automatically.","title":"JITA"},{"location":"device/jita/#just-in-time-access-jita","text":"","title":"Just In Time Access (JITA)"},{"location":"device/jita/#providing-access-to-sensitive-information","text":"When you start naisdevice you will not be automatically connected to all the available gateways. In order to reach some of the services you will need to request \"just in time access\". The purpose of this mechanism is to provide temporary access to selected production environments through self service while keeping an audit trail. JITA requires authentication through the Microsoft login portal. Info The gateways currently requiring JITA are aiven-prod , onprem-k8s-prod and postgres-prod . Access can be requested via the naisdevice menu or through the nais cli . Once authenticated you will be presented with a form where you have to supply a short reason for why access is needed and for how long. You will then be granted access for the requested amount of time instantly and automatically.","title":"Providing access to sensitive information"},{"location":"device/services/","text":"Available services \u00a7 Available services \u00a7 This document describes the different services you will get access to through naisdevice all nais clusters kibana/logs.adeo.no basta vault fasit vera nexus/repo.adeo.no bitbucket/stash minwintid data.adeo.no dolly VDI/Utviklerimage (Use horizon.nav.no as connection server) Microsoft Teams Microsoft Sharepoint/Navet Postgres ( join this team to get access , howto )","title":"Available services"},{"location":"device/services/#available-services","text":"","title":"Available services"},{"location":"device/services/#available-services_1","text":"This document describes the different services you will get access to through naisdevice all nais clusters kibana/logs.adeo.no basta vault fasit vera nexus/repo.adeo.no bitbucket/stash minwintid data.adeo.no dolly VDI/Utviklerimage (Use horizon.nav.no as connection server) Microsoft Teams Microsoft Sharepoint/Navet Postgres ( join this team to get access , howto )","title":"Available services"},{"location":"device/troubleshooting/","text":"Troubleshooting \u00a7 naisdevice cannot connect, yet /msg @Kolide status is happy! Disconnect and re-connect naisdevice -agent =)! If you used MacOS migrate assistant, remove your old config rm -r \"~/Library/Application Support/naisdevice\" before you start naisdevice. Outgoing UDP connections to 51820/UDP must be open in your firewall and or modem provided by your ISP. If Kolide is reporting that your device has not been seen \"in a long time\" try reinstalling. Uninstall If a browser does not pop up after you click connect and naisdevice is stuck authenticating, restart your default browser.","title":"naisdevice Troubleshooting"},{"location":"device/troubleshooting/#troubleshooting","text":"naisdevice cannot connect, yet /msg @Kolide status is happy! Disconnect and re-connect naisdevice -agent =)! If you used MacOS migrate assistant, remove your old config rm -r \"~/Library/Application Support/naisdevice\" before you start naisdevice. Outgoing UDP connections to 51820/UDP must be open in your firewall and or modem provided by your ISP. If Kolide is reporting that your device has not been seen \"in a long time\" try reinstalling. Uninstall If a browser does not pop up after you click connect and naisdevice is stuck authenticating, restart your default browser.","title":"Troubleshooting"},{"location":"device/unenroll/","text":"Unenrolling \u00a7 When you're switching devices, you might want to unenroll your previously used device from the naisdevice ecosystem. This procedure is currently a manual operation done by one of the assigned admins within the Nav organization at Kolide. The list of admins is available for viewing at k2.kolide.com (sign in with your Slack account), under Who can access my data? Messaging one of them with a request to remove the particular model you wish to unenroll should be sufficient to start the process. You can also ask in the #naisdevice channel on Slack - the admins of NAV's Kolide integrations can be reached there. Don't share your device's serial number publicly If you happen to switch devices to an identical model, it might be needed to supply the device's serial number to one of the admins in order to identify a specific device. This serial number should not be posted publicly, so ensure that you're only sharing it in DMs and not in any public channels.","title":"Unenrolling"},{"location":"device/unenroll/#unenrolling","text":"When you're switching devices, you might want to unenroll your previously used device from the naisdevice ecosystem. This procedure is currently a manual operation done by one of the assigned admins within the Nav organization at Kolide. The list of admins is available for viewing at k2.kolide.com (sign in with your Slack account), under Who can access my data? Messaging one of them with a request to remove the particular model you wish to unenroll should be sufficient to start the process. You can also ask in the #naisdevice channel on Slack - the admins of NAV's Kolide integrations can be reached there. Don't share your device's serial number publicly If you happen to switch devices to an identical model, it might be needed to supply the device's serial number to one of the admins in order to identify a specific device. This serial number should not be posted publicly, so ensure that you're only sharing it in DMs and not in any public channels.","title":"Unenrolling"},{"location":"device/uninstall/","text":"Uninstall \u00a7 OS-specific Uninstall steps \u00a7 macOS uninstall \u00a7 Stop and remove Kolide and the related launch mechanisms sudo /bin/launchctl unload /Library/LaunchDaemons/com.kolide-k2.launcher.plist sudo /bin/rm -f /Library/LaunchDaemons/com.kolide-k2.launcher.plist Delete files, configuration and binaries sudo /bin/rm -rf /usr/local/kolide-k2 sudo /bin/rm -rf /etc/kolide-k2 sudo /bin/rm -rf /var/kolide-k2 Uninstall the naisdevice Homebrew cask brew uninstall --force naisdevice Windows uninstall \u00a7 Enter Apps & Features Search for Kolide Click Uninstall Ubuntu uninstall \u00a7 Stop and remove Kolide and the related launch mechanisms sudo systemctl stop launcher.kolide-k2.service sudo systemctl disable launcher.kolide-k2.service Uninstall Kolide program files sudo apt ( -get ) remove launcher-kolide-k2 Delete Kolide files & caches sudo rm -r / { etc,var } /kolide-k2 Uninstall the naisdevice deb package sudo apt remove naisdevice OS-agnostic uninstall steps \u00a7 When the program has been removed from your device, let an admin know in #naisdevice Slack channel. This is necessary so that the record of your device can be purged from our Kolide systems. Info This might be automated soon\u2122.","title":"Uninstall"},{"location":"device/uninstall/#uninstall","text":"","title":"Uninstall"},{"location":"device/uninstall/#os-specific-uninstall-steps","text":"","title":"OS-specific Uninstall steps"},{"location":"device/uninstall/#macos-uninstall","text":"Stop and remove Kolide and the related launch mechanisms sudo /bin/launchctl unload /Library/LaunchDaemons/com.kolide-k2.launcher.plist sudo /bin/rm -f /Library/LaunchDaemons/com.kolide-k2.launcher.plist Delete files, configuration and binaries sudo /bin/rm -rf /usr/local/kolide-k2 sudo /bin/rm -rf /etc/kolide-k2 sudo /bin/rm -rf /var/kolide-k2 Uninstall the naisdevice Homebrew cask brew uninstall --force naisdevice","title":"macOS uninstall"},{"location":"device/uninstall/#windows-uninstall","text":"Enter Apps & Features Search for Kolide Click Uninstall","title":"Windows uninstall"},{"location":"device/uninstall/#ubuntu-uninstall","text":"Stop and remove Kolide and the related launch mechanisms sudo systemctl stop launcher.kolide-k2.service sudo systemctl disable launcher.kolide-k2.service Uninstall Kolide program files sudo apt ( -get ) remove launcher-kolide-k2 Delete Kolide files & caches sudo rm -r / { etc,var } /kolide-k2 Uninstall the naisdevice deb package sudo apt remove naisdevice","title":"Ubuntu uninstall"},{"location":"device/uninstall/#os-agnostic-uninstall-steps","text":"When the program has been removed from your device, let an admin know in #naisdevice Slack channel. This is necessary so that the record of your device can be purged from our Kolide systems. Info This might be automated soon\u2122.","title":"OS-agnostic uninstall steps"},{"location":"device/update/","text":"Updating naisdevice \u00a7 These instructions assume that you already have some version of naisdevice installed, and that you are already familiar with the general method of use. If this is not the case, head over to the installation instructions . OS-specific update instructions \u00a7 macOS \u00a7 If you don't already have naisdevice installed via Homebrew, refer to install . Use the standard mechanisms provided by Homebrew to upgrade the naisdevice cask Windows \u00a7 Download and run the newest naisdevice installer (naisdevice.msi). Ubuntu (using Gnome DE - only supported variant atm) \u00a7 Use the standard mechanisms provided by APT to upgrade the naisdevice package Troubleshooting \u00a7 If your attempt at updating naisdevice at any point fails, refer to the installation instructions and follow the instructions for your operating system from the point after \"Install Kolide agent\". If you still experience issues after trying that, hit us up in the #naisdevice channel on Slack.","title":"Updating"},{"location":"device/update/#updating-naisdevice","text":"These instructions assume that you already have some version of naisdevice installed, and that you are already familiar with the general method of use. If this is not the case, head over to the installation instructions .","title":"Updating naisdevice"},{"location":"device/update/#os-specific-update-instructions","text":"","title":"OS-specific update instructions"},{"location":"device/update/#macos","text":"If you don't already have naisdevice installed via Homebrew, refer to install . Use the standard mechanisms provided by Homebrew to upgrade the naisdevice cask","title":"macOS"},{"location":"device/update/#windows","text":"Download and run the newest naisdevice installer (naisdevice.msi).","title":"Windows"},{"location":"device/update/#ubuntu-using-gnome-de-only-supported-variant-atm","text":"Use the standard mechanisms provided by APT to upgrade the naisdevice package","title":"Ubuntu (using Gnome DE - only supported variant atm)"},{"location":"device/update/#troubleshooting","text":"If your attempt at updating naisdevice at any point fails, refer to the installation instructions and follow the instructions for your operating system from the point after \"Install Kolide agent\". If you still experience issues after trying that, hit us up in the #naisdevice channel on Slack.","title":"Troubleshooting"},{"location":"legacy/am/","text":"AM/OpenAM \u00a7 Dersom applikasjonen din trenger ForgeRock AM oppsett i SBS eller FSS, kan dette settes opp ved \u00e5 kalle tjenesten Named (NAIS Access Management Extension). Dersom deployregimet til ATOM benyttes kan man angi skipOpenam i deploy request for \u00e5 slippe konfigurasjon av dette. Policyoppsett i SBS \u00a7 Oppsett av policy og not enforced urls gj\u00f8res som tidligere p\u00e5 AM-serverne, dvs man benytter samme skript for oppsettet. Forskjellen er at dette kalles fra en tjeneste som heter nameD. Denne tjenesten kan kalles ved en enkel curl eller ved bruk av CLI'et som er laget for tjenesten (se README p\u00e5 https://github.com/nais/named ). F\u00f8lgende krav m\u00e5 v\u00e6re oppfylt ved kall til nameD \u00a7 Innsendte parametere som m\u00e5 settes (se eksempel nedenfor): Parameter name Description application applikasjonsnavn i Fasit version applikasjonens versjon som skal konfigureres environment milj\u00f8navn i Fasit username brukernavn i Fasit password passord i Fasit Det sjekkes for gyldighet av application , environment , username og password mot Fasit, og application , og version mot Nexus Filene app-policies.xml og not-enforced-urls.txt m\u00e5 legges inn p\u00e5 Nexus p\u00e5 f\u00f8lgende sti: https://repo.adeo.no/repository/raw/nais/<appnavn>/<versjon>/am/ Request sendes til tjenesten (eksemplet gjelder for AM konfigurasjon i t og q ): curl -k -d '{\"application\": \"<appnavn>\", \"version\": \"<versjon>\", \"environment\": \"<fasitmilj\u00f8>\", \"username\": \"<fasit brukernavn>\", \"password\": \"<fasit passord>\"}' https://named.nais.oera-q.local/configure NB! Ved oppsett av AM i SBS skal man bruke Named-tjenesten som ligger i SBS (named.nais.oera-q.local eller named.nais.oera.no) ISSO agent oppsett i FSS \u00a7 Oppsett av ISSO agentene gj\u00f8res ved hjelp av REST api'et til AM. Konfigurasjonen vil opprette agent med navn - i AM. For \u00e5 benytte denne agenten m\u00e5 man autentisere mot AM med bruker agentadmin og dennes passord. F\u00f8lgende krav m\u00e5 v\u00e6re oppfylt ved kall til Named \u00a7 Innsendte parametere som m\u00e5 settes (se eksempel nedenfor): Parameter name Description application applikasjonsnavn i Fasit version applikasjonens versjon som skal konfigureres environment milj\u00f8navn i Fasit username brukernavn i Fasit password passord i Fasit contextroots applikasjonens context som agenten skal st\u00f8tte Det sjekkes for gyldighet av application , environment , username , og password mot Fasit, og application , og version mot Nexus I motsetning til AM i SBS trengs det ingen eksterne konfigurasjonsfiler Et ekstra parameter kreves for oppsett av korrekte URL'er i openam agenten, nemlig contextroots , dette er da context-rootene som tidligere ble satt i app-config.xml for gammel plattform, men som n\u00e5 sendes direkte i requesten (disse kan det v\u00e6re flere av) Request sendes til tjenesten (eksemplet gjelder for AM konfigurasjon i t og q ): curl -k -d '{\"application\": \"<appnavn>\", \"version\": \"<versjon>\", \"environment\": \"<fasitmilj\u00f8>\", \"username\": \"<fasit brukernavn>\", \"password\": \"<fasit passord>\", \"contextroots\": [\"/context1\", \"/context2\"]}' https://named.nais.preprod.local/configure Retur fra denne requesten vil inneholde agentnavn i AM og hvilke URL'er som er satt opp for verifikasjon. P\u00e5 alle pod'er i NAIS blir det satt opp to environment variables som kan benyttes for \u00e5 konstruere agentnavnet i applikasjonen, nemlig APP_NAME og FASIT_ENVIRONMENT_NAME . Resten av informasjonen som er n\u00f8dvendig for \u00e5 sette opp OpenID og hente tokens kan hentes fra Fasit. NB! Ved oppsett av ISSO i FSS skal man bruke Named-tjenesten som ligger i FSS (named.nais.preprod.local eller named.nais.adeo.no) Trust Stores \u00a7 For alle applikasjoner ligger vi inn sertifikatet nav_truststore fra Fasit. Sertifikat ligger p\u00e5 disk p\u00e5 pathen angitt av environment variabelen NAV_TRUSTSTORE_PATH . Passord til keystoren blir injected som environment variabel NAV_TRUSTSTORE_PASSWORD . Dersom du bruker NAV sit Java-baseimaget blir keystore og passord til keystore satt automatisk for deg. For \u00e5 f\u00e5 dette til \u00e5 virke M\u00c5 du ha en applikasjonsinstans i Fasit. Denne truststore blir vedlikeholdt p\u00e5 dugnad. Bruk keytool til \u00e5 liste hvilke sertifikater som som keystoren inneholder.","title":"AM/OpenAM"},{"location":"legacy/am/#amopenam","text":"Dersom applikasjonen din trenger ForgeRock AM oppsett i SBS eller FSS, kan dette settes opp ved \u00e5 kalle tjenesten Named (NAIS Access Management Extension). Dersom deployregimet til ATOM benyttes kan man angi skipOpenam i deploy request for \u00e5 slippe konfigurasjon av dette.","title":"AM/OpenAM"},{"location":"legacy/am/#policyoppsett-i-sbs","text":"Oppsett av policy og not enforced urls gj\u00f8res som tidligere p\u00e5 AM-serverne, dvs man benytter samme skript for oppsettet. Forskjellen er at dette kalles fra en tjeneste som heter nameD. Denne tjenesten kan kalles ved en enkel curl eller ved bruk av CLI'et som er laget for tjenesten (se README p\u00e5 https://github.com/nais/named ).","title":"Policyoppsett i SBS"},{"location":"legacy/am/#flgende-krav-ma-vre-oppfylt-ved-kall-til-named","text":"Innsendte parametere som m\u00e5 settes (se eksempel nedenfor): Parameter name Description application applikasjonsnavn i Fasit version applikasjonens versjon som skal konfigureres environment milj\u00f8navn i Fasit username brukernavn i Fasit password passord i Fasit Det sjekkes for gyldighet av application , environment , username og password mot Fasit, og application , og version mot Nexus Filene app-policies.xml og not-enforced-urls.txt m\u00e5 legges inn p\u00e5 Nexus p\u00e5 f\u00f8lgende sti: https://repo.adeo.no/repository/raw/nais/<appnavn>/<versjon>/am/ Request sendes til tjenesten (eksemplet gjelder for AM konfigurasjon i t og q ): curl -k -d '{\"application\": \"<appnavn>\", \"version\": \"<versjon>\", \"environment\": \"<fasitmilj\u00f8>\", \"username\": \"<fasit brukernavn>\", \"password\": \"<fasit passord>\"}' https://named.nais.oera-q.local/configure NB! Ved oppsett av AM i SBS skal man bruke Named-tjenesten som ligger i SBS (named.nais.oera-q.local eller named.nais.oera.no)","title":"F\u00f8lgende krav m\u00e5 v\u00e6re oppfylt ved kall til nameD"},{"location":"legacy/am/#isso-agent-oppsett-i-fss","text":"Oppsett av ISSO agentene gj\u00f8res ved hjelp av REST api'et til AM. Konfigurasjonen vil opprette agent med navn - i AM. For \u00e5 benytte denne agenten m\u00e5 man autentisere mot AM med bruker agentadmin og dennes passord.","title":"ISSO agent oppsett i FSS"},{"location":"legacy/am/#flgende-krav-ma-vre-oppfylt-ved-kall-til-named_1","text":"Innsendte parametere som m\u00e5 settes (se eksempel nedenfor): Parameter name Description application applikasjonsnavn i Fasit version applikasjonens versjon som skal konfigureres environment milj\u00f8navn i Fasit username brukernavn i Fasit password passord i Fasit contextroots applikasjonens context som agenten skal st\u00f8tte Det sjekkes for gyldighet av application , environment , username , og password mot Fasit, og application , og version mot Nexus I motsetning til AM i SBS trengs det ingen eksterne konfigurasjonsfiler Et ekstra parameter kreves for oppsett av korrekte URL'er i openam agenten, nemlig contextroots , dette er da context-rootene som tidligere ble satt i app-config.xml for gammel plattform, men som n\u00e5 sendes direkte i requesten (disse kan det v\u00e6re flere av) Request sendes til tjenesten (eksemplet gjelder for AM konfigurasjon i t og q ): curl -k -d '{\"application\": \"<appnavn>\", \"version\": \"<versjon>\", \"environment\": \"<fasitmilj\u00f8>\", \"username\": \"<fasit brukernavn>\", \"password\": \"<fasit passord>\", \"contextroots\": [\"/context1\", \"/context2\"]}' https://named.nais.preprod.local/configure Retur fra denne requesten vil inneholde agentnavn i AM og hvilke URL'er som er satt opp for verifikasjon. P\u00e5 alle pod'er i NAIS blir det satt opp to environment variables som kan benyttes for \u00e5 konstruere agentnavnet i applikasjonen, nemlig APP_NAME og FASIT_ENVIRONMENT_NAME . Resten av informasjonen som er n\u00f8dvendig for \u00e5 sette opp OpenID og hente tokens kan hentes fra Fasit. NB! Ved oppsett av ISSO i FSS skal man bruke Named-tjenesten som ligger i FSS (named.nais.preprod.local eller named.nais.adeo.no)","title":"F\u00f8lgende krav m\u00e5 v\u00e6re oppfylt ved kall til Named"},{"location":"legacy/am/#trust-stores","text":"For alle applikasjoner ligger vi inn sertifikatet nav_truststore fra Fasit. Sertifikat ligger p\u00e5 disk p\u00e5 pathen angitt av environment variabelen NAV_TRUSTSTORE_PATH . Passord til keystoren blir injected som environment variabel NAV_TRUSTSTORE_PASSWORD . Dersom du bruker NAV sit Java-baseimaget blir keystore og passord til keystore satt automatisk for deg. For \u00e5 f\u00e5 dette til \u00e5 virke M\u00c5 du ha en applikasjonsinstans i Fasit. Denne truststore blir vedlikeholdt p\u00e5 dugnad. Bruk keytool til \u00e5 liste hvilke sertifikater som som keystoren inneholder.","title":"Trust Stores"},{"location":"legacy/sunset/","text":"Sunset - tjenester som nais-teamet gradvis bygger ned \u00a7 Dette er tjenester som ikke videreutvikles av naisteamet, og i noen tilfeller som vil bli avviklet. nais-sbs \u00a7 nais-clustrene dev-sbs og prod-sbs , ogs\u00e5 kjent som \"selvbetjeningssonen\" \u00f8nsker nais-teamet \u00e5 avvikle til fordel for GCP. Grunnen til dette er at tilsvarende clustre i GCP ( dev-gcp og prod-gcp ) har funksjonslikhet med SBS, og vil i tillegg gi tilgang til: automatisk provisjonering av databaser og b\u00f8tter bedre sikkerhet og mer robuste m\u00f8nster for app-til-app kommunikasjon bedre applikasjonsmetrikker, for eksempel p\u00e5 kall inn og ut av appen helautomatisert konfigurasjon n\u00e5r en app skal eksponeres For brukere av nais betyr det at nye applikasjoner skal legges til GCP fremfor SBS, og at eksisterende applikasjoner i SBS m\u00e5 migreres til GCP. F\u00f8lgende frister gjelder for migrering: Fredag 8. oktober 2021 stenges muligheten for \u00e5 lage nye apper i SBS-clustrene Fredag 1. april 2022 skal alle apper i SBS v\u00e6re migrert til GCP Begge datoene gjelder for b\u00e5de prod-sbs og dev-sbs. Dersom deres team har en applikasjon som ikke er mulig \u00e5 migrere til sky vil det v\u00e6re mulig \u00e5 f\u00e5 unntak fra kravet om migrering. Slike unntak skal godkjennes av IT-direkt\u00f8ren, og det vil komme mer info senere om prosessen for \u00e5 s\u00f8ke om unntak. Vi vil komme tilbake til hvilke l\u00f8sninger som blir tilgjengelig onprem for disse appene som er unntatt migrering. Sentralisert ABAC (Axiomatics) \u00a7 I kj\u00f8lvannet av at Team Integrasjon ble oppl\u00f8st har drift av b\u00e5de den tekniske tjenesten, verkt\u00f8yst\u00f8tten, samt forvaltningen av policies i ABAC blitt utf\u00f8rt av \u00e9n person. ABAC er kritisk for tilgangsstyringen i mange tjenester, og inneholder i mange tilfeller domenelogikk. Dette lar seg d\u00e5rlig forene med de effektene vi \u00f8nsker av distribuerte autonome team. Samtidig er policiene som ligger i ABAC implementert i et propriet\u00e6rt spr\u00e5k (Alfa) som gj\u00f8r terskelen for \u00e5 ta over denne logikken h\u00f8yere enn man skulle \u00f8nske. nais-teamet har tatt over ABAC-tjenesten fra Team Integrasjon, og \u00f8nsker \u00e5 desentralisere denne. Det betyr i praksis at teamene som bruker ABAC m\u00e5 ta eierskap til b\u00e5de egne policies og egne instanser av ABAC (alternativt flytte logikken de har i ABAC inn i egne applikasjoner). Se for\u00f8vrig ADR her. Tiln\u00e6rmingen til avvikling av sentralisert ABAC er f\u00f8rst \u00e5 flytte eksisterende implementasjon ut av stash/nexus og flytte til GitHub. Vi flytter samtidig instansen fra WebSphere til nais. S\u00e5 tenker vi \u00e5 splitte ABAC opp i domener, og flytte eierskapet for hver instans til domenet. Om teamene som bruker ABAC det aktuelle domenet \u00f8nsker \u00e5 fortsette \u00e5 bruke Axiomatics-ABAC eller \u00f8nsker \u00e5 implementere logikken direkte i appene sine eller i en egen app er opp til teamene. Uansett hvilket valg de tar vil nais-teamet v\u00e6re tilgjengelig som st\u00f8tte i overgangen. nais.adeo.no, dev.adeo.no, preprod.local, oera-q.local (ingresser) \u00a7 nais.adeo.no erstattes av intern.nav.no dev.adeo.no erstattes av dev.intern.nav.no preprod.local erstattes av dev.intern.nav.no oera-q.local erstattes av dev.nav.no For applikasjoner p\u00e5 GCP er kun de nye ingressene mulig \u00e5 benytte, mens for applikasjoner i FSS er begge mulig \u00e5 bruke. Det er ingen plan om \u00e5 avvikle de gamle ingressene i FSS, men vi anbefaler en gradvis migrering til de nye, og at nye applikasjoner benytter nye ingresser. MQ \u00a7 MQ benyttes i dag kun via on-premises servere som driftes og vedlikeholdes av ATOM og Linux. Det er \u00f8nskelig at de applikasjonene som har mulighet heller benytter Aiven Kafka. Vi \u00f8nsker at man g\u00e5r vekk fra MQ fordi dette vil forenkle overgang til offentlig sky og et eventuelt bytte av leverand\u00f8r der. Dette vil ogs\u00e5 forenkle oppsettet for applikasjonene og plattform. Det er dog st\u00f8tte for MQ fra nais-klusterne on-premises og i GCP, men da kun med nye MQ-servere som er satt opp med autentisering. Bruk av ikke-autentisert MQ er definert som et alvorlig sikkerhetshull . Vi \u00f8nsker at alle migrerer sine applikasjoner til enten autentisert MQ, eller aller helst Kafka, s\u00e5 fort som mulig. Bruk av ikke-autentisert MQ vil ikke v\u00e6re mulig fra nais-klusterne fra og med 01.12.2021. Rook/Ceph \u00a7 Rook/Ceph er i dag satt opp on-premises og tilgjengelig fra alle nais-klustere der. Disse er ikke tilgjengelig fra GCP og det er heller ikke \u00f8nskelig. L\u00f8sningen driftes og vedlikeholdes av nais, men det er en stor rigg med en del vedlikeholdsarbeid. L\u00f8sningen har ingen fullverdig backup, selv om det gj\u00f8res backup av fysiske volum og noder der Rook/Ceph kj\u00f8rer. Rook/Ceph \u00f8nskes sanert, vi \u00f8nsker derfor at ingen nye apper tar dette i bruk og heller benytter seg av buckets i GCP og ElasticSearch i GCP eller Aiven. Rook/ceph anses som en risiko pga utilstrekkelig backup ettersom et kluster ikke kan relokeres eller flyttes. Det er en uttalt plan for sanering av Rook/Ceph, men l\u00f8sningen vil leve videre med vedlikehold til tilstrekkelig mange applikasjoner har flyttet sine data ut i offentlig sky. Kafka onprem \u00a7 Kafka er i dag tilgjengelig b\u00e5de som en tjeneste fra Aiven som kj\u00f8rer i GCP og som en tjeneste vi kj\u00f8rer i egne datasentre. Begge variantene av kafka kan n\u00e5s fra alle nais-clustre. Kafka onprem driftes og vedlikeholdes av en h\u00e5ndfull personer, hvor ingen har Kafka som sin hovedoppgave. Det er ogs\u00e5 identifisert flere potensielle problemer som ikke lar seg l\u00f8se: Av lisensmessige \u00e5rsaker kan vi ikke oppgradere clusteret in-place, s\u00e5 vi er bundet til n\u00e5v\u00e6rende versjon. Clusteret er satt opp med felles diskl\u00f8sning (SAN), noe som gj\u00f8r at alle nodene har et delt point-of-failure. Dette kan i visse feilsituasjoner \u00f8ke sjansene for tap av data. Av historiske \u00e5rsaker er clusteret \"feilkonfigurert\" p\u00e5 en m\u00e5te som gj\u00f8r at consumer offset topicet er veldig stort. Dette f\u00f8rer til lange recovery tider og potensiale for \u00e5 miste offsets i feilsituasjoner. L\u00f8sningen p\u00e5 disse problemene er \u00e5 migrere til et nytt cluster, og da har vi valgt \u00e5 migrere til Aiven Kafka. Aiven er en leverand\u00f8r som har Kafka som hovedprodukt, det er et av de f\u00f8rste produktene de tilb\u00f8d, og de har opparbeidet seg h\u00f8y kompetanse p\u00e5 drift og oppsett av Kafka i sky. Aiven Kafka er derfor \u00e5 regne som langt mer robust enn Kafka onprem, og blir kontinuerlig vedlikeholdt og forbedret av Aiven. Det er enkelt for oss \u00e5 oppgradere til nyere versjoner av Kafka, og vi drar nytte av alle driftsfordelene med sky. Som f\u00f8lge av Aivens gode APIer er det ogs\u00e5 mulig for NAIS \u00e5 integrere Kafka tettere i plattformen, og vi har flere muligheter for videreutvikling. Kombinasjonen av Aiven Kafka og Kafkarator gj\u00f8r det betydelig enklere for team \u00e5 ta i bruk Aiven Kafka sammenlignet med Kafka onprem. Vi hadde tidligere en ambisjon om at Kafka onprem skulle skrus av f\u00f8r sommeren 2021, men siden vi s\u00e5 at enkelte team ville ha problemer med \u00e5 migrere f\u00f8r det har vi g\u00e5tt vekk fra en hard deadline for dette. Vi vil likevel oppfordre alle som har mulighet til \u00e5 migrere til Aiven Kafka s\u00e5 snart som mulig. Vi har allikevel besluttet at vi stenger for \u00e5 opprette nye topics i Kafka onprem fra 1. juni 2021. Hypotesen er at siden det allikevel er et nytt topic, s\u00e5 er det b\u00e5de enklere og mer fremtidsrettet om det opprettes p\u00e5 Aiven Kafka, og vi \u00f8nsker \u00e5 redusere mengden data som g\u00e5r gjennom Kafka onprem. loginservice \u00a7 Loginservice (ved bruk av Azure AD B2C) tilbyr en fellestjeneste for \"delegated authentication\" for alle borgerrettede apper, dvs mot ID-porten. Token f\u00e5tt ved innlogging via Loginservice gir tilgang til en rekke bakenforliggende tjenester, ved at dette tokenet propageres as-is. En sentral fellestjeneste kan utgj\u00f8re en risiko, b\u00e5de operasjonelt og sikkerhetsmessig. Dersom Loginservice er utilgjengelig er i praksis alle innloggede tjenester p\u00e5 nav.no nede. Ved bruk av loginservice og propagering av id_token as-is s\u00e5 har vi i praksis \"one token to rule them all\" mtp tilgang til APIer. Denne arkitekturen gj\u00f8r at kompromitterte tokens har et stort skadepotensiale, og er heller ikke i tr\u00e5d med \"zero trust\"-prinsippene vi designer systemene v\u00e5re etter. Som med alle andre fellestjenester skapes det sterke koblinger. Endringer i tjenesten medf\u00f8rer et stort behov for koordinering, og dermed bremses farta til teamene. Det \u00e5 holde tjenesten oppdatert er ogs\u00e5 en vedlikeholdsbyrde som stjeler tid fra andre og mere framtidsrettede aktiviteter i plattformteamet. Vi har et \u00f8nske om en sikkerhetsarkitektur hvor hvert enkelt token er spesifikt beregnet til det APIet som man skal kalle. Dette kan vi ikke oppn\u00e5 med en fellestjeneste som Loginservice. I GCP har teamene n\u00e5 f\u00e5tt mulighet til \u00e5 integrere direkte mot ID-porten selv, og NAV ITs strategi sier at teamene skal ta ansvar for alle aspekter ved produktene sine (ogs\u00e5 autentisering). Vi tilbyr ogs\u00e5 \"TokenX\" og andre mekanismer som er i tr\u00e5d med \"zero trust\". Vi vil derfor ikke utvikle mer p\u00e5 Loginservice, men kun s\u00f8rge for \"lyset holdes p\u00e5\" fram til den kan skrus av. En gulrot for teamene som integrerer seg direkte med IDPorten er at de da kan benytte seg av refresh_tokens for en mer s\u00f8ml\u00f8s og brukervennlig opplevelse for brukere p\u00e5 nav.no mtp utl\u00f8pt token. OpenAM (ESSO) \u00a7 OpenAM ESSO (ESSO - ekstern single-sign-on) er en legacy l\u00f8sning for single-sign-on (SSO) p\u00e5 tvers av nav.no applikasjoner basert p\u00e5 en reverse-proxy arkitektur. Dette inneb\u00e6rer at applikasjoner som benytter denne l\u00f8sningen ligger bak en revers-proxy som h\u00e5ndhever autentisering, og applikasjoner henter ut brukerinformasjon ved \u00e5 gj\u00f8re oppslag i OpenAM basert p\u00e5 et sesjonstoken. OpenAM integrerer med IDPorten vha SAML (som er deprekert hos IDPorten til fordel for OpenID Connect). Det er et \u00f8nske om at applikasjoner migrerer seg helt bort fra OpenAM ESSO da dette er en l\u00f8sning som stammer fra en helt annen tid og som har v\u00e6rt ansett som deprekert i NAV i lang tid. Den holdes i live on-prem til de siste applikasjonene er flyttet over til enten direktebasert integrasjon med IDPorten eller til loginservice (som et midlertidig steg). Ambisjonen er \u00e5 skru av OpenAM ESSO til sommeren 2021 for \u00e5 unng\u00e5 \u00e5 utl\u00f8se enda et \u00e5r med support fra Forgerock. OpenAM (ISSO) \u00a7 OpenAM ISSO (intern single-sign-on) en on-prem l\u00f8sning som tilbyr innlogging for NAV ansatte (f.eks saksbehandlere) ved bruk av OpenID Connect (OIDC) og on-prem AD. Applikasjoner som integrerer med OpenAM f\u00e5r tilbake et id_token som benyttes ved kall mot andre APIer. Arkitekturen ved API til API kall for denne l\u00f8sningen ligner den vi har med loginservice, dvs. \"one token to rule them all\" - et token gir tilgang til et stort sett APIer. OpenAM ISSO driftes on-prem ved ATOM og har en sterk avhengighet til AD. Det \u00f8nskelig at flest mulig applikasjoner migrerer seg over til Azure AD som ogs\u00e5 tilbyr innlogging for NAV ansatte vha OIDC. I tillegg til fordelen ved at Azure AD er managed skybasert l\u00f8sning, st\u00f8tter den ogs\u00e5 store deler av OAuth 2.0 spesfikasjonene som gj\u00f8r at vi kan benytte oss av tokens som er spesifikt beregnet p\u00e5 APIet man skal kalle. Vi er klar over at en del APIer baserer seg p\u00e5 \u00e5 motta tokens fra OpenAM ISSO og enda ikke st\u00f8tter Azure AD som tilbyder. Det er derfor viktig at flest mulig APIer ogs\u00e5 implementerer st\u00f8tte for Azure AD tokens, da applikasjoner som benytter OpenAM for innlogging ikke kan switche til Azure AD f\u00f8r avhengighetene deres st\u00f8tter det.","title":"Sunset"},{"location":"legacy/sunset/#sunset-tjenester-som-nais-teamet-gradvis-bygger-ned","text":"Dette er tjenester som ikke videreutvikles av naisteamet, og i noen tilfeller som vil bli avviklet.","title":"Sunset - tjenester som nais-teamet gradvis bygger ned"},{"location":"legacy/sunset/#nais-sbs","text":"nais-clustrene dev-sbs og prod-sbs , ogs\u00e5 kjent som \"selvbetjeningssonen\" \u00f8nsker nais-teamet \u00e5 avvikle til fordel for GCP. Grunnen til dette er at tilsvarende clustre i GCP ( dev-gcp og prod-gcp ) har funksjonslikhet med SBS, og vil i tillegg gi tilgang til: automatisk provisjonering av databaser og b\u00f8tter bedre sikkerhet og mer robuste m\u00f8nster for app-til-app kommunikasjon bedre applikasjonsmetrikker, for eksempel p\u00e5 kall inn og ut av appen helautomatisert konfigurasjon n\u00e5r en app skal eksponeres For brukere av nais betyr det at nye applikasjoner skal legges til GCP fremfor SBS, og at eksisterende applikasjoner i SBS m\u00e5 migreres til GCP. F\u00f8lgende frister gjelder for migrering: Fredag 8. oktober 2021 stenges muligheten for \u00e5 lage nye apper i SBS-clustrene Fredag 1. april 2022 skal alle apper i SBS v\u00e6re migrert til GCP Begge datoene gjelder for b\u00e5de prod-sbs og dev-sbs. Dersom deres team har en applikasjon som ikke er mulig \u00e5 migrere til sky vil det v\u00e6re mulig \u00e5 f\u00e5 unntak fra kravet om migrering. Slike unntak skal godkjennes av IT-direkt\u00f8ren, og det vil komme mer info senere om prosessen for \u00e5 s\u00f8ke om unntak. Vi vil komme tilbake til hvilke l\u00f8sninger som blir tilgjengelig onprem for disse appene som er unntatt migrering.","title":"nais-sbs"},{"location":"legacy/sunset/#sentralisert-abac-axiomatics","text":"I kj\u00f8lvannet av at Team Integrasjon ble oppl\u00f8st har drift av b\u00e5de den tekniske tjenesten, verkt\u00f8yst\u00f8tten, samt forvaltningen av policies i ABAC blitt utf\u00f8rt av \u00e9n person. ABAC er kritisk for tilgangsstyringen i mange tjenester, og inneholder i mange tilfeller domenelogikk. Dette lar seg d\u00e5rlig forene med de effektene vi \u00f8nsker av distribuerte autonome team. Samtidig er policiene som ligger i ABAC implementert i et propriet\u00e6rt spr\u00e5k (Alfa) som gj\u00f8r terskelen for \u00e5 ta over denne logikken h\u00f8yere enn man skulle \u00f8nske. nais-teamet har tatt over ABAC-tjenesten fra Team Integrasjon, og \u00f8nsker \u00e5 desentralisere denne. Det betyr i praksis at teamene som bruker ABAC m\u00e5 ta eierskap til b\u00e5de egne policies og egne instanser av ABAC (alternativt flytte logikken de har i ABAC inn i egne applikasjoner). Se for\u00f8vrig ADR her. Tiln\u00e6rmingen til avvikling av sentralisert ABAC er f\u00f8rst \u00e5 flytte eksisterende implementasjon ut av stash/nexus og flytte til GitHub. Vi flytter samtidig instansen fra WebSphere til nais. S\u00e5 tenker vi \u00e5 splitte ABAC opp i domener, og flytte eierskapet for hver instans til domenet. Om teamene som bruker ABAC det aktuelle domenet \u00f8nsker \u00e5 fortsette \u00e5 bruke Axiomatics-ABAC eller \u00f8nsker \u00e5 implementere logikken direkte i appene sine eller i en egen app er opp til teamene. Uansett hvilket valg de tar vil nais-teamet v\u00e6re tilgjengelig som st\u00f8tte i overgangen.","title":"Sentralisert ABAC (Axiomatics)"},{"location":"legacy/sunset/#naisadeono-devadeono-preprodlocal-oera-qlocal-ingresser","text":"nais.adeo.no erstattes av intern.nav.no dev.adeo.no erstattes av dev.intern.nav.no preprod.local erstattes av dev.intern.nav.no oera-q.local erstattes av dev.nav.no For applikasjoner p\u00e5 GCP er kun de nye ingressene mulig \u00e5 benytte, mens for applikasjoner i FSS er begge mulig \u00e5 bruke. Det er ingen plan om \u00e5 avvikle de gamle ingressene i FSS, men vi anbefaler en gradvis migrering til de nye, og at nye applikasjoner benytter nye ingresser.","title":"nais.adeo.no, dev.adeo.no, preprod.local, oera-q.local (ingresser)"},{"location":"legacy/sunset/#mq","text":"MQ benyttes i dag kun via on-premises servere som driftes og vedlikeholdes av ATOM og Linux. Det er \u00f8nskelig at de applikasjonene som har mulighet heller benytter Aiven Kafka. Vi \u00f8nsker at man g\u00e5r vekk fra MQ fordi dette vil forenkle overgang til offentlig sky og et eventuelt bytte av leverand\u00f8r der. Dette vil ogs\u00e5 forenkle oppsettet for applikasjonene og plattform. Det er dog st\u00f8tte for MQ fra nais-klusterne on-premises og i GCP, men da kun med nye MQ-servere som er satt opp med autentisering. Bruk av ikke-autentisert MQ er definert som et alvorlig sikkerhetshull . Vi \u00f8nsker at alle migrerer sine applikasjoner til enten autentisert MQ, eller aller helst Kafka, s\u00e5 fort som mulig. Bruk av ikke-autentisert MQ vil ikke v\u00e6re mulig fra nais-klusterne fra og med 01.12.2021.","title":"MQ"},{"location":"legacy/sunset/#rookceph","text":"Rook/Ceph er i dag satt opp on-premises og tilgjengelig fra alle nais-klustere der. Disse er ikke tilgjengelig fra GCP og det er heller ikke \u00f8nskelig. L\u00f8sningen driftes og vedlikeholdes av nais, men det er en stor rigg med en del vedlikeholdsarbeid. L\u00f8sningen har ingen fullverdig backup, selv om det gj\u00f8res backup av fysiske volum og noder der Rook/Ceph kj\u00f8rer. Rook/Ceph \u00f8nskes sanert, vi \u00f8nsker derfor at ingen nye apper tar dette i bruk og heller benytter seg av buckets i GCP og ElasticSearch i GCP eller Aiven. Rook/ceph anses som en risiko pga utilstrekkelig backup ettersom et kluster ikke kan relokeres eller flyttes. Det er en uttalt plan for sanering av Rook/Ceph, men l\u00f8sningen vil leve videre med vedlikehold til tilstrekkelig mange applikasjoner har flyttet sine data ut i offentlig sky.","title":"Rook/Ceph"},{"location":"legacy/sunset/#kafka-onprem","text":"Kafka er i dag tilgjengelig b\u00e5de som en tjeneste fra Aiven som kj\u00f8rer i GCP og som en tjeneste vi kj\u00f8rer i egne datasentre. Begge variantene av kafka kan n\u00e5s fra alle nais-clustre. Kafka onprem driftes og vedlikeholdes av en h\u00e5ndfull personer, hvor ingen har Kafka som sin hovedoppgave. Det er ogs\u00e5 identifisert flere potensielle problemer som ikke lar seg l\u00f8se: Av lisensmessige \u00e5rsaker kan vi ikke oppgradere clusteret in-place, s\u00e5 vi er bundet til n\u00e5v\u00e6rende versjon. Clusteret er satt opp med felles diskl\u00f8sning (SAN), noe som gj\u00f8r at alle nodene har et delt point-of-failure. Dette kan i visse feilsituasjoner \u00f8ke sjansene for tap av data. Av historiske \u00e5rsaker er clusteret \"feilkonfigurert\" p\u00e5 en m\u00e5te som gj\u00f8r at consumer offset topicet er veldig stort. Dette f\u00f8rer til lange recovery tider og potensiale for \u00e5 miste offsets i feilsituasjoner. L\u00f8sningen p\u00e5 disse problemene er \u00e5 migrere til et nytt cluster, og da har vi valgt \u00e5 migrere til Aiven Kafka. Aiven er en leverand\u00f8r som har Kafka som hovedprodukt, det er et av de f\u00f8rste produktene de tilb\u00f8d, og de har opparbeidet seg h\u00f8y kompetanse p\u00e5 drift og oppsett av Kafka i sky. Aiven Kafka er derfor \u00e5 regne som langt mer robust enn Kafka onprem, og blir kontinuerlig vedlikeholdt og forbedret av Aiven. Det er enkelt for oss \u00e5 oppgradere til nyere versjoner av Kafka, og vi drar nytte av alle driftsfordelene med sky. Som f\u00f8lge av Aivens gode APIer er det ogs\u00e5 mulig for NAIS \u00e5 integrere Kafka tettere i plattformen, og vi har flere muligheter for videreutvikling. Kombinasjonen av Aiven Kafka og Kafkarator gj\u00f8r det betydelig enklere for team \u00e5 ta i bruk Aiven Kafka sammenlignet med Kafka onprem. Vi hadde tidligere en ambisjon om at Kafka onprem skulle skrus av f\u00f8r sommeren 2021, men siden vi s\u00e5 at enkelte team ville ha problemer med \u00e5 migrere f\u00f8r det har vi g\u00e5tt vekk fra en hard deadline for dette. Vi vil likevel oppfordre alle som har mulighet til \u00e5 migrere til Aiven Kafka s\u00e5 snart som mulig. Vi har allikevel besluttet at vi stenger for \u00e5 opprette nye topics i Kafka onprem fra 1. juni 2021. Hypotesen er at siden det allikevel er et nytt topic, s\u00e5 er det b\u00e5de enklere og mer fremtidsrettet om det opprettes p\u00e5 Aiven Kafka, og vi \u00f8nsker \u00e5 redusere mengden data som g\u00e5r gjennom Kafka onprem.","title":"Kafka onprem"},{"location":"legacy/sunset/#loginservice","text":"Loginservice (ved bruk av Azure AD B2C) tilbyr en fellestjeneste for \"delegated authentication\" for alle borgerrettede apper, dvs mot ID-porten. Token f\u00e5tt ved innlogging via Loginservice gir tilgang til en rekke bakenforliggende tjenester, ved at dette tokenet propageres as-is. En sentral fellestjeneste kan utgj\u00f8re en risiko, b\u00e5de operasjonelt og sikkerhetsmessig. Dersom Loginservice er utilgjengelig er i praksis alle innloggede tjenester p\u00e5 nav.no nede. Ved bruk av loginservice og propagering av id_token as-is s\u00e5 har vi i praksis \"one token to rule them all\" mtp tilgang til APIer. Denne arkitekturen gj\u00f8r at kompromitterte tokens har et stort skadepotensiale, og er heller ikke i tr\u00e5d med \"zero trust\"-prinsippene vi designer systemene v\u00e5re etter. Som med alle andre fellestjenester skapes det sterke koblinger. Endringer i tjenesten medf\u00f8rer et stort behov for koordinering, og dermed bremses farta til teamene. Det \u00e5 holde tjenesten oppdatert er ogs\u00e5 en vedlikeholdsbyrde som stjeler tid fra andre og mere framtidsrettede aktiviteter i plattformteamet. Vi har et \u00f8nske om en sikkerhetsarkitektur hvor hvert enkelt token er spesifikt beregnet til det APIet som man skal kalle. Dette kan vi ikke oppn\u00e5 med en fellestjeneste som Loginservice. I GCP har teamene n\u00e5 f\u00e5tt mulighet til \u00e5 integrere direkte mot ID-porten selv, og NAV ITs strategi sier at teamene skal ta ansvar for alle aspekter ved produktene sine (ogs\u00e5 autentisering). Vi tilbyr ogs\u00e5 \"TokenX\" og andre mekanismer som er i tr\u00e5d med \"zero trust\". Vi vil derfor ikke utvikle mer p\u00e5 Loginservice, men kun s\u00f8rge for \"lyset holdes p\u00e5\" fram til den kan skrus av. En gulrot for teamene som integrerer seg direkte med IDPorten er at de da kan benytte seg av refresh_tokens for en mer s\u00f8ml\u00f8s og brukervennlig opplevelse for brukere p\u00e5 nav.no mtp utl\u00f8pt token.","title":"loginservice"},{"location":"legacy/sunset/#openam-esso","text":"OpenAM ESSO (ESSO - ekstern single-sign-on) er en legacy l\u00f8sning for single-sign-on (SSO) p\u00e5 tvers av nav.no applikasjoner basert p\u00e5 en reverse-proxy arkitektur. Dette inneb\u00e6rer at applikasjoner som benytter denne l\u00f8sningen ligger bak en revers-proxy som h\u00e5ndhever autentisering, og applikasjoner henter ut brukerinformasjon ved \u00e5 gj\u00f8re oppslag i OpenAM basert p\u00e5 et sesjonstoken. OpenAM integrerer med IDPorten vha SAML (som er deprekert hos IDPorten til fordel for OpenID Connect). Det er et \u00f8nske om at applikasjoner migrerer seg helt bort fra OpenAM ESSO da dette er en l\u00f8sning som stammer fra en helt annen tid og som har v\u00e6rt ansett som deprekert i NAV i lang tid. Den holdes i live on-prem til de siste applikasjonene er flyttet over til enten direktebasert integrasjon med IDPorten eller til loginservice (som et midlertidig steg). Ambisjonen er \u00e5 skru av OpenAM ESSO til sommeren 2021 for \u00e5 unng\u00e5 \u00e5 utl\u00f8se enda et \u00e5r med support fra Forgerock.","title":"OpenAM (ESSO)"},{"location":"legacy/sunset/#openam-isso","text":"OpenAM ISSO (intern single-sign-on) en on-prem l\u00f8sning som tilbyr innlogging for NAV ansatte (f.eks saksbehandlere) ved bruk av OpenID Connect (OIDC) og on-prem AD. Applikasjoner som integrerer med OpenAM f\u00e5r tilbake et id_token som benyttes ved kall mot andre APIer. Arkitekturen ved API til API kall for denne l\u00f8sningen ligner den vi har med loginservice, dvs. \"one token to rule them all\" - et token gir tilgang til et stort sett APIer. OpenAM ISSO driftes on-prem ved ATOM og har en sterk avhengighet til AD. Det \u00f8nskelig at flest mulig applikasjoner migrerer seg over til Azure AD som ogs\u00e5 tilbyr innlogging for NAV ansatte vha OIDC. I tillegg til fordelen ved at Azure AD er managed skybasert l\u00f8sning, st\u00f8tter den ogs\u00e5 store deler av OAuth 2.0 spesfikasjonene som gj\u00f8r at vi kan benytte oss av tokens som er spesifikt beregnet p\u00e5 APIet man skal kalle. Vi er klar over at en del APIer baserer seg p\u00e5 \u00e5 motta tokens fra OpenAM ISSO og enda ikke st\u00f8tter Azure AD som tilbyder. Det er derfor viktig at flest mulig APIer ogs\u00e5 implementerer st\u00f8tte for Azure AD tokens, da applikasjoner som benytter OpenAM for innlogging ikke kan switche til Azure AD f\u00f8r avhengighetene deres st\u00f8tter det.","title":"OpenAM (ISSO)"},{"location":"legal/app-pvk/","text":"Application privacy impact assessments (PVK) \u00a7 Before deploying any application that processes and/or stores data to nais, a privacy assessment (PVK) must be conducted. This PVK should be kept reasonably up-to-date through the life cycle of the application. Refer to the PVK process documentation for details (NAV internal link). Ideally, the language in your PVK should be tech stack agnostic, meaning there will not be any need to change this document as components are changed or the application is moved between on-premises and cloud clusters . Any technology specific considerations belong in the risk assessment .","title":"Application privacy impact assessments (PVK)"},{"location":"legal/app-pvk/#application-privacy-impact-assessments-pvk","text":"Before deploying any application that processes and/or stores data to nais, a privacy assessment (PVK) must be conducted. This PVK should be kept reasonably up-to-date through the life cycle of the application. Refer to the PVK process documentation for details (NAV internal link). Ideally, the language in your PVK should be tech stack agnostic, meaning there will not be any need to change this document as components are changed or the application is moved between on-premises and cloud clusters . Any technology specific considerations belong in the risk assessment .","title":"Application privacy impact assessments (PVK)"},{"location":"legal/app-ros/","text":"Application risk assessments (ROS) \u00a7 Before deploying any application to nais, the team owning the application must conduct a risk assessment (ROS) in the TryggNok tool (NAV internal link). This risk assessment should be kept reasonably up-to-date through the lifetime of the application. When conducting the risk assessment, any assumptions about underlying component risks can be verified by checking the Platform risk assessments . These may also be referred to in the application risk assessment. For details about the ROS process, see the ROS documentation (NAV internal link).","title":"Application risk assessments (ROS)"},{"location":"legal/app-ros/#application-risk-assessments-ros","text":"Before deploying any application to nais, the team owning the application must conduct a risk assessment (ROS) in the TryggNok tool (NAV internal link). This risk assessment should be kept reasonably up-to-date through the lifetime of the application. When conducting the risk assessment, any assumptions about underlying component risks can be verified by checking the Platform risk assessments . These may also be referred to in the application risk assessment. For details about the ROS process, see the ROS documentation (NAV internal link).","title":"Application risk assessments (ROS)"},{"location":"legal/arkivloven/","text":"Arkivloven \u00a7 TLDR \u00a7 Evaluate whether your application\u00b4s information storage needs are affected by the paragraphs in The Archival Act, and whether you might need to operate with a separate database physically placed on Norwegian soil (for example NAV\u00b4s on-premise data center). If it has archival value, the documents must be stored in systems in Norway but copies can be stored abroad. As long as the documents in and out of your applications end up in archives that are stored in Norway, a migration to GCP should not pose any problems. There is also an ongoing issue as to whether an in-built archive will be sufficient for our purpose. I.e. the obligatory archive can be sustained in the domain applications, since much of the information there is necessary to archive as well. There is a new proposal regarding public cloud infrastructure which hopefully will be ready this autumn as well. This proposal will allow storage on public cloud systems, to a degree. See [New public archive proposal]. Background \u00a7 When moving to GCP or other public cloud providers not offering data centers in Norway, an evaluation of the documentation requirements for the application must be done. There are several important regulations that apply to this documentation, as stated by The Archival Act For Public Archives of 1992 (referred to as The Archival Act in the following). The requirements are set to ensure documents that contain judicial or important administrative information are stored and made available for future inspection (\u00a71) NAV is, as a public body and central government agency, obligated to keep archival records to make sure the documents are stored safely as information sources for present and future (\u00a72). The Public Information Act states that everyone shall have access to case documents to facilitate transparency and as such strenghtens: Freedom of expression and information Democratic participation Security under the law Public trust and control There are three requirements that need to be in place for a document to be considered obligatory for archiving: There must exist a case document for the public body The document must have been sent from or received by the public body The document must be case handled and have value as documentation Public bodies and agencys are obligated to archive all documents that are created during its handling of business: The documents have value as documentation or are being case handled The documents must be archived whether they have been used externally or are created for internal use The documents that are created electronically must be made available for The National Archives The Archival Act states: A document is defined, by The Archival Act, as a logically limited source of information stored no a media for subsequent reading, listening, viewing or transfer. An archive is defined as documents that are created during business handling. Obligatory archiving is defined as storage for documenting the handling. General regulations when migrating to public cloud providers \u00a7 Without a motion or expressed consent from the National Archivist the records cannot (\u00a79): Be moved out of country Be deleted Be redacted or edited (if it applies to the obligatory documentation) NAV has requested and received a clarification regarding this from the National Archival Services, which means we can store documentation abroad as long as they are archived in Norway. See letter of clarification . Documents that requires archival storage must be stored on a media and in a format that fulfills the necessary requirements for durability and accessability (\u00a76). I.e. these documents must be stored in a way that ensures authenticity, reliability, integrity and usability. Rules must be made for deletion of all documentation (\u00a716). Public bodies are to deliver older and finished archives to an archival depot (\u00a718). Sourced in large part from documentation for SalesForce migration to cloud: https://confluence.adeo.no/display/PTC/Arkivering+og+dokumentasjon+i+Salesforce All paragraphs of law except \u00a716 and \u00a718 are from The Archival Act: The Archival Act For Public Archives Paragraphs \u00a716 and \u00a718: The Regulation concerning Public Archives Archival proposal for public cloud solutions: New public archive proposal","title":"Arkivloven"},{"location":"legal/arkivloven/#arkivloven","text":"","title":"Arkivloven"},{"location":"legal/arkivloven/#tldr","text":"Evaluate whether your application\u00b4s information storage needs are affected by the paragraphs in The Archival Act, and whether you might need to operate with a separate database physically placed on Norwegian soil (for example NAV\u00b4s on-premise data center). If it has archival value, the documents must be stored in systems in Norway but copies can be stored abroad. As long as the documents in and out of your applications end up in archives that are stored in Norway, a migration to GCP should not pose any problems. There is also an ongoing issue as to whether an in-built archive will be sufficient for our purpose. I.e. the obligatory archive can be sustained in the domain applications, since much of the information there is necessary to archive as well. There is a new proposal regarding public cloud infrastructure which hopefully will be ready this autumn as well. This proposal will allow storage on public cloud systems, to a degree. See [New public archive proposal].","title":"TLDR"},{"location":"legal/arkivloven/#background","text":"When moving to GCP or other public cloud providers not offering data centers in Norway, an evaluation of the documentation requirements for the application must be done. There are several important regulations that apply to this documentation, as stated by The Archival Act For Public Archives of 1992 (referred to as The Archival Act in the following). The requirements are set to ensure documents that contain judicial or important administrative information are stored and made available for future inspection (\u00a71) NAV is, as a public body and central government agency, obligated to keep archival records to make sure the documents are stored safely as information sources for present and future (\u00a72). The Public Information Act states that everyone shall have access to case documents to facilitate transparency and as such strenghtens: Freedom of expression and information Democratic participation Security under the law Public trust and control There are three requirements that need to be in place for a document to be considered obligatory for archiving: There must exist a case document for the public body The document must have been sent from or received by the public body The document must be case handled and have value as documentation Public bodies and agencys are obligated to archive all documents that are created during its handling of business: The documents have value as documentation or are being case handled The documents must be archived whether they have been used externally or are created for internal use The documents that are created electronically must be made available for The National Archives The Archival Act states: A document is defined, by The Archival Act, as a logically limited source of information stored no a media for subsequent reading, listening, viewing or transfer. An archive is defined as documents that are created during business handling. Obligatory archiving is defined as storage for documenting the handling.","title":"Background"},{"location":"legal/arkivloven/#general-regulations-when-migrating-to-public-cloud-providers","text":"Without a motion or expressed consent from the National Archivist the records cannot (\u00a79): Be moved out of country Be deleted Be redacted or edited (if it applies to the obligatory documentation) NAV has requested and received a clarification regarding this from the National Archival Services, which means we can store documentation abroad as long as they are archived in Norway. See letter of clarification . Documents that requires archival storage must be stored on a media and in a format that fulfills the necessary requirements for durability and accessability (\u00a76). I.e. these documents must be stored in a way that ensures authenticity, reliability, integrity and usability. Rules must be made for deletion of all documentation (\u00a716). Public bodies are to deliver older and finished archives to an archival depot (\u00a718). Sourced in large part from documentation for SalesForce migration to cloud: https://confluence.adeo.no/display/PTC/Arkivering+og+dokumentasjon+i+Salesforce All paragraphs of law except \u00a716 and \u00a718 are from The Archival Act: The Archival Act For Public Archives Paragraphs \u00a716 and \u00a718: The Regulation concerning Public Archives Archival proposal for public cloud solutions: New public archive proposal","title":"General regulations when migrating to public cloud providers"},{"location":"legal/nais-pvk/","text":"Platform privacy impact assessments (PVK) \u00a7 While nais platform components are used by the teams, the platform in itself is \"data agnostic\" - that is, the platform team does not know what kind of data is stored and processed on the platform. As such, it is up to each team to perform a privacy assessment for their data processing and storage needs. The exception to the data agnostic rule is data that the platform itself is the processor of - such as configuration data and certain system and access logs (not including application logs, which falls under each individual team's purview). Thus, the only personally identifiable information (PII) that the platform processes are access logs for developers at NAV. A PVK for this data processing has been conducted. For details on what each team's responsibilites are with respect to data processing on the platform, refer to roles and responsibilities .","title":"Platform privacy impact assessments (PVK)"},{"location":"legal/nais-pvk/#platform-privacy-impact-assessments-pvk","text":"While nais platform components are used by the teams, the platform in itself is \"data agnostic\" - that is, the platform team does not know what kind of data is stored and processed on the platform. As such, it is up to each team to perform a privacy assessment for their data processing and storage needs. The exception to the data agnostic rule is data that the platform itself is the processor of - such as configuration data and certain system and access logs (not including application logs, which falls under each individual team's purview). Thus, the only personally identifiable information (PII) that the platform processes are access logs for developers at NAV. A PVK for this data processing has been conducted. For details on what each team's responsibilites are with respect to data processing on the platform, refer to roles and responsibilities .","title":"Platform privacy impact assessments (PVK)"},{"location":"legal/nais-ros/","text":"Platform risk assessments (ROS) \u00a7 The nais team has conducted the following risk assessments: GCP Lagring av data (Buckets og Postgres) GCP Tilgangskontrolloppsett Tilgang til Kafka fra GCP Logger p\u00e5 GCP Google Kubernetes Engine Sentry i GCP Aiven Kafka - Leverand\u00f8rvurdering Aiven ElasticSearch Google Secret Manager (Note that all risk assessments link to the NAV internal tool TryggNok.)","title":"Platform risk assessments (ROS)"},{"location":"legal/nais-ros/#platform-risk-assessments-ros","text":"The nais team has conducted the following risk assessments: GCP Lagring av data (Buckets og Postgres) GCP Tilgangskontrolloppsett Tilgang til Kafka fra GCP Logger p\u00e5 GCP Google Kubernetes Engine Sentry i GCP Aiven Kafka - Leverand\u00f8rvurdering Aiven ElasticSearch Google Secret Manager (Note that all risk assessments link to the NAV internal tool TryggNok.)","title":"Platform risk assessments (ROS)"},{"location":"legal/roles-responsibilities/","text":"Responsibilities - nais and teams \u00a7 \"Nais\" is a term that denotes a set of services offered by the nais team which the product teams can use when building their products. The nais services are built on infrastructure and underlying services offered by different infrastructure teams and cloud platform vendors. In general, responsibilities for services are separated along these lines: Cloud platform vendors (and internal infrastructure teams) are responsible for keeping the infrastructure and underlying platforms running. The nais team is responsible for ensuring the nais services are up and running and available for use by the product teams. The product teams are responsible for their own applications (including business logic, code, data, etc) and for how they use the nais services. As a general rule the lines of ownership goes along the project lines. The services running in team-projects are the responsibility of the team. Thus the nais-team owns the usage of cloud services running in the nais-project. This includes kubernetes, load-balancing and DNS. What constitutes a nais service, an underlying service or what is owned by a product team varies from case to case. In some instances, the product team is a user of common resources owned by the nais team (e.g. the ELK stack, or the kubernetes clusters). In these cases the nais team operates the resources and is responsible for the service availability. In other cases the product teams provision their own resources directly from the underlying vendors. In these cases, the product teams have greater direct control over the service. The division of responsibility in the different cases is best described through examples: About security and privacy when using platform services \u00a7 In general, the necessary agreements and assessments regarding security and privacy for use of the NAIS platform in GCP are in place. A summary can be found in: NAIS on GCP - privacy and security . This document is only available for NAV employees and in Norwegian. A security assessment ( ROS ) has been carried out covering the services that NAIS offers. Teams that wish to use NAIS are responsible for assessing whether the security is sufficient for their use. Most safety assessments are documented in the PowerApps application TryggNok (internal link). Contact the NAIS team if you can't find a relevant security assessment. In NAV, we conduct Privacy Impact Assessments ( PVK ) to document that we operate in accordance with GDPR. Each team must have a PVK that covers any processing of personal data that may occur in their applications. Adopting a new technology may also impact privacy, in which case the team should update their PVK. ROS will often include information security, and can provide support in the completion of the PVK. If the team wants to process personal data in a new way, the existing PVK might not cover that purpose. This requires a new PVK in order to document the purpose and legal grounds for the treatment. A legal coach from \"juridisk seksjon\" can assist with such an assessment. Postgres on GCP \u00a7 Developers are responsible for their own credentials The product team is responsible for their applications' service accounts and the service account credentials The product team is responsible for the data in the database The product team is responsible for how the database is configured (e.g how backup is configured) The product team is responsible for monitoring the postgres instance, and acting upon the alerts. The product team is responsible for knowing how to restore a backup The product team is responsible for informing the nais team if they need any changes in tooling for provisioning and operating the database The nais team is responsible for aggregating and prioritizing requirements for changes in provisioning and operating tooling The nais team is responsible for which configuration options are offered to the product teams, including setting \"sane defaults\" The nais team is responsible for developer access control The nais team is responsible for the tools they make for configuring and operating the databases The platform vendor (GCP) is responsible for access control between applications and databases The platform vendor is responsible for operating the underlying Postgres service and its infrastructure The platform vendor is responsible for operating any additional required services offered by the platform (e.g. backup services) Metrics and alerts \u00a7 The product team is responsible for making metrics from their applications available The product team is responsible for configuring alerts The nais team is responsible for collecting metrics in prometheus and influx The nais team is responsible for operating prometheus and influx The nais team is responsible for ensuring that correctly configured alerts are sent The infrastructure team (NAV internal) is responsible for operating the underlying infrastructure","title":"Roles and responsibilities"},{"location":"legal/roles-responsibilities/#responsibilities-nais-and-teams","text":"\"Nais\" is a term that denotes a set of services offered by the nais team which the product teams can use when building their products. The nais services are built on infrastructure and underlying services offered by different infrastructure teams and cloud platform vendors. In general, responsibilities for services are separated along these lines: Cloud platform vendors (and internal infrastructure teams) are responsible for keeping the infrastructure and underlying platforms running. The nais team is responsible for ensuring the nais services are up and running and available for use by the product teams. The product teams are responsible for their own applications (including business logic, code, data, etc) and for how they use the nais services. As a general rule the lines of ownership goes along the project lines. The services running in team-projects are the responsibility of the team. Thus the nais-team owns the usage of cloud services running in the nais-project. This includes kubernetes, load-balancing and DNS. What constitutes a nais service, an underlying service or what is owned by a product team varies from case to case. In some instances, the product team is a user of common resources owned by the nais team (e.g. the ELK stack, or the kubernetes clusters). In these cases the nais team operates the resources and is responsible for the service availability. In other cases the product teams provision their own resources directly from the underlying vendors. In these cases, the product teams have greater direct control over the service. The division of responsibility in the different cases is best described through examples:","title":"Responsibilities - nais and teams"},{"location":"legal/roles-responsibilities/#about-security-and-privacy-when-using-platform-services","text":"In general, the necessary agreements and assessments regarding security and privacy for use of the NAIS platform in GCP are in place. A summary can be found in: NAIS on GCP - privacy and security . This document is only available for NAV employees and in Norwegian. A security assessment ( ROS ) has been carried out covering the services that NAIS offers. Teams that wish to use NAIS are responsible for assessing whether the security is sufficient for their use. Most safety assessments are documented in the PowerApps application TryggNok (internal link). Contact the NAIS team if you can't find a relevant security assessment. In NAV, we conduct Privacy Impact Assessments ( PVK ) to document that we operate in accordance with GDPR. Each team must have a PVK that covers any processing of personal data that may occur in their applications. Adopting a new technology may also impact privacy, in which case the team should update their PVK. ROS will often include information security, and can provide support in the completion of the PVK. If the team wants to process personal data in a new way, the existing PVK might not cover that purpose. This requires a new PVK in order to document the purpose and legal grounds for the treatment. A legal coach from \"juridisk seksjon\" can assist with such an assessment.","title":"About security and privacy when using platform services"},{"location":"legal/roles-responsibilities/#postgres-on-gcp","text":"Developers are responsible for their own credentials The product team is responsible for their applications' service accounts and the service account credentials The product team is responsible for the data in the database The product team is responsible for how the database is configured (e.g how backup is configured) The product team is responsible for monitoring the postgres instance, and acting upon the alerts. The product team is responsible for knowing how to restore a backup The product team is responsible for informing the nais team if they need any changes in tooling for provisioning and operating the database The nais team is responsible for aggregating and prioritizing requirements for changes in provisioning and operating tooling The nais team is responsible for which configuration options are offered to the product teams, including setting \"sane defaults\" The nais team is responsible for developer access control The nais team is responsible for the tools they make for configuring and operating the databases The platform vendor (GCP) is responsible for access control between applications and databases The platform vendor is responsible for operating the underlying Postgres service and its infrastructure The platform vendor is responsible for operating any additional required services offered by the platform (e.g. backup services)","title":"Postgres on GCP"},{"location":"legal/roles-responsibilities/#metrics-and-alerts","text":"The product team is responsible for making metrics from their applications available The product team is responsible for configuring alerts The nais team is responsible for collecting metrics in prometheus and influx The nais team is responsible for operating prometheus and influx The nais team is responsible for ensuring that correctly configured alerts are sent The infrastructure team (NAV internal) is responsible for operating the underlying infrastructure","title":"Metrics and alerts"},{"location":"legal/sikkerhetsloven/","text":"Sikkerhetsloven \u00a7 (Coming soon)","title":"Sikkerhetsloven"},{"location":"legal/sikkerhetsloven/#sikkerhetsloven","text":"(Coming soon)","title":"Sikkerhetsloven"},{"location":"legal/dpa/","text":"Vendor Data Processor Agreements \u00a7 Before we can use any *aaS-service that processes or stores any user data, NAV needs to have a signed Data Processor Agreement (DPA) with the vendor in question. These agreements are in place for the vendors supplying components to the nais platform. Google Cloud Platform DPA Microsoft Azure DPA Aiven DPA","title":"DPA Overview"},{"location":"legal/dpa/#vendor-data-processor-agreements","text":"Before we can use any *aaS-service that processes or stores any user data, NAV needs to have a signed Data Processor Agreement (DPA) with the vendor in question. These agreements are in place for the vendors supplying components to the nais platform. Google Cloud Platform DPA Microsoft Azure DPA Aiven DPA","title":"Vendor Data Processor Agreements"},{"location":"legal/dpa/aiven-dpa/","text":"Aiven Data Processor Agreement \u00a7 NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Support is to be given from Aiven supportcenters in EU/E\u00d8S. This is already the case regarding Kafka service at Aiven, so the DPA between NAV and Aiven is still valid. Other relevant documents: Risk analysis related to the Kafka service and underlaying platform","title":"Aiven Data Processor Agreement"},{"location":"legal/dpa/aiven-dpa/#aiven-data-processor-agreement","text":"NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Support is to be given from Aiven supportcenters in EU/E\u00d8S. This is already the case regarding Kafka service at Aiven, so the DPA between NAV and Aiven is still valid. Other relevant documents: Risk analysis related to the Kafka service and underlaying platform","title":"Aiven Data Processor Agreement"},{"location":"legal/dpa/azure-dpa/","text":"Azure Data Processor Agreement \u00a7 Microsoft has Online Service Terms (OST) that govern Customer\u2019s use of the Online Services and the Microsoft Online Services Data Protection Addendum (DPA) that sets forth the parties obligations with respect to the processing and security of Customer Data and Personal Data by the Online Services. In addition to the generic OST a set of online services have their own service specific OST. In the event of any conflict or inconsistency between the DPA and any other terms in Customer\u2019s volume licensing agreement (including the Product Terms or the Online Services Terms), the DPA shall prevail. When moving to Microsoft Azure risk assessment for your application must be updated. Beslutningsrapport - Avtale med Microsoft lists the risks related to NAVs use of the services available in Microsoft Azure regarding privacy/GDPR and information security in the overall platform. NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Apart from this, the DPA with Microsoft is still valid. Other relevant documents: Online Services Data Protection Addendum (DPA) Microsoft Online Service Terms (OST)","title":"Azure Data Processor Agreement"},{"location":"legal/dpa/azure-dpa/#azure-data-processor-agreement","text":"Microsoft has Online Service Terms (OST) that govern Customer\u2019s use of the Online Services and the Microsoft Online Services Data Protection Addendum (DPA) that sets forth the parties obligations with respect to the processing and security of Customer Data and Personal Data by the Online Services. In addition to the generic OST a set of online services have their own service specific OST. In the event of any conflict or inconsistency between the DPA and any other terms in Customer\u2019s volume licensing agreement (including the Product Terms or the Online Services Terms), the DPA shall prevail. When moving to Microsoft Azure risk assessment for your application must be updated. Beslutningsrapport - Avtale med Microsoft lists the risks related to NAVs use of the services available in Microsoft Azure regarding privacy/GDPR and information security in the overall platform. NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Apart from this, the DPA with Microsoft is still valid. Other relevant documents: Online Services Data Protection Addendum (DPA) Microsoft Online Service Terms (OST)","title":"Azure Data Processor Agreement"},{"location":"legal/dpa/gcp-dpa/","text":"GCP Data Processor Agreement \u00a7 The Data Protection Agreement, DPA, (named \"Data Processing and Security Terms\" by Google) for Google Cloud Platform is part of the Business agreement between NAV and Google Irland Limited. This Business agreement (NAV internal link) is an offline variant which governs our use of the Google Cloud Platform Services. The offline terms uses the online terms as a base, but extend the online terms with for instance English law as the governing law (not US law) and 60 days (not 30) for NAV to object to changes in the terms. When updating the risk assessments for your application, read the Beslutningsnotat Google avtale (NAV internal link). This document contains NAVs risks related to GCP and privacy/GDPR, and the risks most highly associated with information security in the Google Cloud platform in general (in addition the services NAV use in GCP has their own risk assessments ). NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Apart from this, DPA with Google is still valid. Note that any use of Google Support that requires Support staff to access production data will need to be cleared with the NAV CIO in advance. The Online data processing terms (DPA) can be found here .","title":"GCP Data Processor Agreement"},{"location":"legal/dpa/gcp-dpa/#gcp-data-processor-agreement","text":"The Data Protection Agreement, DPA, (named \"Data Processing and Security Terms\" by Google) for Google Cloud Platform is part of the Business agreement between NAV and Google Irland Limited. This Business agreement (NAV internal link) is an offline variant which governs our use of the Google Cloud Platform Services. The offline terms uses the online terms as a base, but extend the online terms with for instance English law as the governing law (not US law) and 60 days (not 30) for NAV to object to changes in the terms. When updating the risk assessments for your application, read the Beslutningsnotat Google avtale (NAV internal link). This document contains NAVs risks related to GCP and privacy/GDPR, and the risks most highly associated with information security in the Google Cloud platform in general (in addition the services NAV use in GCP has their own risk assessments ). NAV has, based on the Schrems II verdict that invalidated The EU-US Privacy Shield transfer mechanism, decided that all cloud services and all storage of data in cloud services shall take place in the EU / EEA. Apart from this, DPA with Google is still valid. Note that any use of Google Support that requires Support staff to access production data will need to be cleared with the NAV CIO in advance. The Online data processing terms (DPA) can be found here .","title":"GCP Data Processor Agreement"},{"location":"nais-application/access-policy/","text":"Access Policy \u00a7 Info Network policies are only applied in GCP clusters. However, inbound rules for authorization in the context of TokenX or Azure AD apply to all clusters. Access Policy \u00a7 Access policies express which applications and services you are able to communicate with, both inbound and outbound. The default policy is to deny all incoming and outgoing traffic for your application, meaning you must be conscious of which services/application you consume, and who your consumers are. Warning The Access policies only apply when communicating interally within the cluster with service discovery . Outbound requests to ingresses are regarded as external hosts, even if these ingresses exist in the same cluster. Analogously, inbound access policies are thus not enforced for requests coming through exposed ingresses. Inbound rules \u00a7 Inbound rules specifies what other applications in the same cluster your application receives traffic from. Receive requests from other app in the same namespace \u00a7 For app app-a to be able to receive incoming requests from app-b in the same cluster and the same namespace, this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : inbound : rules : - application : app-b Receive requests from other app in the another namespace \u00a7 For app app-a to be able to receive incoming requests from app-b in the same cluster but another namespace ( othernamespace ), this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : inbound : rules : - application : app-b namespace : othernamespace Outbound rules \u00a7 Inbound rules specifies what other applications your application receives traffic from. spec.accessPolicy.outbound.rules specifies which applications in the same cluster to open for. To open for external applications, use the field spec.accessPolicy.outbound.external . Send requests to other app in the same namespace \u00a7 For app app-a to be able to send requests to app-b in the same cluster and the same namespace, this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : app-b Send requests to other app in the another namespace \u00a7 For app app-a to be able to send requests requests to app-b in the same cluster but in another namespace ( othernamespace ), this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : app-b namespace : othernamespace External services \u00a7 In order to send requests to services outside of the cluster, external.host is needed: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : external : - host : www.external-application.com Global Service Entries There are some services that are automatically added to the mesh in dev-gcp and prod-gcp (search for global_serviceentries ). Advanced: Resources created by Naiserator \u00a7 The previous application manifest examples will create Kubernetes Network Policies. Kubernetes Network Policy \u00a7 Default policy Every app created will have this default network policy that allows traffic to Linkerd and kube-dns. It also allows incoming traffic from the Linkerd control plane and from tap and prometheus in the linkerd-viz namespace. This is what enables monitoring via the linkerd dashboard. These policies will be created for every app, also those who don't have any access policies specified. apiVersion : extensions/v1beta1 kind : NetworkPolicy metadata : labels : app : appname team : teamname name : appname namespace : teamname spec : egress : - to : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - namespaceSelector : {} podSelector : matchLabels : k8s-app : kube-dns - ipBlock : cidr : 0.0.0.0/0 except : - 10.6.0.0/15 - 172.16.0.0/12 - 192.168.0.0/16 ingress : - from : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : tap - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : prometheus podSelector : matchLabels : app : appname policyTypes : - Ingress - Egress Kubernetes network policies The applications specified in spec.accessPolicy.inbound.rules and spec.accessPolicy.outbound.rules will append these fields to the default Network Policy: apiVersion : extensions/v1beta1 kind : NetworkPolicy ... spec : egress : - to : ... - namespaceSelector : matchLabels : name : othernamespace podSelector : matchLabels : app : app-b - podSelector : matchLabels : app : app-b - from : - namespaceSelector : matchLabels : name : othernamespace podSelector : matchLabels : app : app-b - podSelector : matchLabels : app : app-b podSelector : matchLabels : app : appname policyTypes : - Egress - Ingress Info Note that for namespace match labels to work, the namespaces must be labeled with name: namespacename . kube-system should be labeled accordingly for the default rule that allows traffic to kube-dns , but in GCP, the label is removed by some job in regular intervals...","title":"Access policy"},{"location":"nais-application/access-policy/#access-policy","text":"Info Network policies are only applied in GCP clusters. However, inbound rules for authorization in the context of TokenX or Azure AD apply to all clusters.","title":"Access Policy"},{"location":"nais-application/access-policy/#access-policy_1","text":"Access policies express which applications and services you are able to communicate with, both inbound and outbound. The default policy is to deny all incoming and outgoing traffic for your application, meaning you must be conscious of which services/application you consume, and who your consumers are. Warning The Access policies only apply when communicating interally within the cluster with service discovery . Outbound requests to ingresses are regarded as external hosts, even if these ingresses exist in the same cluster. Analogously, inbound access policies are thus not enforced for requests coming through exposed ingresses.","title":"Access Policy"},{"location":"nais-application/access-policy/#inbound-rules","text":"Inbound rules specifies what other applications in the same cluster your application receives traffic from.","title":"Inbound rules"},{"location":"nais-application/access-policy/#receive-requests-from-other-app-in-the-same-namespace","text":"For app app-a to be able to receive incoming requests from app-b in the same cluster and the same namespace, this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : inbound : rules : - application : app-b","title":"Receive requests from other app in the same namespace"},{"location":"nais-application/access-policy/#receive-requests-from-other-app-in-the-another-namespace","text":"For app app-a to be able to receive incoming requests from app-b in the same cluster but another namespace ( othernamespace ), this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : inbound : rules : - application : app-b namespace : othernamespace","title":"Receive requests from other app in the another namespace"},{"location":"nais-application/access-policy/#outbound-rules","text":"Inbound rules specifies what other applications your application receives traffic from. spec.accessPolicy.outbound.rules specifies which applications in the same cluster to open for. To open for external applications, use the field spec.accessPolicy.outbound.external .","title":"Outbound rules"},{"location":"nais-application/access-policy/#send-requests-to-other-app-in-the-same-namespace","text":"For app app-a to be able to send requests to app-b in the same cluster and the same namespace, this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : app-b","title":"Send requests to other app in the same namespace"},{"location":"nais-application/access-policy/#send-requests-to-other-app-in-the-another-namespace","text":"For app app-a to be able to send requests requests to app-b in the same cluster but in another namespace ( othernamespace ), this specification is needed for app-a : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : app-b namespace : othernamespace","title":"Send requests to other app in the another namespace"},{"location":"nais-application/access-policy/#external-services","text":"In order to send requests to services outside of the cluster, external.host is needed: apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : external : - host : www.external-application.com Global Service Entries There are some services that are automatically added to the mesh in dev-gcp and prod-gcp (search for global_serviceentries ).","title":"External services"},{"location":"nais-application/access-policy/#advanced-resources-created-by-naiserator","text":"The previous application manifest examples will create Kubernetes Network Policies.","title":"Advanced: Resources created by Naiserator"},{"location":"nais-application/access-policy/#kubernetes-network-policy","text":"Default policy Every app created will have this default network policy that allows traffic to Linkerd and kube-dns. It also allows incoming traffic from the Linkerd control plane and from tap and prometheus in the linkerd-viz namespace. This is what enables monitoring via the linkerd dashboard. These policies will be created for every app, also those who don't have any access policies specified. apiVersion : extensions/v1beta1 kind : NetworkPolicy metadata : labels : app : appname team : teamname name : appname namespace : teamname spec : egress : - to : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - namespaceSelector : {} podSelector : matchLabels : k8s-app : kube-dns - ipBlock : cidr : 0.0.0.0/0 except : - 10.6.0.0/15 - 172.16.0.0/12 - 192.168.0.0/16 ingress : - from : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : tap - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : prometheus podSelector : matchLabels : app : appname policyTypes : - Ingress - Egress Kubernetes network policies The applications specified in spec.accessPolicy.inbound.rules and spec.accessPolicy.outbound.rules will append these fields to the default Network Policy: apiVersion : extensions/v1beta1 kind : NetworkPolicy ... spec : egress : - to : ... - namespaceSelector : matchLabels : name : othernamespace podSelector : matchLabels : app : app-b - podSelector : matchLabels : app : app-b - from : - namespaceSelector : matchLabels : name : othernamespace podSelector : matchLabels : app : app-b - podSelector : matchLabels : app : app-b podSelector : matchLabels : app : appname policyTypes : - Egress - Ingress Info Note that for namespace match labels to work, the namespaces must be labeled with name: namespacename . kube-system should be labeled accordingly for the default rule that allows traffic to kube-dns , but in GCP, the label is removed by some job in regular intervals...","title":"Kubernetes Network Policy"},{"location":"nais-application/application/","text":"NAIS Application reference \u00a7 This document describes all possible configuration values in the Application spec, commonly known as the nais.yaml file. accessPolicy \u00a7 By default, no traffic is allowed between applications inside the cluster. Configure access policies to explicitly allow communication between applications. This is also used for granting inbound access in the context of Azure AD and TokenX clients. Relevant information: https://doc.nais.io/appendix/zero-trust/ https://doc.nais.io/security/auth/azure-ad/access-policy https://doc.nais.io/security/auth/tokenx/#access-policies Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.inbound \u00a7 Configures inbound access for your application. Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules \u00a7 List of NAIS applications that may access your application. These settings apply both to Zero Trust network connectivity and token validity for Azure AD and TokenX tokens. Type: array Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].application \u00a7 The application's name. Type: string Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].cluster \u00a7 The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].namespace \u00a7 The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions \u00a7 Permissions contains a set of permissions that are granted to the given application. Currently only applicable for Azure AD clients. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#fine-grained-access-control Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions.roles \u00a7 Roles is a set of custom permission roles that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-roles Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions.scopes \u00a7 Scopes is a set of custom permission scopes that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-scopes Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.outbound \u00a7 Configures outbound access for your application. Type: object Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.external \u00a7 List of external resources that your applications should be able to reach. Type: array Required: false Availability: GCP Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].host \u00a7 The host that your application should be able to reach, i.e. without the protocol (e.g. https:// ). Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports \u00a7 List of port rules for external communication. Must be specified if using protocols other than HTTPS. Type: array Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].name \u00a7 Human-readable identifier for this rule. Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].port \u00a7 The port used for communication. Type: integer Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].protocol \u00a7 The protocol used for communication. Type: enum Required: true Allowed values: GRPC , HTTP , HTTP2 , HTTPS , MONGO , TCP , TLS Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.rules \u00a7 List of NAIS applications that your application needs to access. These settings apply to Zero Trust network connectivity. Type: array Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].application \u00a7 The application's name. Type: string Required: true Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].cluster \u00a7 The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].namespace \u00a7 The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 azure \u00a7 Provisions and configures Azure resources. Type: object Required: false Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no sidecar : autoLogin : true enabled : true errorPath : /error azure.application \u00a7 Configures an Azure AD client for this application. Relevant information: https://doc.nais.io/security/auth/azure-ad/ Type: object Required: true Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no azure.application.allowAllUsers \u00a7 AllowAllUsers denotes whether or not all users within the tenant should be allowed to access this AzureAdApplication. If undefined will default to true when Spec.Claims.Groups is undefined, and false if Spec,Claims.Groups is defined. Type: boolean Required: false Example spec : azure : application : allowAllUsers : true azure.application.claims \u00a7 Claims defines additional configuration of the emitted claims in tokens returned to the Azure AD application. Type: object Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.claims.extra \u00a7 Extra is a list of additional claims to be mapped from an associated claim-mapping policy. Currently, the only supported values are NAVident and azp_name . Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#extra Type: array Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name azure.application.claims.groups \u00a7 Groups is a list of Azure AD group IDs to be emitted in the 'Groups' claim. This also restricts access to only contain users of the defined groups unless overridden by Spec.AllowAllUsers. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#groups Type: array Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.claims.groups[].id \u00a7 ID is the actual object ID associated with the given group in Azure AD. Type: string Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.enabled \u00a7 Whether to enable provisioning of an Azure AD application. If enabled, an Azure AD application will be provisioned. Type: boolean Required: true Default value: false Example spec : azure : application : enabled : true azure.application.replyURLs \u00a7 ReplyURLs is a list of allowed redirect URLs used when performing OpenID Connect flows for authenticating end-users. Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#reply-urls Type: array Required: false Example spec : azure : application : replyURLs : - https://myapplication.nav.no/oauth2/callback azure.application.singlePageApplication \u00a7 SinglePageApplication denotes whether or not this Azure AD application should be registered as a single-page-application. Type: boolean Required: false Example spec : azure : application : singlePageApplication : true azure.application.tenant \u00a7 A Tenant represents an organization in Azure AD. If unspecified, will default to trygdeetaten.no for development clusters and nav.no for production clusters. Relevant information: https://doc.nais.io/security/auth/azure-ad/concepts#tenants Type: enum Required: false Allowed values: nav.no , trygdeetaten.no Example spec : azure : application : tenant : nav.no azure.sidecar \u00a7 Sidecar configures a sidecar that intercepts every HTTP request, and performs the OIDC flow if necessary. All requests to ingress + /oauth2 will be processed only by the sidecar, whereas all other requests will be proxied to the application. If the client is authenticated with Azure AD, the Authorization header will be set to Bearer <JWT> . Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar/ Type: object Required: false Example spec : azure : sidecar : autoLogin : true enabled : true errorPath : /error azure.sidecar.autoLogin \u00a7 Automatically redirect the user to login for all proxied routes. Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar#auto-login Type: boolean Required: false Default value: false Example spec : azure : sidecar : autoLogin : true azure.sidecar.enabled \u00a7 Enable the sidecar. Type: boolean Required: true Example spec : azure : sidecar : enabled : true azure.sidecar.errorPath \u00a7 Absolute path to redirect the user to on authentication errors for custom error handling. Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar#error-handling Type: string Required: false Example spec : azure : sidecar : errorPath : /error cleanup \u00a7 Configuration for automatic cleanup of failing pods Type: object Required: false Example spec : cleanup : enabled : true gracePeriod : 24h strategy : - downscale cleanup.enabled \u00a7 Enables automatic cleanup Default: true Type: boolean Required: true Example spec : cleanup : enabled : true cleanup.gracePeriod \u00a7 Default: 24h Type: string Required: false Pattern: ^[0-9]+h$ Example spec : cleanup : gracePeriod : 24h cleanup.strategy \u00a7 Strategy sets how a deployment might be handled. Setting this to an empty list is equivalent to setting enabled: false . Default: [\"abort-rollout\", \"downscale\"] . - abort-rollout : if new pods in a deployment are failing, but previous pods from the previous working revision are still running, Babylon can roll the deployment back to the working revision, aborting the rollout. - downscale : if all pods in a deployment are failing, Babylon will set replicaset to 0 Type: array Required: false Example spec : cleanup : strategy : - downscale command \u00a7 Override command when starting Docker image. Type: array Required: false Example spec : command : - /app/myapplication - --param - value - --other-param - other-value elastic \u00a7 To get your own Elastic Search instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : elastic : access : readwrite instance : my-elastic-instance elastic.access \u00a7 Access level for elastic user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : elastic : access : readwrite elastic.instance \u00a7 Provisions an Elasticsearch instance and configures your application so it can access it. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : elastic : instance : my-elastic-instance env \u00a7 Custom environment variables injected into your container. Specify either value or valueFrom , but not both. Type: array Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].name \u00a7 Environment variable name. May only contain letters, digits, and the underscore _ character. Type: string Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].value \u00a7 Environment variable value. Numbers and boolean values must be quoted. Required unless valueFrom is specified. Type: string Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom \u00a7 Dynamically set environment variables based on fields found in the Pod spec. Relevant information: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ Type: object Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom.fieldRef \u00a7 Type: object Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom.fieldRef.fieldPath \u00a7 Field value from the Pod spec that should be copied into the environment variable. Type: enum Required: true Allowed values: (empty string) , metadata.annotations , metadata.labels , metadata.name , metadata.namespace , spec.nodeName , spec.serviceAccountName , status.hostIP , status.podIP Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom \u00a7 EnvFrom exposes all variables in the ConfigMap or Secret resources as environment variables. One of configMap or secret is required. Environment variables will take the form KEY=VALUE , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Application resource. Type: array Required: false Availability: team namespaces Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs envFrom[].configmap \u00a7 Name of the ConfigMap where environment variables are specified. Required unless secret is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs envFrom[].secret \u00a7 Name of the Secret where environment variables are specified. Required unless configMap is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs filesFrom \u00a7 List of ConfigMap or Secret resources that will have their contents mounted into the containers as files. Either configMap or secret is required. Files will take the path <mountPath>/<key> , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret, and they will all be mounted to the same directory. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Application resource. Type: array Required: false Availability: team namespaces Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].configmap \u00a7 Name of the ConfigMap that contains files that should be mounted into the container. Required unless secret or persistentVolumeClaim is set. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].mountPath \u00a7 Filesystem path inside the pod where files are mounted. The directory will be created if it does not exist. If the directory exists, any files in the directory will be made unaccessible. Defaults to /var/run/configmaps/<NAME> , /var/run/secrets , or /var/run/pvc/<NAME> , depending on which of them is specified. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].persistentVolumeClaim \u00a7 Name of the PersistentVolumeClaim that should be mounted into the container. Required unless configMap or secret is set. This feature requires coordination with the NAIS team. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].secret \u00a7 Name of the Secret that contains files that should be mounted into the container. Required unless configMap or persistentVolumeClaim is set. If mounting multiple secrets, mountPath MUST be set to avoid collisions. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp \u00a7 Type: object Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.bigQueryDatasets \u00a7 Provision BigQuery datasets and give your application's pod mountable secrets for connecting to each dataset. Datasets are immutable and cannot be changed. Relevant information: https://cloud.google.com/bigquery/docs Type: array Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].cascadingDelete \u00a7 When set to true will delete the dataset, when the application resource is deleted. NB: If no tables exist in the bigquery dataset, it will delete the dataset even if this value is set/defaulted to false . Default value is false . Type: boolean Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].description \u00a7 Human-readable description of what this BigQuery dataset contains, or is used for. Will be visible in the GCP Console. Type: string Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].name \u00a7 Name of the BigQuery Dataset. The canonical name of the dataset will be <TEAM_PROJECT_ID>:<NAME> . Type: string Required: true Immutable: true Pattern: ^[a-z0-9][a-z0-9_]+$ Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].permission \u00a7 Permission level given to application. Type: enum Required: true Immutable: true Allowed values: READ , READWRITE Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.buckets \u00a7 Provision cloud storage buckets and connect them to your application. Relevant information: https://doc.nais.io/persistence/buckets/ Type: array Required: false Availability: GCP Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].cascadingDelete \u00a7 Allows deletion of bucket. Set to true if you want to delete the bucket. Type: boolean Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition \u00a7 Conditions for the bucket to use when selecting objects to delete in cleanup. Relevant information: https://cloud.google.com/storage/docs/lifecycle Type: object Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.age \u00a7 Condition is satisfied when the object reaches the specified age in days. These will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.createdBefore \u00a7 Condition is satisfied when the object is created before midnight on the specified date. These will be deleted. Type: string Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.numNewerVersions \u00a7 Condition is satisfied when the object has the specified number of newer versions. The older versions will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.withState \u00a7 Condition is satisfied when the object has the specified state. Type: enum Required: false Allowed values: (empty string) , ANY , ARCHIVED , LIVE Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].name \u00a7 The name of the bucket Type: string Required: true Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].retentionPeriodDays \u00a7 The number of days to hold objects in the bucket before it is allowed to delete them. Type: integer Required: false Value range: 1 - 36500 Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].uniformBucketLevelAccess \u00a7 Allows you to uniformly control access to your Cloud Storage resources. When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled, and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains. Uniform access control can not be reversed after 90 days! This is controlled by Google. Relevant information: https://cloud.google.com/storage/docs/uniform-bucket-level-access Type: boolean Required: false Default value: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.permissions \u00a7 List of additional permissions that should be granted to your application for accessing external GCP resources that have not been provisioned through NAIS. Relevant information: https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember#external_organization_level_policy_member Type: array Required: false Availability: GCP Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource \u00a7 IAM resource to bind the role to. Type: object Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.apiVersion \u00a7 Kubernetes APIVersion . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.kind \u00a7 Kubernetes Kind . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.name \u00a7 Kubernetes Name . Type: string Required: false Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].role \u00a7 Name of the GCP role to bind the resource to. Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.sqlInstances \u00a7 Provision database instances and connect them to your application. Relevant information: https://doc.nais.io/persistence/postgres/ https://cloud.google.com/sql/docs/postgres/instance-settings#impact Type: array Required: false Availability: GCP Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].autoBackupHour \u00a7 If specified, run automatic backups of the SQL database at the given hour. Note that this will backup the whole SQL instance, and not separate databases. Restores are done using the Google Cloud Console. Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].cascadingDelete \u00a7 Remove the entire Postgres server including all data when the Kubernetes resource is deleted. THIS IS A DESTRUCTIVE OPERATION ! Set cascading delete only when you want to remove data forever. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].collation \u00a7 Sort order for ORDER BY ... clauses. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases \u00a7 List of databases that should be created on this Postgres server. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].envVarPrefix \u00a7 Prefix to add to environment variables made available for database connection. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].name \u00a7 Database name. Type: string Required: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].users \u00a7 Add extra users for database access. These users need to be manually given access to database tables. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].users[].name \u00a7 User name. Type: string Required: true Pattern: ^[_a-zA-Z][-_a-zA-Z0-9]+$ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskAutoresize \u00a7 When set to true, GCP will automatically increase storage by XXX for the database when disk usage is above the high water mark. Relevant information: https://cloud.google.com/sql/docs/postgres/instance-settings#threshold Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskSize \u00a7 How much hard drive space to allocate for the SQL server, in gigabytes. Type: integer Required: false Minimum value: 10 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskType \u00a7 Disk type to use for storage in the database. Type: enum Required: false Allowed values: HDD , SSD Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].highAvailability \u00a7 When set to true this will set up standby database for failover. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights \u00a7 Configures query insights which are now default for new sql instances. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.enabled \u00a7 True if Query Insights feature is enabled. Type: boolean Required: false Default value: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.queryStringLength \u00a7 Maximum query length stored in bytes. Between 256 and 4500. Default to 1024. Type: integer Required: false Value range: 256 - 4500 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.recordApplicationTags \u00a7 True if Query Insights will record application tags from query when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.recordClientAddress \u00a7 True if Query Insights will record client address when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance \u00a7 Desired maintenance window for database updates. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance.day \u00a7 Type: integer Required: false Value range: 1 - 7 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance.hour \u00a7 Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].name \u00a7 The name of the instance, if omitted the application name will be used. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].pointInTimeRecovery \u00a7 Enables point-in-time recovery for sql instances using write-ahead logs. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].tier \u00a7 Server tier, i.e. how much CPU and memory allocated. Available tiers are db-f1-micro , db-g1-small and custom db-custom-CPU-RAM . Custom memory must be mulitple of 256 MB and at least 3.75 GB (e.g. db-custom-1-3840 for 1 cpu, 3840 MB ram) Also check out sizing your database . Type: string Required: false Default value: db-f1-micro Pattern: db-.+ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].type \u00a7 PostgreSQL version. Type: enum Required: true Allowed values: POSTGRES_11 , POSTGRES_12 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 idporten \u00a7 Configures an ID-porten client for this application. See ID-porten for more details. Type: object Required: false Example spec : idporten : accessTokenLifetime : 3600 clientURI : https://www.nav.no enabled : true frontchannelLogoutPath : /oauth2/logout frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout postLogoutRedirectURIs : - https://www.nav.no redirectPath : /oauth2/callback redirectURI : https://myapplication.nav.no/oauth2/callback sessionLifetime : 7200 sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb idporten.accessTokenLifetime \u00a7 AccessTokenLifetime is the lifetime in seconds for any issued access token from ID-porten. If unspecified, defaults to 3600 seconds (1 hour). Type: integer Required: false Default value: 3600 Value range: 1 - 3600 Example spec : idporten : accessTokenLifetime : 3600 idporten.clientURI \u00a7 ClientURI is the URL shown to the user at ID-porten when displaying a 'back' button or on errors. Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : clientURI : https://www.nav.no idporten.enabled \u00a7 Whether to enable provisioning of an ID-porten client. If enabled, an ID-porten client be provisioned. Type: boolean Required: true Availability: team namespaces Example spec : idporten : enabled : true idporten.frontchannelLogoutPath \u00a7 FrontchannelLogoutPath is a valid path for your application where ID-porten sends a request to whenever the user has initiated a logout elsewhere as part of a single logout (front channel logout) process. Relevant information: https://doc.nais.io/security/auth/idporten/#front-channel-logout https://docs.digdir.no/oidc_func_sso.html#2-h%C3%A5ndtere-utlogging-fra-id-porten Type: string Required: false Default value: /oauth2/logout Pattern: ^\\/.*$ Example spec : idporten : frontchannelLogoutPath : /oauth2/logout idporten.frontchannelLogoutURI \u00a7 Prefer using frontchannelLogoutPath . Deprecated This feature is deprecated, preserved only for backwards compatibility. Relevant information: https://doc.nais.io/security/auth/idporten/#front-channel-logout https://docs.digdir.no/oidc_func_sso.html#2-h%C3%A5ndtere-utlogging-fra-id-porten Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout idporten.postLogoutRedirectURIs \u00a7 PostLogoutRedirectURIs are valid URIs that ID-porten will allow redirecting the end-user to after a single logout has been initiated and performed by the application. Relevant information: https://doc.nais.io/security/auth/idporten/#self-initiated-logout https://docs.digdir.no/oidc_func_sso.html#1-utlogging-fra-egen-tjeneste Type: array Required: false Default value: https://www.nav.no Example spec : idporten : postLogoutRedirectURIs : - https://www.nav.no idporten.redirectPath \u00a7 RedirectPath is a valid path that ID-porten redirects back to after a successful authorization request. Type: string Required: false Default value: /oauth2/callback Pattern: ^\\/.*$ Example spec : idporten : redirectPath : /oauth2/callback idporten.redirectURI \u00a7 Use redirectPath instead. Deprecated This feature is deprecated, preserved only for backwards compatibility. Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : redirectURI : https://myapplication.nav.no/oauth2/callback idporten.sessionLifetime \u00a7 SessionLifetime is the maximum lifetime in seconds for any given user's session in your application. The timeout starts whenever the user is redirected from the authorization_endpoint at ID-porten. If unspecified, defaults to 7200 seconds (2 hours). Note: Attempting to refresh the user's access_token beyond this timeout will yield an error. Type: integer Required: false Default value: 7200 Value range: 3600 - 7200 Example spec : idporten : sessionLifetime : 7200 idporten.sidecar \u00a7 Sidecar configures a sidecar that intercepts every HTTP request, and performs the OIDC flow if necessary. All requests to ingress + /oauth2 will be processed only by the sidecar, whereas all other requests will be proxied to the application. If the client is authenticated with IDPorten, the Authorization header will be set to Bearer <JWT> . Relevant information: https://doc.nais.io/security/auth/idporten/sidecar/ Type: object Required: false Example spec : idporten : sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb idporten.sidecar.autoLogin \u00a7 Automatically redirect the user to login for all proxied routes. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#auto-login Type: boolean Required: false Default value: false Example spec : idporten : sidecar : autoLogin : true idporten.sidecar.enabled \u00a7 Enable the sidecar. Type: boolean Required: true Example spec : idporten : sidecar : enabled : true idporten.sidecar.errorPath \u00a7 Absolute path to redirect the user to on authentication errors for custom error handling. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#error-handling Type: string Required: false Example spec : idporten : sidecar : errorPath : /error idporten.sidecar.level \u00a7 Default security level for all authentication requests. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#security-levels Type: enum Required: false Default value: Level4 Allowed values: Level3 , Level4 Example spec : idporten : sidecar : level : Level4 idporten.sidecar.locale \u00a7 Default user interface locale for all authentication requests. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#locales Type: enum Required: false Default value: nb Allowed values: en , nb , nn , se Example spec : idporten : sidecar : locale : nb image \u00a7 Your application's Docker image location and tag. Type: string Required: true Example spec : image : navikt/testapp:69.0.0 influx \u00a7 An InfluxDB via Aiven. A typical use case for influxdb is to store metrics from your application and visualize them in Grafana. Type: object Required: false Availability: GCP Example spec : influx : instance : influx-instance influx.instance \u00a7 Provisions an InfluxDB instance and configures your application to access it. Use the prefix: influx- + team that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : influx : instance : influx-instance ingresses \u00a7 List of URLs that will route HTTPS traffic to the application. All URLs must start with https:// . Domain availability differs according to which environment your application is running in. Relevant information: https://doc.nais.io/clusters/gcp/ https://doc.nais.io/clusters/on-premises/ Type: array Required: false Example spec : ingresses : - https://myapplication.nav.no kafka \u00a7 Set up Aiven Kafka for your application. Relevant information: https://doc.nais.io/persistence/kafka/ Type: object Required: false Example spec : kafka : pool : nav-dev streams : true kafka.pool \u00a7 Configures your application to access an Aiven Kafka cluster. Type: enum Required: true Allowed values: nav-dev , nav-infrastructure , nav-prod Example spec : kafka : pool : nav-dev kafka.streams \u00a7 Allow this app to use kafka streams Relevant information: https://doc.nais.io/persistence/kafka/application/#using-kafka-streams-with-internal-topics Type: boolean Required: false Default value: false Availability: GCP Example spec : kafka : streams : true leaderElection \u00a7 If true, an HTTP endpoint will be available at $ELECTOR_PATH that returns the current leader. Relevant information: https://doc.nais.io/addons/leader-election/ Type: boolean Required: false Example spec : leaderElection : true liveness \u00a7 Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations. Read more about this over at the Kubernetes probes documentation . Type: object Required: false Example spec : liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 liveness.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Default value: 3 Example spec : liveness : failureThreshold : 10 liveness.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : liveness : initialDelay : 20 liveness.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : liveness : path : /isalive liveness.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Default value: 10 Example spec : liveness : periodSeconds : 5 liveness.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : liveness : port : 8080 liveness.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Default value: 1 Example spec : liveness : timeout : 1 logformat \u00a7 Format of the logs from the container. Use this if the container doesn't support JSON logging and the log is in a special format that need to be parsed. Type: enum Required: false Allowed values: (empty string) , accesslog , accesslog_with_processing_time , accesslog_with_referer_useragent , capnslog , glog , gokit , influxdb , log15 , logrus , redis , simple Example spec : logformat : accesslog_with_referer_useragent logtransform \u00a7 Extra filters for modifying log content. This can e.g. be used for setting loglevel based on http status code. Type: enum Required: false Allowed values: dns_loglevel , http_loglevel Example spec : logtransform : http_loglevel maskinporten \u00a7 Configures a Maskinporten client for this application. See Maskinporten for more details. Type: object Required: false Example spec : maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.enabled \u00a7 If enabled, provisions and configures a Maskinporten client with consumed scopes and/or Exposed scopes with DigDir. Type: boolean Required: true Default value: false Availability: team namespaces Example spec : maskinporten : enabled : true maskinporten.scopes \u00a7 Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes. Type: object Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.consumes \u00a7 This is the Schema for the consumes and exposes API. consumes is a list of scopes that your client can request access to. Type: array Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read maskinporten.scopes.consumes[].name \u00a7 The scope consumed by the application to gain access to an external organization API. Ensure that the NAV organization has been granted access to the scope prior to requesting access. Relevant information: https://doc.nais.io/security/auth/maskinporten/#consume-scopes Type: string Required: true Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read maskinporten.scopes.exposes \u00a7 exposes is a list of scopes your application want to expose to other organization where access to the scope is based on organization number. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].allowedIntegrations \u00a7 Whitelisting of integration's allowed. Default is maskinporten Relevant information: https://docs.digdir.no/maskinporten_guide_apitilbyder.html#scope-begrensninger Type: array Required: false Default value: maskinporten Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].atMaxAge \u00a7 Max time in seconds for a issued access_token. Default is 30 sec. Type: integer Required: false Default value: 30 Value range: 30 - 680 Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers \u00a7 External consumers granted access to this scope and able to request access_token. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers[].name \u00a7 This is a describing field intended for clarity not used for any other purpose. Type: string Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers[].orgno \u00a7 The external business/organization number. Type: string Required: true Pattern: ^\\d{9}$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].enabled \u00a7 If Enabled the configured scope is available to be used and consumed by organizations granted access. Relevant information: https://doc.nais.io/naisjob/reference/#maskinportenscopesexposesconsumers Type: boolean Required: true Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].name \u00a7 The actual subscope combined with Product . Ensure that <Product><Name> matches Pattern . Type: string Required: true Default value: false Pattern: ^([a-z\u00e6\u00f8\u00e50-9]+\\/?)+(\\:[a-z\u00e6\u00f8\u00e50-9]+)*[a-z\u00e6\u00f8\u00e50-9]+(\\.[a-z\u00e6\u00f8\u00e50-9]+)*$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].product \u00a7 The product-area your application belongs to e.g. arbeid, helse ... This will be included in the final scope nav:<Product><Name> . Type: string Required: true Pattern: ^[a-z0-9]+$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch \u00a7 To get your own OpenSearch instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : openSearch : access : readwrite instance : my-open-search-instance openSearch.access \u00a7 Access level for OpenSearch user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : openSearch : access : readwrite openSearch.instance \u00a7 Configure your application to access your OpenSearch instance. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : openSearch : instance : my-open-search-instance port \u00a7 The port number which is exposed by the container and should receive traffic. Note that ports under 1024 are unavailable. Type: integer Required: false Default value: 8080 Example spec : port : 8080 preStopHook \u00a7 PreStopHook is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits by itself. The reason for termination is passed to the handler. Relevant information: https://doc.nais.io/naisjob/#handles-termination-gracefully https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 preStopHook.exec \u00a7 Command that should be run inside the main container just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script preStopHook.exec.command \u00a7 Command is the command line to execute inside the container before the pod is shut down. The command is not run inside a shell, so traditional shell instructions (pipes, redirects, etc.) won't work. To use a shell, you need to explicitly call out to that shell. If the exit status is non-zero, the pod will still be shut down, and marked as Failed . Type: array Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script preStopHook.http \u00a7 HTTP GET request that is called just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : http : path : /internal/stop port : 8080 preStopHook.http.path \u00a7 Path to access on the HTTP server. Type: string Required: true Example spec : preStopHook : http : path : /internal/stop preStopHook.http.port \u00a7 Port to access on the container. Defaults to application port, as defined in .spec.port . Type: integer Required: false Value range: 1 - 65535 Example spec : preStopHook : http : port : 8080 preStopHookPath \u00a7 A HTTP GET will be issued to this endpoint at least once before the pod is terminated. This feature is deprecated and will be removed in the next major version (nais.io/v1). Relevant information: https://doc.nais.io/nais-application/#handles-termination-gracefully Type: string Required: false Example spec : preStopHookPath : /internal/stop prometheus \u00a7 Prometheus is used to scrape metrics from the pod . Use this configuration to override the default values. Type: object Required: false Example spec : prometheus : enabled : true path : /metrics port : \"8080\" prometheus.enabled \u00a7 Type: boolean Required: false Example spec : prometheus : enabled : true prometheus.path \u00a7 Type: string Required: false Default value: /metrics Example spec : prometheus : path : /metrics prometheus.port \u00a7 Type: string Required: false Example spec : prometheus : port : \"8080\" readiness \u00a7 Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the application, but you don\u2019t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services. Read more about this over at the Kubernetes readiness documentation . Type: object Required: false Example spec : readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 readiness.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : readiness : failureThreshold : 10 readiness.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : readiness : initialDelay : 20 readiness.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : readiness : path : /isready readiness.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Example spec : readiness : periodSeconds : 5 readiness.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : readiness : port : 8080 readiness.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Example spec : readiness : timeout : 1 replicas \u00a7 The numbers of pods to run in parallel. Type: object Required: false Example spec : replicas : cpuThresholdPercentage : 50 disableAutoScaling : true max : 4 min : 2 replicas.cpuThresholdPercentage \u00a7 Amount of CPU usage before the autoscaler kicks in. Type: integer Required: false Default value: 50 Example spec : replicas : cpuThresholdPercentage : 50 replicas.disableAutoScaling \u00a7 Disable autoscaling Type: boolean Required: false Default value: false Example spec : replicas : disableAutoScaling : true replicas.max \u00a7 The pod autoscaler will increase replicas when required up to the maximum. Type: integer Required: false Default value: 4 Example spec : replicas : max : 4 replicas.min \u00a7 The minimum amount of running replicas for a deployment. Type: integer Required: false Default value: 2 Example spec : replicas : min : 2 resources \u00a7 When Containers have resource requests specified, the Kubernetes scheduler can make better decisions about which nodes to place pods on. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi resources.limits \u00a7 Limit defines the maximum amount of resources a container can use before getting evicted. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi resources.limits.cpu \u00a7 Type: string Required: false Default value: 500m Pattern: ^\\d+m?$ Example spec : resources : limits : cpu : 500m resources.limits.memory \u00a7 Type: string Required: false Default value: 512Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : limits : memory : 512Mi resources.requests \u00a7 Request defines the amount of resources a container is allocated on startup. Type: object Required: false Example spec : resources : requests : cpu : 200m memory : 256Mi resources.requests.cpu \u00a7 Type: string Required: false Default value: 200m Pattern: ^\\d+m?$ Example spec : resources : requests : cpu : 200m resources.requests.memory \u00a7 Type: string Required: false Default value: 256Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : requests : memory : 256Mi secureLogs \u00a7 Whether or not to enable a sidecar container for secure logging. Type: object Required: false Example spec : secureLogs : enabled : true secureLogs.enabled \u00a7 Whether to enable a sidecar container for secure logging. If enabled, a volume is mounted in the pods where secure logs can be saved. Type: boolean Required: true Default value: false Example spec : secureLogs : enabled : true service \u00a7 Specify which port and protocol is used to connect to the application in the container. Defaults to HTTP on port 80. Type: object Required: false Example spec : service : port : 80 protocol : http service.port \u00a7 Port for the default service. Default port is 80. Type: integer Required: true Default value: 80 Example spec : service : port : 80 service.protocol \u00a7 Which protocol the backend service runs on. Default is http . Type: enum Required: false Default value: http Allowed values: grpc , http , redis , tcp Example spec : service : protocol : http skipCaBundle \u00a7 Whether to skip injection of NAV certificate authority bundle or not. Defaults to false. Type: boolean Required: false Example spec : skipCaBundle : true startup \u00a7 Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Type: object Required: false Example spec : startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 startup.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : startup : failureThreshold : 10 startup.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : startup : initialDelay : 20 startup.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : startup : path : /started startup.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Example spec : startup : periodSeconds : 5 startup.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : startup : port : 8080 startup.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Example spec : startup : timeout : 1 strategy \u00a7 Specifies the strategy used to replace old Pods by new ones. Type: object Required: false Example spec : strategy : type : RollingUpdate strategy.type \u00a7 Specifies the strategy used to replace old Pods by new ones. RollingUpdate is the default value. Type: enum Required: true Default value: RollingUpdate Allowed values: Recreate , RollingUpdate Example spec : strategy : type : RollingUpdate tokenx \u00a7 Provisions and configures a TokenX client for your application. Relevant information: https://doc.nais.io/security/auth/tokenx/ Type: object Required: false Example spec : tokenx : enabled : true mountSecretsAsFilesOnly : true tokenx.enabled \u00a7 If enabled, will provision and configure a TokenX client and inject an accompanying secret. Type: boolean Required: true Default value: false Example spec : tokenx : enabled : true tokenx.mountSecretsAsFilesOnly \u00a7 If enabled, secrets for TokenX will be mounted as files only, i.e. not as environment variables. Type: boolean Required: false Example spec : tokenx : mountSecretsAsFilesOnly : true vault \u00a7 Provides secrets management, identity-based access, and encrypting application data for auditing of secrets for applications, systems, and users. Relevant information: https://github.com/navikt/vault-iac/tree/master/doc Type: object Required: false Availability: on-premises Example spec : vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true vault.enabled \u00a7 If set to true, fetch secrets from Vault and inject into the pods. Type: boolean Required: false Example spec : vault : enabled : true vault.paths \u00a7 List of secret paths to be read from Vault and injected into the pod's filesystem. Overriding the paths array is optional, and will give you fine-grained control over which Vault paths that will be mounted on the file system. By default, the list will contain an entry with kvPath: /kv/<environment>/<zone>/<application>/<namespace> mountPath: /var/run/secrets/nais.io/vault that will always be attempted to be mounted. Type: array Required: false Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].format \u00a7 Format of the secret that should be processed. Type: enum Required: false Allowed values: (empty string) , env , flatten , json , properties , yaml Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].kvPath \u00a7 Path to Vault key/value store that should be mounted into the file system. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].mountPath \u00a7 File system path that the secret will be mounted into. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.sidecar \u00a7 If enabled, the sidecar will automatically refresh the token's Time-To-Live before it expires. Type: boolean Required: false Example spec : vault : sidecar : true webproxy \u00a7 Inject on-premises web proxy configuration into the application pod. Most Linux applications should auto-detect these settings from the $HTTP_PROXY , $HTTPS_PROXY and $NO_PROXY environment variables (and their lowercase counterparts). Java applications can start the JVM using parameters from the $JAVA_PROXY_OPTIONS environment variable. Type: boolean Required: false Availability: on-premises Example spec : webproxy : true","title":"Reference"},{"location":"nais-application/application/#nais-application-reference","text":"This document describes all possible configuration values in the Application spec, commonly known as the nais.yaml file.","title":"NAIS Application reference"},{"location":"nais-application/application/#accesspolicy","text":"By default, no traffic is allowed between applications inside the cluster. Configure access policies to explicitly allow communication between applications. This is also used for granting inbound access in the context of Azure AD and TokenX clients. Relevant information: https://doc.nais.io/appendix/zero-trust/ https://doc.nais.io/security/auth/azure-ad/access-policy https://doc.nais.io/security/auth/tokenx/#access-policies Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy"},{"location":"nais-application/application/#accesspolicyinbound","text":"Configures inbound access for your application. Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound"},{"location":"nais-application/application/#accesspolicyinboundrules","text":"List of NAIS applications that may access your application. These settings apply both to Zero Trust network connectivity and token validity for Azure AD and TokenX tokens. Type: array Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules"},{"location":"nais-application/application/#accesspolicyinboundrulesapplication","text":"The application's name. Type: string Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].application"},{"location":"nais-application/application/#accesspolicyinboundrulescluster","text":"The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].cluster"},{"location":"nais-application/application/#accesspolicyinboundrulesnamespace","text":"The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].namespace"},{"location":"nais-application/application/#accesspolicyinboundrulespermissions","text":"Permissions contains a set of permissions that are granted to the given application. Currently only applicable for Azure AD clients. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#fine-grained-access-control Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions"},{"location":"nais-application/application/#accesspolicyinboundrulespermissionsroles","text":"Roles is a set of custom permission roles that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-roles Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions.roles"},{"location":"nais-application/application/#accesspolicyinboundrulespermissionsscopes","text":"Scopes is a set of custom permission scopes that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-scopes Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions.scopes"},{"location":"nais-application/application/#accesspolicyoutbound","text":"Configures outbound access for your application. Type: object Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound"},{"location":"nais-application/application/#accesspolicyoutboundexternal","text":"List of external resources that your applications should be able to reach. Type: array Required: false Availability: GCP Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external"},{"location":"nais-application/application/#accesspolicyoutboundexternalhost","text":"The host that your application should be able to reach, i.e. without the protocol (e.g. https:// ). Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].host"},{"location":"nais-application/application/#accesspolicyoutboundexternalports","text":"List of port rules for external communication. Must be specified if using protocols other than HTTPS. Type: array Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports"},{"location":"nais-application/application/#accesspolicyoutboundexternalportsname","text":"Human-readable identifier for this rule. Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].name"},{"location":"nais-application/application/#accesspolicyoutboundexternalportsport","text":"The port used for communication. Type: integer Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].port"},{"location":"nais-application/application/#accesspolicyoutboundexternalportsprotocol","text":"The protocol used for communication. Type: enum Required: true Allowed values: GRPC , HTTP , HTTP2 , HTTPS , MONGO , TCP , TLS Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].protocol"},{"location":"nais-application/application/#accesspolicyoutboundrules","text":"List of NAIS applications that your application needs to access. These settings apply to Zero Trust network connectivity. Type: array Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules"},{"location":"nais-application/application/#accesspolicyoutboundrulesapplication","text":"The application's name. Type: string Required: true Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].application"},{"location":"nais-application/application/#accesspolicyoutboundrulescluster","text":"The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].cluster"},{"location":"nais-application/application/#accesspolicyoutboundrulesnamespace","text":"The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].namespace"},{"location":"nais-application/application/#azure","text":"Provisions and configures Azure resources. Type: object Required: false Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no sidecar : autoLogin : true enabled : true errorPath : /error","title":"azure"},{"location":"nais-application/application/#azureapplication","text":"Configures an Azure AD client for this application. Relevant information: https://doc.nais.io/security/auth/azure-ad/ Type: object Required: true Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no","title":"azure.application"},{"location":"nais-application/application/#azureapplicationallowallusers","text":"AllowAllUsers denotes whether or not all users within the tenant should be allowed to access this AzureAdApplication. If undefined will default to true when Spec.Claims.Groups is undefined, and false if Spec,Claims.Groups is defined. Type: boolean Required: false Example spec : azure : application : allowAllUsers : true","title":"azure.application.allowAllUsers"},{"location":"nais-application/application/#azureapplicationclaims","text":"Claims defines additional configuration of the emitted claims in tokens returned to the Azure AD application. Type: object Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims"},{"location":"nais-application/application/#azureapplicationclaimsextra","text":"Extra is a list of additional claims to be mapped from an associated claim-mapping policy. Currently, the only supported values are NAVident and azp_name . Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#extra Type: array Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name","title":"azure.application.claims.extra"},{"location":"nais-application/application/#azureapplicationclaimsgroups","text":"Groups is a list of Azure AD group IDs to be emitted in the 'Groups' claim. This also restricts access to only contain users of the defined groups unless overridden by Spec.AllowAllUsers. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#groups Type: array Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims.groups"},{"location":"nais-application/application/#azureapplicationclaimsgroupsid","text":"ID is the actual object ID associated with the given group in Azure AD. Type: string Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims.groups[].id"},{"location":"nais-application/application/#azureapplicationenabled","text":"Whether to enable provisioning of an Azure AD application. If enabled, an Azure AD application will be provisioned. Type: boolean Required: true Default value: false Example spec : azure : application : enabled : true","title":"azure.application.enabled"},{"location":"nais-application/application/#azureapplicationreplyurls","text":"ReplyURLs is a list of allowed redirect URLs used when performing OpenID Connect flows for authenticating end-users. Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#reply-urls Type: array Required: false Example spec : azure : application : replyURLs : - https://myapplication.nav.no/oauth2/callback","title":"azure.application.replyURLs"},{"location":"nais-application/application/#azureapplicationsinglepageapplication","text":"SinglePageApplication denotes whether or not this Azure AD application should be registered as a single-page-application. Type: boolean Required: false Example spec : azure : application : singlePageApplication : true","title":"azure.application.singlePageApplication"},{"location":"nais-application/application/#azureapplicationtenant","text":"A Tenant represents an organization in Azure AD. If unspecified, will default to trygdeetaten.no for development clusters and nav.no for production clusters. Relevant information: https://doc.nais.io/security/auth/azure-ad/concepts#tenants Type: enum Required: false Allowed values: nav.no , trygdeetaten.no Example spec : azure : application : tenant : nav.no","title":"azure.application.tenant"},{"location":"nais-application/application/#azuresidecar","text":"Sidecar configures a sidecar that intercepts every HTTP request, and performs the OIDC flow if necessary. All requests to ingress + /oauth2 will be processed only by the sidecar, whereas all other requests will be proxied to the application. If the client is authenticated with Azure AD, the Authorization header will be set to Bearer <JWT> . Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar/ Type: object Required: false Example spec : azure : sidecar : autoLogin : true enabled : true errorPath : /error","title":"azure.sidecar"},{"location":"nais-application/application/#azuresidecarautologin","text":"Automatically redirect the user to login for all proxied routes. Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar#auto-login Type: boolean Required: false Default value: false Example spec : azure : sidecar : autoLogin : true","title":"azure.sidecar.autoLogin"},{"location":"nais-application/application/#azuresidecarenabled","text":"Enable the sidecar. Type: boolean Required: true Example spec : azure : sidecar : enabled : true","title":"azure.sidecar.enabled"},{"location":"nais-application/application/#azuresidecarerrorpath","text":"Absolute path to redirect the user to on authentication errors for custom error handling. Relevant information: https://doc.nais.io/security/auth/azure-ad/sidecar#error-handling Type: string Required: false Example spec : azure : sidecar : errorPath : /error","title":"azure.sidecar.errorPath"},{"location":"nais-application/application/#cleanup","text":"Configuration for automatic cleanup of failing pods Type: object Required: false Example spec : cleanup : enabled : true gracePeriod : 24h strategy : - downscale","title":"cleanup"},{"location":"nais-application/application/#cleanupenabled","text":"Enables automatic cleanup Default: true Type: boolean Required: true Example spec : cleanup : enabled : true","title":"cleanup.enabled"},{"location":"nais-application/application/#cleanupgraceperiod","text":"Default: 24h Type: string Required: false Pattern: ^[0-9]+h$ Example spec : cleanup : gracePeriod : 24h","title":"cleanup.gracePeriod"},{"location":"nais-application/application/#cleanupstrategy","text":"Strategy sets how a deployment might be handled. Setting this to an empty list is equivalent to setting enabled: false . Default: [\"abort-rollout\", \"downscale\"] . - abort-rollout : if new pods in a deployment are failing, but previous pods from the previous working revision are still running, Babylon can roll the deployment back to the working revision, aborting the rollout. - downscale : if all pods in a deployment are failing, Babylon will set replicaset to 0 Type: array Required: false Example spec : cleanup : strategy : - downscale","title":"cleanup.strategy"},{"location":"nais-application/application/#command","text":"Override command when starting Docker image. Type: array Required: false Example spec : command : - /app/myapplication - --param - value - --other-param - other-value","title":"command"},{"location":"nais-application/application/#elastic","text":"To get your own Elastic Search instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : elastic : access : readwrite instance : my-elastic-instance","title":"elastic"},{"location":"nais-application/application/#elasticaccess","text":"Access level for elastic user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : elastic : access : readwrite","title":"elastic.access"},{"location":"nais-application/application/#elasticinstance","text":"Provisions an Elasticsearch instance and configures your application so it can access it. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : elastic : instance : my-elastic-instance","title":"elastic.instance"},{"location":"nais-application/application/#env","text":"Custom environment variables injected into your container. Specify either value or valueFrom , but not both. Type: array Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env"},{"location":"nais-application/application/#envname","text":"Environment variable name. May only contain letters, digits, and the underscore _ character. Type: string Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].name"},{"location":"nais-application/application/#envvalue","text":"Environment variable value. Numbers and boolean values must be quoted. Required unless valueFrom is specified. Type: string Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].value"},{"location":"nais-application/application/#envvaluefrom","text":"Dynamically set environment variables based on fields found in the Pod spec. Relevant information: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ Type: object Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom"},{"location":"nais-application/application/#envvaluefromfieldref","text":"Type: object Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom.fieldRef"},{"location":"nais-application/application/#envvaluefromfieldreffieldpath","text":"Field value from the Pod spec that should be copied into the environment variable. Type: enum Required: true Allowed values: (empty string) , metadata.annotations , metadata.labels , metadata.name , metadata.namespace , spec.nodeName , spec.serviceAccountName , status.hostIP , status.podIP Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom.fieldRef.fieldPath"},{"location":"nais-application/application/#envfrom","text":"EnvFrom exposes all variables in the ConfigMap or Secret resources as environment variables. One of configMap or secret is required. Environment variables will take the form KEY=VALUE , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Application resource. Type: array Required: false Availability: team namespaces Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom"},{"location":"nais-application/application/#envfromconfigmap","text":"Name of the ConfigMap where environment variables are specified. Required unless secret is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom[].configmap"},{"location":"nais-application/application/#envfromsecret","text":"Name of the Secret where environment variables are specified. Required unless configMap is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom[].secret"},{"location":"nais-application/application/#filesfrom","text":"List of ConfigMap or Secret resources that will have their contents mounted into the containers as files. Either configMap or secret is required. Files will take the path <mountPath>/<key> , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret, and they will all be mounted to the same directory. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Application resource. Type: array Required: false Availability: team namespaces Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom"},{"location":"nais-application/application/#filesfromconfigmap","text":"Name of the ConfigMap that contains files that should be mounted into the container. Required unless secret or persistentVolumeClaim is set. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].configmap"},{"location":"nais-application/application/#filesfrommountpath","text":"Filesystem path inside the pod where files are mounted. The directory will be created if it does not exist. If the directory exists, any files in the directory will be made unaccessible. Defaults to /var/run/configmaps/<NAME> , /var/run/secrets , or /var/run/pvc/<NAME> , depending on which of them is specified. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].mountPath"},{"location":"nais-application/application/#filesfrompersistentvolumeclaim","text":"Name of the PersistentVolumeClaim that should be mounted into the container. Required unless configMap or secret is set. This feature requires coordination with the NAIS team. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].persistentVolumeClaim"},{"location":"nais-application/application/#filesfromsecret","text":"Name of the Secret that contains files that should be mounted into the container. Required unless configMap or persistentVolumeClaim is set. If mounting multiple secrets, mountPath MUST be set to avoid collisions. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].secret"},{"location":"nais-application/application/#gcp","text":"Type: object Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp"},{"location":"nais-application/application/#gcpbigquerydatasets","text":"Provision BigQuery datasets and give your application's pod mountable secrets for connecting to each dataset. Datasets are immutable and cannot be changed. Relevant information: https://cloud.google.com/bigquery/docs Type: array Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets"},{"location":"nais-application/application/#gcpbigquerydatasetscascadingdelete","text":"When set to true will delete the dataset, when the application resource is deleted. NB: If no tables exist in the bigquery dataset, it will delete the dataset even if this value is set/defaulted to false . Default value is false . Type: boolean Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].cascadingDelete"},{"location":"nais-application/application/#gcpbigquerydatasetsdescription","text":"Human-readable description of what this BigQuery dataset contains, or is used for. Will be visible in the GCP Console. Type: string Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].description"},{"location":"nais-application/application/#gcpbigquerydatasetsname","text":"Name of the BigQuery Dataset. The canonical name of the dataset will be <TEAM_PROJECT_ID>:<NAME> . Type: string Required: true Immutable: true Pattern: ^[a-z0-9][a-z0-9_]+$ Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].name"},{"location":"nais-application/application/#gcpbigquerydatasetspermission","text":"Permission level given to application. Type: enum Required: true Immutable: true Allowed values: READ , READWRITE Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].permission"},{"location":"nais-application/application/#gcpbuckets","text":"Provision cloud storage buckets and connect them to your application. Relevant information: https://doc.nais.io/persistence/buckets/ Type: array Required: false Availability: GCP Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets"},{"location":"nais-application/application/#gcpbucketscascadingdelete","text":"Allows deletion of bucket. Set to true if you want to delete the bucket. Type: boolean Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].cascadingDelete"},{"location":"nais-application/application/#gcpbucketslifecyclecondition","text":"Conditions for the bucket to use when selecting objects to delete in cleanup. Relevant information: https://cloud.google.com/storage/docs/lifecycle Type: object Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition"},{"location":"nais-application/application/#gcpbucketslifecycleconditionage","text":"Condition is satisfied when the object reaches the specified age in days. These will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.age"},{"location":"nais-application/application/#gcpbucketslifecycleconditioncreatedbefore","text":"Condition is satisfied when the object is created before midnight on the specified date. These will be deleted. Type: string Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.createdBefore"},{"location":"nais-application/application/#gcpbucketslifecycleconditionnumnewerversions","text":"Condition is satisfied when the object has the specified number of newer versions. The older versions will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.numNewerVersions"},{"location":"nais-application/application/#gcpbucketslifecycleconditionwithstate","text":"Condition is satisfied when the object has the specified state. Type: enum Required: false Allowed values: (empty string) , ANY , ARCHIVED , LIVE Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.withState"},{"location":"nais-application/application/#gcpbucketsname","text":"The name of the bucket Type: string Required: true Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].name"},{"location":"nais-application/application/#gcpbucketsretentionperioddays","text":"The number of days to hold objects in the bucket before it is allowed to delete them. Type: integer Required: false Value range: 1 - 36500 Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].retentionPeriodDays"},{"location":"nais-application/application/#gcpbucketsuniformbucketlevelaccess","text":"Allows you to uniformly control access to your Cloud Storage resources. When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled, and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains. Uniform access control can not be reversed after 90 days! This is controlled by Google. Relevant information: https://cloud.google.com/storage/docs/uniform-bucket-level-access Type: boolean Required: false Default value: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].uniformBucketLevelAccess"},{"location":"nais-application/application/#gcppermissions","text":"List of additional permissions that should be granted to your application for accessing external GCP resources that have not been provisioned through NAIS. Relevant information: https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember#external_organization_level_policy_member Type: array Required: false Availability: GCP Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions"},{"location":"nais-application/application/#gcppermissionsresource","text":"IAM resource to bind the role to. Type: object Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource"},{"location":"nais-application/application/#gcppermissionsresourceapiversion","text":"Kubernetes APIVersion . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.apiVersion"},{"location":"nais-application/application/#gcppermissionsresourcekind","text":"Kubernetes Kind . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.kind"},{"location":"nais-application/application/#gcppermissionsresourcename","text":"Kubernetes Name . Type: string Required: false Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.name"},{"location":"nais-application/application/#gcppermissionsrole","text":"Name of the GCP role to bind the resource to. Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].role"},{"location":"nais-application/application/#gcpsqlinstances","text":"Provision database instances and connect them to your application. Relevant information: https://doc.nais.io/persistence/postgres/ https://cloud.google.com/sql/docs/postgres/instance-settings#impact Type: array Required: false Availability: GCP Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances"},{"location":"nais-application/application/#gcpsqlinstancesautobackuphour","text":"If specified, run automatic backups of the SQL database at the given hour. Note that this will backup the whole SQL instance, and not separate databases. Restores are done using the Google Cloud Console. Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].autoBackupHour"},{"location":"nais-application/application/#gcpsqlinstancescascadingdelete","text":"Remove the entire Postgres server including all data when the Kubernetes resource is deleted. THIS IS A DESTRUCTIVE OPERATION ! Set cascading delete only when you want to remove data forever. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].cascadingDelete"},{"location":"nais-application/application/#gcpsqlinstancescollation","text":"Sort order for ORDER BY ... clauses. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].collation"},{"location":"nais-application/application/#gcpsqlinstancesdatabases","text":"List of databases that should be created on this Postgres server. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases"},{"location":"nais-application/application/#gcpsqlinstancesdatabasesenvvarprefix","text":"Prefix to add to environment variables made available for database connection. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].envVarPrefix"},{"location":"nais-application/application/#gcpsqlinstancesdatabasesname","text":"Database name. Type: string Required: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].name"},{"location":"nais-application/application/#gcpsqlinstancesdatabasesusers","text":"Add extra users for database access. These users need to be manually given access to database tables. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].users"},{"location":"nais-application/application/#gcpsqlinstancesdatabasesusersname","text":"User name. Type: string Required: true Pattern: ^[_a-zA-Z][-_a-zA-Z0-9]+$ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].users[].name"},{"location":"nais-application/application/#gcpsqlinstancesdiskautoresize","text":"When set to true, GCP will automatically increase storage by XXX for the database when disk usage is above the high water mark. Relevant information: https://cloud.google.com/sql/docs/postgres/instance-settings#threshold Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskAutoresize"},{"location":"nais-application/application/#gcpsqlinstancesdisksize","text":"How much hard drive space to allocate for the SQL server, in gigabytes. Type: integer Required: false Minimum value: 10 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskSize"},{"location":"nais-application/application/#gcpsqlinstancesdisktype","text":"Disk type to use for storage in the database. Type: enum Required: false Allowed values: HDD , SSD Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskType"},{"location":"nais-application/application/#gcpsqlinstanceshighavailability","text":"When set to true this will set up standby database for failover. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].highAvailability"},{"location":"nais-application/application/#gcpsqlinstancesinsights","text":"Configures query insights which are now default for new sql instances. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights"},{"location":"nais-application/application/#gcpsqlinstancesinsightsenabled","text":"True if Query Insights feature is enabled. Type: boolean Required: false Default value: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.enabled"},{"location":"nais-application/application/#gcpsqlinstancesinsightsquerystringlength","text":"Maximum query length stored in bytes. Between 256 and 4500. Default to 1024. Type: integer Required: false Value range: 256 - 4500 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.queryStringLength"},{"location":"nais-application/application/#gcpsqlinstancesinsightsrecordapplicationtags","text":"True if Query Insights will record application tags from query when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.recordApplicationTags"},{"location":"nais-application/application/#gcpsqlinstancesinsightsrecordclientaddress","text":"True if Query Insights will record client address when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.recordClientAddress"},{"location":"nais-application/application/#gcpsqlinstancesmaintenance","text":"Desired maintenance window for database updates. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance"},{"location":"nais-application/application/#gcpsqlinstancesmaintenanceday","text":"Type: integer Required: false Value range: 1 - 7 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance.day"},{"location":"nais-application/application/#gcpsqlinstancesmaintenancehour","text":"Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance.hour"},{"location":"nais-application/application/#gcpsqlinstancesname","text":"The name of the instance, if omitted the application name will be used. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].name"},{"location":"nais-application/application/#gcpsqlinstancespointintimerecovery","text":"Enables point-in-time recovery for sql instances using write-ahead logs. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].pointInTimeRecovery"},{"location":"nais-application/application/#gcpsqlinstancestier","text":"Server tier, i.e. how much CPU and memory allocated. Available tiers are db-f1-micro , db-g1-small and custom db-custom-CPU-RAM . Custom memory must be mulitple of 256 MB and at least 3.75 GB (e.g. db-custom-1-3840 for 1 cpu, 3840 MB ram) Also check out sizing your database . Type: string Required: false Default value: db-f1-micro Pattern: db-.+ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].tier"},{"location":"nais-application/application/#gcpsqlinstancestype","text":"PostgreSQL version. Type: enum Required: true Allowed values: POSTGRES_11 , POSTGRES_12 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].type"},{"location":"nais-application/application/#idporten","text":"Configures an ID-porten client for this application. See ID-porten for more details. Type: object Required: false Example spec : idporten : accessTokenLifetime : 3600 clientURI : https://www.nav.no enabled : true frontchannelLogoutPath : /oauth2/logout frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout postLogoutRedirectURIs : - https://www.nav.no redirectPath : /oauth2/callback redirectURI : https://myapplication.nav.no/oauth2/callback sessionLifetime : 7200 sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb","title":"idporten"},{"location":"nais-application/application/#idportenaccesstokenlifetime","text":"AccessTokenLifetime is the lifetime in seconds for any issued access token from ID-porten. If unspecified, defaults to 3600 seconds (1 hour). Type: integer Required: false Default value: 3600 Value range: 1 - 3600 Example spec : idporten : accessTokenLifetime : 3600","title":"idporten.accessTokenLifetime"},{"location":"nais-application/application/#idportenclienturi","text":"ClientURI is the URL shown to the user at ID-porten when displaying a 'back' button or on errors. Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : clientURI : https://www.nav.no","title":"idporten.clientURI"},{"location":"nais-application/application/#idportenenabled","text":"Whether to enable provisioning of an ID-porten client. If enabled, an ID-porten client be provisioned. Type: boolean Required: true Availability: team namespaces Example spec : idporten : enabled : true","title":"idporten.enabled"},{"location":"nais-application/application/#idportenfrontchannellogoutpath","text":"FrontchannelLogoutPath is a valid path for your application where ID-porten sends a request to whenever the user has initiated a logout elsewhere as part of a single logout (front channel logout) process. Relevant information: https://doc.nais.io/security/auth/idporten/#front-channel-logout https://docs.digdir.no/oidc_func_sso.html#2-h%C3%A5ndtere-utlogging-fra-id-porten Type: string Required: false Default value: /oauth2/logout Pattern: ^\\/.*$ Example spec : idporten : frontchannelLogoutPath : /oauth2/logout","title":"idporten.frontchannelLogoutPath"},{"location":"nais-application/application/#idportenfrontchannellogouturi","text":"Prefer using frontchannelLogoutPath . Deprecated This feature is deprecated, preserved only for backwards compatibility. Relevant information: https://doc.nais.io/security/auth/idporten/#front-channel-logout https://docs.digdir.no/oidc_func_sso.html#2-h%C3%A5ndtere-utlogging-fra-id-porten Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout","title":"idporten.frontchannelLogoutURI"},{"location":"nais-application/application/#idportenpostlogoutredirecturis","text":"PostLogoutRedirectURIs are valid URIs that ID-porten will allow redirecting the end-user to after a single logout has been initiated and performed by the application. Relevant information: https://doc.nais.io/security/auth/idporten/#self-initiated-logout https://docs.digdir.no/oidc_func_sso.html#1-utlogging-fra-egen-tjeneste Type: array Required: false Default value: https://www.nav.no Example spec : idporten : postLogoutRedirectURIs : - https://www.nav.no","title":"idporten.postLogoutRedirectURIs"},{"location":"nais-application/application/#idportenredirectpath","text":"RedirectPath is a valid path that ID-porten redirects back to after a successful authorization request. Type: string Required: false Default value: /oauth2/callback Pattern: ^\\/.*$ Example spec : idporten : redirectPath : /oauth2/callback","title":"idporten.redirectPath"},{"location":"nais-application/application/#idportenredirecturi","text":"Use redirectPath instead. Deprecated This feature is deprecated, preserved only for backwards compatibility. Type: string Required: false Pattern: ^https:\\/\\/.+$ Example spec : idporten : redirectURI : https://myapplication.nav.no/oauth2/callback","title":"idporten.redirectURI"},{"location":"nais-application/application/#idportensessionlifetime","text":"SessionLifetime is the maximum lifetime in seconds for any given user's session in your application. The timeout starts whenever the user is redirected from the authorization_endpoint at ID-porten. If unspecified, defaults to 7200 seconds (2 hours). Note: Attempting to refresh the user's access_token beyond this timeout will yield an error. Type: integer Required: false Default value: 7200 Value range: 3600 - 7200 Example spec : idporten : sessionLifetime : 7200","title":"idporten.sessionLifetime"},{"location":"nais-application/application/#idportensidecar","text":"Sidecar configures a sidecar that intercepts every HTTP request, and performs the OIDC flow if necessary. All requests to ingress + /oauth2 will be processed only by the sidecar, whereas all other requests will be proxied to the application. If the client is authenticated with IDPorten, the Authorization header will be set to Bearer <JWT> . Relevant information: https://doc.nais.io/security/auth/idporten/sidecar/ Type: object Required: false Example spec : idporten : sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb","title":"idporten.sidecar"},{"location":"nais-application/application/#idportensidecarautologin","text":"Automatically redirect the user to login for all proxied routes. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#auto-login Type: boolean Required: false Default value: false Example spec : idporten : sidecar : autoLogin : true","title":"idporten.sidecar.autoLogin"},{"location":"nais-application/application/#idportensidecarenabled","text":"Enable the sidecar. Type: boolean Required: true Example spec : idporten : sidecar : enabled : true","title":"idporten.sidecar.enabled"},{"location":"nais-application/application/#idportensidecarerrorpath","text":"Absolute path to redirect the user to on authentication errors for custom error handling. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#error-handling Type: string Required: false Example spec : idporten : sidecar : errorPath : /error","title":"idporten.sidecar.errorPath"},{"location":"nais-application/application/#idportensidecarlevel","text":"Default security level for all authentication requests. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#security-levels Type: enum Required: false Default value: Level4 Allowed values: Level3 , Level4 Example spec : idporten : sidecar : level : Level4","title":"idporten.sidecar.level"},{"location":"nais-application/application/#idportensidecarlocale","text":"Default user interface locale for all authentication requests. Relevant information: https://doc.nais.io/security/auth/idporten/sidecar#locales Type: enum Required: false Default value: nb Allowed values: en , nb , nn , se Example spec : idporten : sidecar : locale : nb","title":"idporten.sidecar.locale"},{"location":"nais-application/application/#image","text":"Your application's Docker image location and tag. Type: string Required: true Example spec : image : navikt/testapp:69.0.0","title":"image"},{"location":"nais-application/application/#influx","text":"An InfluxDB via Aiven. A typical use case for influxdb is to store metrics from your application and visualize them in Grafana. Type: object Required: false Availability: GCP Example spec : influx : instance : influx-instance","title":"influx"},{"location":"nais-application/application/#influxinstance","text":"Provisions an InfluxDB instance and configures your application to access it. Use the prefix: influx- + team that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : influx : instance : influx-instance","title":"influx.instance"},{"location":"nais-application/application/#ingresses","text":"List of URLs that will route HTTPS traffic to the application. All URLs must start with https:// . Domain availability differs according to which environment your application is running in. Relevant information: https://doc.nais.io/clusters/gcp/ https://doc.nais.io/clusters/on-premises/ Type: array Required: false Example spec : ingresses : - https://myapplication.nav.no","title":"ingresses"},{"location":"nais-application/application/#kafka","text":"Set up Aiven Kafka for your application. Relevant information: https://doc.nais.io/persistence/kafka/ Type: object Required: false Example spec : kafka : pool : nav-dev streams : true","title":"kafka"},{"location":"nais-application/application/#kafkapool","text":"Configures your application to access an Aiven Kafka cluster. Type: enum Required: true Allowed values: nav-dev , nav-infrastructure , nav-prod Example spec : kafka : pool : nav-dev","title":"kafka.pool"},{"location":"nais-application/application/#kafkastreams","text":"Allow this app to use kafka streams Relevant information: https://doc.nais.io/persistence/kafka/application/#using-kafka-streams-with-internal-topics Type: boolean Required: false Default value: false Availability: GCP Example spec : kafka : streams : true","title":"kafka.streams"},{"location":"nais-application/application/#leaderelection","text":"If true, an HTTP endpoint will be available at $ELECTOR_PATH that returns the current leader. Relevant information: https://doc.nais.io/addons/leader-election/ Type: boolean Required: false Example spec : leaderElection : true","title":"leaderElection"},{"location":"nais-application/application/#liveness","text":"Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations. Read more about this over at the Kubernetes probes documentation . Type: object Required: false Example spec : liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1","title":"liveness"},{"location":"nais-application/application/#livenessfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Default value: 3 Example spec : liveness : failureThreshold : 10","title":"liveness.failureThreshold"},{"location":"nais-application/application/#livenessinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : liveness : initialDelay : 20","title":"liveness.initialDelay"},{"location":"nais-application/application/#livenesspath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : liveness : path : /isalive","title":"liveness.path"},{"location":"nais-application/application/#livenessperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Default value: 10 Example spec : liveness : periodSeconds : 5","title":"liveness.periodSeconds"},{"location":"nais-application/application/#livenessport","text":"Port for the startup probe. Type: integer Required: false Example spec : liveness : port : 8080","title":"liveness.port"},{"location":"nais-application/application/#livenesstimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Default value: 1 Example spec : liveness : timeout : 1","title":"liveness.timeout"},{"location":"nais-application/application/#logformat","text":"Format of the logs from the container. Use this if the container doesn't support JSON logging and the log is in a special format that need to be parsed. Type: enum Required: false Allowed values: (empty string) , accesslog , accesslog_with_processing_time , accesslog_with_referer_useragent , capnslog , glog , gokit , influxdb , log15 , logrus , redis , simple Example spec : logformat : accesslog_with_referer_useragent","title":"logformat"},{"location":"nais-application/application/#logtransform","text":"Extra filters for modifying log content. This can e.g. be used for setting loglevel based on http status code. Type: enum Required: false Allowed values: dns_loglevel , http_loglevel Example spec : logtransform : http_loglevel","title":"logtransform"},{"location":"nais-application/application/#maskinporten","text":"Configures a Maskinporten client for this application. See Maskinporten for more details. Type: object Required: false Example spec : maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten"},{"location":"nais-application/application/#maskinportenenabled","text":"If enabled, provisions and configures a Maskinporten client with consumed scopes and/or Exposed scopes with DigDir. Type: boolean Required: true Default value: false Availability: team namespaces Example spec : maskinporten : enabled : true","title":"maskinporten.enabled"},{"location":"nais-application/application/#maskinportenscopes","text":"Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes. Type: object Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes"},{"location":"nais-application/application/#maskinportenscopesconsumes","text":"This is the Schema for the consumes and exposes API. consumes is a list of scopes that your client can request access to. Type: array Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read","title":"maskinporten.scopes.consumes"},{"location":"nais-application/application/#maskinportenscopesconsumesname","text":"The scope consumed by the application to gain access to an external organization API. Ensure that the NAV organization has been granted access to the scope prior to requesting access. Relevant information: https://doc.nais.io/security/auth/maskinporten/#consume-scopes Type: string Required: true Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read","title":"maskinporten.scopes.consumes[].name"},{"location":"nais-application/application/#maskinportenscopesexposes","text":"exposes is a list of scopes your application want to expose to other organization where access to the scope is based on organization number. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes"},{"location":"nais-application/application/#maskinportenscopesexposesallowedintegrations","text":"Whitelisting of integration's allowed. Default is maskinporten Relevant information: https://docs.digdir.no/maskinporten_guide_apitilbyder.html#scope-begrensninger Type: array Required: false Default value: maskinporten Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].allowedIntegrations"},{"location":"nais-application/application/#maskinportenscopesexposesatmaxage","text":"Max time in seconds for a issued access_token. Default is 30 sec. Type: integer Required: false Default value: 30 Value range: 30 - 680 Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].atMaxAge"},{"location":"nais-application/application/#maskinportenscopesexposesconsumers","text":"External consumers granted access to this scope and able to request access_token. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers"},{"location":"nais-application/application/#maskinportenscopesexposesconsumersname","text":"This is a describing field intended for clarity not used for any other purpose. Type: string Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers[].name"},{"location":"nais-application/application/#maskinportenscopesexposesconsumersorgno","text":"The external business/organization number. Type: string Required: true Pattern: ^\\d{9}$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers[].orgno"},{"location":"nais-application/application/#maskinportenscopesexposesenabled","text":"If Enabled the configured scope is available to be used and consumed by organizations granted access. Relevant information: https://doc.nais.io/naisjob/reference/#maskinportenscopesexposesconsumers Type: boolean Required: true Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].enabled"},{"location":"nais-application/application/#maskinportenscopesexposesname","text":"The actual subscope combined with Product . Ensure that <Product><Name> matches Pattern . Type: string Required: true Default value: false Pattern: ^([a-z\u00e6\u00f8\u00e50-9]+\\/?)+(\\:[a-z\u00e6\u00f8\u00e50-9]+)*[a-z\u00e6\u00f8\u00e50-9]+(\\.[a-z\u00e6\u00f8\u00e50-9]+)*$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].name"},{"location":"nais-application/application/#maskinportenscopesexposesproduct","text":"The product-area your application belongs to e.g. arbeid, helse ... This will be included in the final scope nav:<Product><Name> . Type: string Required: true Pattern: ^[a-z0-9]+$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].product"},{"location":"nais-application/application/#opensearch","text":"To get your own OpenSearch instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : openSearch : access : readwrite instance : my-open-search-instance","title":"openSearch"},{"location":"nais-application/application/#opensearchaccess","text":"Access level for OpenSearch user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : openSearch : access : readwrite","title":"openSearch.access"},{"location":"nais-application/application/#opensearchinstance","text":"Configure your application to access your OpenSearch instance. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : openSearch : instance : my-open-search-instance","title":"openSearch.instance"},{"location":"nais-application/application/#port","text":"The port number which is exposed by the container and should receive traffic. Note that ports under 1024 are unavailable. Type: integer Required: false Default value: 8080 Example spec : port : 8080","title":"port"},{"location":"nais-application/application/#prestophook","text":"PreStopHook is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits by itself. The reason for termination is passed to the handler. Relevant information: https://doc.nais.io/naisjob/#handles-termination-gracefully https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080","title":"preStopHook"},{"location":"nais-application/application/#prestophookexec","text":"Command that should be run inside the main container just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script","title":"preStopHook.exec"},{"location":"nais-application/application/#prestophookexeccommand","text":"Command is the command line to execute inside the container before the pod is shut down. The command is not run inside a shell, so traditional shell instructions (pipes, redirects, etc.) won't work. To use a shell, you need to explicitly call out to that shell. If the exit status is non-zero, the pod will still be shut down, and marked as Failed . Type: array Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script","title":"preStopHook.exec.command"},{"location":"nais-application/application/#prestophookhttp","text":"HTTP GET request that is called just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : http : path : /internal/stop port : 8080","title":"preStopHook.http"},{"location":"nais-application/application/#prestophookhttppath","text":"Path to access on the HTTP server. Type: string Required: true Example spec : preStopHook : http : path : /internal/stop","title":"preStopHook.http.path"},{"location":"nais-application/application/#prestophookhttpport","text":"Port to access on the container. Defaults to application port, as defined in .spec.port . Type: integer Required: false Value range: 1 - 65535 Example spec : preStopHook : http : port : 8080","title":"preStopHook.http.port"},{"location":"nais-application/application/#prestophookpath","text":"A HTTP GET will be issued to this endpoint at least once before the pod is terminated. This feature is deprecated and will be removed in the next major version (nais.io/v1). Relevant information: https://doc.nais.io/nais-application/#handles-termination-gracefully Type: string Required: false Example spec : preStopHookPath : /internal/stop","title":"preStopHookPath"},{"location":"nais-application/application/#prometheus","text":"Prometheus is used to scrape metrics from the pod . Use this configuration to override the default values. Type: object Required: false Example spec : prometheus : enabled : true path : /metrics port : \"8080\"","title":"prometheus"},{"location":"nais-application/application/#prometheusenabled","text":"Type: boolean Required: false Example spec : prometheus : enabled : true","title":"prometheus.enabled"},{"location":"nais-application/application/#prometheuspath","text":"Type: string Required: false Default value: /metrics Example spec : prometheus : path : /metrics","title":"prometheus.path"},{"location":"nais-application/application/#prometheusport","text":"Type: string Required: false Example spec : prometheus : port : \"8080\"","title":"prometheus.port"},{"location":"nais-application/application/#readiness","text":"Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the application, but you don\u2019t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services. Read more about this over at the Kubernetes readiness documentation . Type: object Required: false Example spec : readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1","title":"readiness"},{"location":"nais-application/application/#readinessfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : readiness : failureThreshold : 10","title":"readiness.failureThreshold"},{"location":"nais-application/application/#readinessinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : readiness : initialDelay : 20","title":"readiness.initialDelay"},{"location":"nais-application/application/#readinesspath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : readiness : path : /isready","title":"readiness.path"},{"location":"nais-application/application/#readinessperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Example spec : readiness : periodSeconds : 5","title":"readiness.periodSeconds"},{"location":"nais-application/application/#readinessport","text":"Port for the startup probe. Type: integer Required: false Example spec : readiness : port : 8080","title":"readiness.port"},{"location":"nais-application/application/#readinesstimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Example spec : readiness : timeout : 1","title":"readiness.timeout"},{"location":"nais-application/application/#replicas","text":"The numbers of pods to run in parallel. Type: object Required: false Example spec : replicas : cpuThresholdPercentage : 50 disableAutoScaling : true max : 4 min : 2","title":"replicas"},{"location":"nais-application/application/#replicascputhresholdpercentage","text":"Amount of CPU usage before the autoscaler kicks in. Type: integer Required: false Default value: 50 Example spec : replicas : cpuThresholdPercentage : 50","title":"replicas.cpuThresholdPercentage"},{"location":"nais-application/application/#replicasdisableautoscaling","text":"Disable autoscaling Type: boolean Required: false Default value: false Example spec : replicas : disableAutoScaling : true","title":"replicas.disableAutoScaling"},{"location":"nais-application/application/#replicasmax","text":"The pod autoscaler will increase replicas when required up to the maximum. Type: integer Required: false Default value: 4 Example spec : replicas : max : 4","title":"replicas.max"},{"location":"nais-application/application/#replicasmin","text":"The minimum amount of running replicas for a deployment. Type: integer Required: false Default value: 2 Example spec : replicas : min : 2","title":"replicas.min"},{"location":"nais-application/application/#resources","text":"When Containers have resource requests specified, the Kubernetes scheduler can make better decisions about which nodes to place pods on. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi","title":"resources"},{"location":"nais-application/application/#resourceslimits","text":"Limit defines the maximum amount of resources a container can use before getting evicted. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi","title":"resources.limits"},{"location":"nais-application/application/#resourceslimitscpu","text":"Type: string Required: false Default value: 500m Pattern: ^\\d+m?$ Example spec : resources : limits : cpu : 500m","title":"resources.limits.cpu"},{"location":"nais-application/application/#resourceslimitsmemory","text":"Type: string Required: false Default value: 512Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : limits : memory : 512Mi","title":"resources.limits.memory"},{"location":"nais-application/application/#resourcesrequests","text":"Request defines the amount of resources a container is allocated on startup. Type: object Required: false Example spec : resources : requests : cpu : 200m memory : 256Mi","title":"resources.requests"},{"location":"nais-application/application/#resourcesrequestscpu","text":"Type: string Required: false Default value: 200m Pattern: ^\\d+m?$ Example spec : resources : requests : cpu : 200m","title":"resources.requests.cpu"},{"location":"nais-application/application/#resourcesrequestsmemory","text":"Type: string Required: false Default value: 256Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : requests : memory : 256Mi","title":"resources.requests.memory"},{"location":"nais-application/application/#securelogs","text":"Whether or not to enable a sidecar container for secure logging. Type: object Required: false Example spec : secureLogs : enabled : true","title":"secureLogs"},{"location":"nais-application/application/#securelogsenabled","text":"Whether to enable a sidecar container for secure logging. If enabled, a volume is mounted in the pods where secure logs can be saved. Type: boolean Required: true Default value: false Example spec : secureLogs : enabled : true","title":"secureLogs.enabled"},{"location":"nais-application/application/#service","text":"Specify which port and protocol is used to connect to the application in the container. Defaults to HTTP on port 80. Type: object Required: false Example spec : service : port : 80 protocol : http","title":"service"},{"location":"nais-application/application/#serviceport","text":"Port for the default service. Default port is 80. Type: integer Required: true Default value: 80 Example spec : service : port : 80","title":"service.port"},{"location":"nais-application/application/#serviceprotocol","text":"Which protocol the backend service runs on. Default is http . Type: enum Required: false Default value: http Allowed values: grpc , http , redis , tcp Example spec : service : protocol : http","title":"service.protocol"},{"location":"nais-application/application/#skipcabundle","text":"Whether to skip injection of NAV certificate authority bundle or not. Defaults to false. Type: boolean Required: false Example spec : skipCaBundle : true","title":"skipCaBundle"},{"location":"nais-application/application/#startup","text":"Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Type: object Required: false Example spec : startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1","title":"startup"},{"location":"nais-application/application/#startupfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : startup : failureThreshold : 10","title":"startup.failureThreshold"},{"location":"nais-application/application/#startupinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : startup : initialDelay : 20","title":"startup.initialDelay"},{"location":"nais-application/application/#startuppath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : startup : path : /started","title":"startup.path"},{"location":"nais-application/application/#startupperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Example spec : startup : periodSeconds : 5","title":"startup.periodSeconds"},{"location":"nais-application/application/#startupport","text":"Port for the startup probe. Type: integer Required: false Example spec : startup : port : 8080","title":"startup.port"},{"location":"nais-application/application/#startuptimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Example spec : startup : timeout : 1","title":"startup.timeout"},{"location":"nais-application/application/#strategy","text":"Specifies the strategy used to replace old Pods by new ones. Type: object Required: false Example spec : strategy : type : RollingUpdate","title":"strategy"},{"location":"nais-application/application/#strategytype","text":"Specifies the strategy used to replace old Pods by new ones. RollingUpdate is the default value. Type: enum Required: true Default value: RollingUpdate Allowed values: Recreate , RollingUpdate Example spec : strategy : type : RollingUpdate","title":"strategy.type"},{"location":"nais-application/application/#tokenx","text":"Provisions and configures a TokenX client for your application. Relevant information: https://doc.nais.io/security/auth/tokenx/ Type: object Required: false Example spec : tokenx : enabled : true mountSecretsAsFilesOnly : true","title":"tokenx"},{"location":"nais-application/application/#tokenxenabled","text":"If enabled, will provision and configure a TokenX client and inject an accompanying secret. Type: boolean Required: true Default value: false Example spec : tokenx : enabled : true","title":"tokenx.enabled"},{"location":"nais-application/application/#tokenxmountsecretsasfilesonly","text":"If enabled, secrets for TokenX will be mounted as files only, i.e. not as environment variables. Type: boolean Required: false Example spec : tokenx : mountSecretsAsFilesOnly : true","title":"tokenx.mountSecretsAsFilesOnly"},{"location":"nais-application/application/#vault","text":"Provides secrets management, identity-based access, and encrypting application data for auditing of secrets for applications, systems, and users. Relevant information: https://github.com/navikt/vault-iac/tree/master/doc Type: object Required: false Availability: on-premises Example spec : vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true","title":"vault"},{"location":"nais-application/application/#vaultenabled","text":"If set to true, fetch secrets from Vault and inject into the pods. Type: boolean Required: false Example spec : vault : enabled : true","title":"vault.enabled"},{"location":"nais-application/application/#vaultpaths","text":"List of secret paths to be read from Vault and injected into the pod's filesystem. Overriding the paths array is optional, and will give you fine-grained control over which Vault paths that will be mounted on the file system. By default, the list will contain an entry with kvPath: /kv/<environment>/<zone>/<application>/<namespace> mountPath: /var/run/secrets/nais.io/vault that will always be attempted to be mounted. Type: array Required: false Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths"},{"location":"nais-application/application/#vaultpathsformat","text":"Format of the secret that should be processed. Type: enum Required: false Allowed values: (empty string) , env , flatten , json , properties , yaml Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].format"},{"location":"nais-application/application/#vaultpathskvpath","text":"Path to Vault key/value store that should be mounted into the file system. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].kvPath"},{"location":"nais-application/application/#vaultpathsmountpath","text":"File system path that the secret will be mounted into. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].mountPath"},{"location":"nais-application/application/#vaultsidecar","text":"If enabled, the sidecar will automatically refresh the token's Time-To-Live before it expires. Type: boolean Required: false Example spec : vault : sidecar : true","title":"vault.sidecar"},{"location":"nais-application/application/#webproxy","text":"Inject on-premises web proxy configuration into the application pod. Most Linux applications should auto-detect these settings from the $HTTP_PROXY , $HTTPS_PROXY and $NO_PROXY environment variables (and their lowercase counterparts). Java applications can start the JVM using parameters from the $JAVA_PROXY_OPTIONS environment variable. Type: boolean Required: false Availability: on-premises Example spec : webproxy : true","title":"webproxy"},{"location":"nais-application/automatic-scaling/","text":"Automatic Scaling \u00a7 Warning In order to use custom scaling policies and rules, make sure you disable default NAIS HPA by setting spec.replicas.min == spec.replicas.max Scaling based on custom metrics \u00a7 A custom metric is based on a direct value or a rate over time. To make the custom metric available for scaling, you have to label it with either hpa=\"value\" or hpa=\"rate\" Example metric output: # HELP active_sessions number of active sessions # TYPE active_sessions gauge active_sessions{hpa=\"value\"} 100 # HELP documents_received how many documents have we received # TYPE documents_received counter documents_received{hpa=\"rate\"} 69 Once the metric is labelled correctly, it can be used in a HorizontalPodAutoscaler Kubernetes object. Refer to the Kubernetes documentation for details. In the example below, the amount of replicas will be increased once the average of active_sessions exceeds 150 across all currently running pods. apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : name : hpa-example namespace : team-namespace spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : deployment-name minReplicas : 2 maxReplicas : 10 metrics : - type : Pods pods : metric : name : active_sessions target : type : AverageValue averageValue : 150 Info The platform provides metrics from the LinkerD sidecar by default: linkerd_request_total . This metric contains the rate of inbound requests. Scaling based on external metrics \u00a7 External metrics are provided by the platform for services external to the application, i.e. Kafka lag. If you want your application to scale based on external metrics, replace the metrics section of the previous example with the one below. metrics : - type : External external : metric : name : kafka_consumergroup_group_lag selector : matchLabels : topic : your-topic group : your-consumer-group target : type : AverageValue averageValue : 10000 Scaling behaviour \u00a7 You can also override the default behaviour of the autoscaler by configuring the HPA See Kubernetes documentation for details","title":"Automatic scaling"},{"location":"nais-application/automatic-scaling/#automatic-scaling","text":"Warning In order to use custom scaling policies and rules, make sure you disable default NAIS HPA by setting spec.replicas.min == spec.replicas.max","title":"Automatic Scaling"},{"location":"nais-application/automatic-scaling/#scaling-based-on-custom-metrics","text":"A custom metric is based on a direct value or a rate over time. To make the custom metric available for scaling, you have to label it with either hpa=\"value\" or hpa=\"rate\" Example metric output: # HELP active_sessions number of active sessions # TYPE active_sessions gauge active_sessions{hpa=\"value\"} 100 # HELP documents_received how many documents have we received # TYPE documents_received counter documents_received{hpa=\"rate\"} 69 Once the metric is labelled correctly, it can be used in a HorizontalPodAutoscaler Kubernetes object. Refer to the Kubernetes documentation for details. In the example below, the amount of replicas will be increased once the average of active_sessions exceeds 150 across all currently running pods. apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : name : hpa-example namespace : team-namespace spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : deployment-name minReplicas : 2 maxReplicas : 10 metrics : - type : Pods pods : metric : name : active_sessions target : type : AverageValue averageValue : 150 Info The platform provides metrics from the LinkerD sidecar by default: linkerd_request_total . This metric contains the rate of inbound requests.","title":"Scaling based on custom metrics"},{"location":"nais-application/automatic-scaling/#scaling-based-on-external-metrics","text":"External metrics are provided by the platform for services external to the application, i.e. Kafka lag. If you want your application to scale based on external metrics, replace the metrics section of the previous example with the one below. metrics : - type : External external : metric : name : kafka_consumergroup_group_lag selector : matchLabels : topic : your-topic group : your-consumer-group target : type : AverageValue averageValue : 10000","title":"Scaling based on external metrics"},{"location":"nais-application/automatic-scaling/#scaling-behaviour","text":"You can also override the default behaviour of the autoscaler by configuring the HPA See Kubernetes documentation for details","title":"Scaling behaviour"},{"location":"nais-application/config-reloading/","text":"Configuration reloading \u00a7 Info New feature; introduced August 30 th 2021 The NAIS platform has the ability to restart your application if the underlying configuration in a ConfigMap or Secret resource changes. This is an opt-in feature that is enabled by adding the following annotation to the ConfigMap or Secret resource. Note that this works only for applications that are deployed using an Application resource. kind : Secret # or ConfigMap metadata : annotations : reloader.stakater.com/match : \"true\"","title":"Config reloading"},{"location":"nais-application/config-reloading/#configuration-reloading","text":"Info New feature; introduced August 30 th 2021 The NAIS platform has the ability to restart your application if the underlying configuration in a ConfigMap or Secret resource changes. This is an opt-in feature that is enabled by adding the following annotation to the ConfigMap or Secret resource. Note that this works only for applications that are deployed using an Application resource. kind : Secret # or ConfigMap metadata : annotations : reloader.stakater.com/match : \"true\"","title":"Configuration reloading"},{"location":"nais-application/debugging/","text":"If you have problems getting your pods running you should check out the official documentation from Kubernetes: Troubleshoot Applications Debug Running Pods PS: Applications in the context above is not the NAIS applications. Debugging a NAIS applications resource is done with kubectl describe application $app_name .","title":"Debugging"},{"location":"nais-application/example/","text":"NAIS Application example YAML \u00a7 This is a complete example of an Application resource, commonly known as the nais.yaml file. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1alpha1 kind : Application metadata : creationTimestamp : null labels : team : myteam name : myapplication namespace : myteam spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no sidecar : autoLogin : true enabled : true errorPath : /error cleanup : enabled : true gracePeriod : 24h strategy : - downscale command : - /app/myapplication - --param - value - --other-param - other-value elastic : access : readwrite instance : my-elastic-instance env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 idporten : accessTokenLifetime : 3600 clientURI : https://www.nav.no enabled : true frontchannelLogoutPath : /oauth2/logout frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout postLogoutRedirectURIs : - https://www.nav.no redirectPath : /oauth2/callback redirectURI : https://myapplication.nav.no/oauth2/callback sessionLifetime : 7200 sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb image : navikt/testapp:69.0.0 influx : instance : influx-instance ingresses : - https://myapplication.nav.no kafka : pool : nav-dev streams : true leaderElection : true liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 logformat : accesslog_with_referer_useragent logtransform : http_loglevel maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch : access : readwrite instance : my-open-search-instance port : 8080 preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 preStopHookPath : /internal/stop prometheus : enabled : true path : /metrics port : \"8080\" readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 replicas : cpuThresholdPercentage : 50 disableAutoScaling : true max : 4 min : 2 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi secureLogs : enabled : true service : port : 80 protocol : http skipCaBundle : true startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 strategy : type : RollingUpdate tokenx : enabled : true mountSecretsAsFilesOnly : true vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true webproxy : true status : {}","title":"Full example"},{"location":"nais-application/example/#nais-application-example-yaml","text":"This is a complete example of an Application resource, commonly known as the nais.yaml file. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1alpha1 kind : Application metadata : creationTimestamp : null labels : team : myteam name : myapplication namespace : myteam spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no sidecar : autoLogin : true enabled : true errorPath : /error cleanup : enabled : true gracePeriod : 24h strategy : - downscale command : - /app/myapplication - --param - value - --other-param - other-value elastic : access : readwrite instance : my-elastic-instance env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 idporten : accessTokenLifetime : 3600 clientURI : https://www.nav.no enabled : true frontchannelLogoutPath : /oauth2/logout frontchannelLogoutURI : https://myapplication.nav.no/oauth2/logout postLogoutRedirectURIs : - https://www.nav.no redirectPath : /oauth2/callback redirectURI : https://myapplication.nav.no/oauth2/callback sessionLifetime : 7200 sidecar : autoLogin : true enabled : true errorPath : /error level : Level4 locale : nb image : navikt/testapp:69.0.0 influx : instance : influx-instance ingresses : - https://myapplication.nav.no kafka : pool : nav-dev streams : true leaderElection : true liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 logformat : accesslog_with_referer_useragent logtransform : http_loglevel maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch : access : readwrite instance : my-open-search-instance port : 8080 preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 preStopHookPath : /internal/stop prometheus : enabled : true path : /metrics port : \"8080\" readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 replicas : cpuThresholdPercentage : 50 disableAutoScaling : true max : 4 min : 2 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi secureLogs : enabled : true service : port : 80 protocol : http skipCaBundle : true startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 strategy : type : RollingUpdate tokenx : enabled : true mountSecretsAsFilesOnly : true vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true webproxy : true status : {}","title":"NAIS Application example YAML"},{"location":"nais-application/good-practices/","text":"This document describes the different properties a NAIS application should have. Set reasonable resource requests and limits \u00a7 Setting reasonable resource requests and limits helps keep the clusters healthy, while not wasting resources. A rule of thumb is to set the requested CPU and memory to what your applications uses under \"normal\" circumstances, and set limits to what it is reasonable to allow the application to use at most. Most of the time, the important aspect is requests , because this says how much Kubernetes should reserve for your application. If you request too much, we are wasting resources. If you request too little, you run the risk of your application being starved in a low-resource situation. limits is mostly a mechanism to protect the cluster when something goes wrong. A memory leak that results in your application using all available memory on the node is inconvenient. Handles termination gracefully \u00a7 The application should make sure it listens to the SIGTERM signal, and prepare for shutdown (closing connections etc.) upon receival. When running on NAIS (or Kubernetes, actually) your application must be able to handle being shut down at any given time. This is because the platform might have to reboot the node your application is running on (e.g. because of a OS patch requiring restart), and in that case will reschedule your application on a different node. To best be able to handle this in your application, it helps to be aware of the relevant parts of the termination lifecycle. Application (pod) gets status TERMINATING , and grace period starts (default 30s) (simultaneous with 1) If the pod has a preStop hook defined, this is invoked (simultaneous with 1) The pod is removed from the list of endpoints i.e. taken out of load balancing (simultaneous with 1, but after preStop if defined) Container receives SIGTERM , and should prepare for shutdown Grace period ends, and container receives SIGKILL Pod disappears from the API, and is no longer visible for the client. The platform will automatically add a preStop -hook that pauses the termination sufficiently that e.g. the ingress controller has time to update it's list of endpoints (thus avoid sending traffic to a application while terminating). Exposes relevant application metrics \u00a7 The application should be instrumented using Prometheus , exposing the relevant application metrics. See the metrics documentation for more info. Writes structured logs to stdout \u00a7 The application should emit json -formatted logs by writing directly to standard output. This will make it easier to index, view and search the logs later. See more details in the logs documentation . Crashes on fatal errors \u00a7 If the application reaches an unrecoverable error, you should let it crash. Instead, you should immediately exit the process and let the kubelet restart the container. By restarting the container, you allow for the eventual readiness of other dependencies. Implements readiness and liveness endpoints \u00a7 The readiness -probe is used by Kubernetes to determine if the application should receive traffic, while the liveness -probe lets Kubernetes know if your application is alive. If it's dead, Kubernetes will remove the pod and bring up a new one. They should be implemented as separate services as they usually have different characteristics. liveness -probe should simply return HTTP 200 OK if main loop is running, and HTTP 5xx if not. readiness -probe returns HTTP 200 OK is able to process requests, and HTTP 5xx if not. If the application has dependencies to e.g. a database to serve traffic, it's a good idea to check if the database is available in the readiness -probe. Useful resources on the topic: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes https://medium.com/metrosystemsro/kubernetes-readiness-liveliness-probes-best-practices-86c3cd9f0b4a https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-revisited-how-to-avoid-shooting-yourself-in-the-other-foot/#letitcrash \u00a7","title":"Good practices"},{"location":"nais-application/good-practices/#set-reasonable-resource-requests-and-limits","text":"Setting reasonable resource requests and limits helps keep the clusters healthy, while not wasting resources. A rule of thumb is to set the requested CPU and memory to what your applications uses under \"normal\" circumstances, and set limits to what it is reasonable to allow the application to use at most. Most of the time, the important aspect is requests , because this says how much Kubernetes should reserve for your application. If you request too much, we are wasting resources. If you request too little, you run the risk of your application being starved in a low-resource situation. limits is mostly a mechanism to protect the cluster when something goes wrong. A memory leak that results in your application using all available memory on the node is inconvenient.","title":"Set reasonable resource requests and limits"},{"location":"nais-application/good-practices/#handles-termination-gracefully","text":"The application should make sure it listens to the SIGTERM signal, and prepare for shutdown (closing connections etc.) upon receival. When running on NAIS (or Kubernetes, actually) your application must be able to handle being shut down at any given time. This is because the platform might have to reboot the node your application is running on (e.g. because of a OS patch requiring restart), and in that case will reschedule your application on a different node. To best be able to handle this in your application, it helps to be aware of the relevant parts of the termination lifecycle. Application (pod) gets status TERMINATING , and grace period starts (default 30s) (simultaneous with 1) If the pod has a preStop hook defined, this is invoked (simultaneous with 1) The pod is removed from the list of endpoints i.e. taken out of load balancing (simultaneous with 1, but after preStop if defined) Container receives SIGTERM , and should prepare for shutdown Grace period ends, and container receives SIGKILL Pod disappears from the API, and is no longer visible for the client. The platform will automatically add a preStop -hook that pauses the termination sufficiently that e.g. the ingress controller has time to update it's list of endpoints (thus avoid sending traffic to a application while terminating).","title":"Handles termination gracefully"},{"location":"nais-application/good-practices/#exposes-relevant-application-metrics","text":"The application should be instrumented using Prometheus , exposing the relevant application metrics. See the metrics documentation for more info.","title":"Exposes relevant application metrics"},{"location":"nais-application/good-practices/#writes-structured-logs-to-stdout","text":"The application should emit json -formatted logs by writing directly to standard output. This will make it easier to index, view and search the logs later. See more details in the logs documentation .","title":"Writes structured logs to stdout"},{"location":"nais-application/good-practices/#crashes-on-fatal-errors","text":"If the application reaches an unrecoverable error, you should let it crash. Instead, you should immediately exit the process and let the kubelet restart the container. By restarting the container, you allow for the eventual readiness of other dependencies.","title":"Crashes on fatal errors"},{"location":"nais-application/good-practices/#implements-readiness-and-liveness-endpoints","text":"The readiness -probe is used by Kubernetes to determine if the application should receive traffic, while the liveness -probe lets Kubernetes know if your application is alive. If it's dead, Kubernetes will remove the pod and bring up a new one. They should be implemented as separate services as they usually have different characteristics. liveness -probe should simply return HTTP 200 OK if main loop is running, and HTTP 5xx if not. readiness -probe returns HTTP 200 OK is able to process requests, and HTTP 5xx if not. If the application has dependencies to e.g. a database to serve traffic, it's a good idea to check if the database is available in the readiness -probe. Useful resources on the topic: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes https://medium.com/metrosystemsro/kubernetes-readiness-liveliness-probes-best-practices-86c3cd9f0b4a https://blog.colinbreck.com/kubernetes-liveness-and-readiness-probes-revisited-how-to-avoid-shooting-yourself-in-the-other-foot/#letitcrash","title":"Implements readiness and liveness endpoints"},{"location":"nais-application/good-practices/#_1","text":"","title":""},{"location":"nais-application/ingress/","text":"Ingress parameters \u00a7 Info Ingress parameter customization is enabled for GCP clusters only. You can tweak the Ingress configuration by specifying certain Kubernetes annotations in your app spec. A list of supported variables are specified in the Nginx ingress documentation . As the Nginx ingress documentation states, these parameters are set on the Ingress object. However, Naiserator will copy these parameters from your Application spec. Example \u00a7 apiVersion: nais.io/v1alpha1 kind: Application metadata: name: myapplication namespace: myteam annotations: nginx.ingress.kubernetes.io/proxy-body-size: \"256M\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" spec: service: protocol: http ingresses: - https://myapplication.nav.no This will result in an ingress with: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: myapplication-gw-nav-no-abcd1234 namespace: myteam annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\" # from \".service.protocol\" nginx.ingress.kubernetes.io/use-regex: \"true\" # this is always on nginx.ingress.kubernetes.io/proxy-body-size: \"256M\" # copied from annotations nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" # copied from annotations","title":"Ingress customization"},{"location":"nais-application/ingress/#ingress-parameters","text":"Info Ingress parameter customization is enabled for GCP clusters only. You can tweak the Ingress configuration by specifying certain Kubernetes annotations in your app spec. A list of supported variables are specified in the Nginx ingress documentation . As the Nginx ingress documentation states, these parameters are set on the Ingress object. However, Naiserator will copy these parameters from your Application spec.","title":"Ingress parameters"},{"location":"nais-application/ingress/#example","text":"apiVersion: nais.io/v1alpha1 kind: Application metadata: name: myapplication namespace: myteam annotations: nginx.ingress.kubernetes.io/proxy-body-size: \"256M\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" spec: service: protocol: http ingresses: - https://myapplication.nav.no This will result in an ingress with: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: myapplication-gw-nav-no-abcd1234 namespace: myteam annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\" # from \".service.protocol\" nginx.ingress.kubernetes.io/use-regex: \"true\" # this is always on nginx.ingress.kubernetes.io/proxy-body-size: \"256M\" # copied from annotations nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\" # copied from annotations","title":"Example"},{"location":"nais-application/linkerd/","text":"Linkerd/sidecar customizations \u00a7 Info Linkerd customization is enabled for GCP clusters only. You can tweak the Linkerd configuration by specifying certain Kubernetes annotations in your app spec. A list of supported variables are specified in the Linkerd proxy configuration documentation . Annotations starting with config.linkerd.io/ will be propagated to the PodSpec . Example \u00a7 apiVersion: nais.io/v1alpha1 kind: Application metadata: name: myapplication namespace: myteam annotations: config.linkerd.io/proxy-memory-limit: \"400M\" config.linkerd.io/proxy-memory-request: \"200M\" spec: ...","title":"Linkerd / sidecar customization"},{"location":"nais-application/linkerd/#linkerdsidecar-customizations","text":"Info Linkerd customization is enabled for GCP clusters only. You can tweak the Linkerd configuration by specifying certain Kubernetes annotations in your app spec. A list of supported variables are specified in the Linkerd proxy configuration documentation . Annotations starting with config.linkerd.io/ will be propagated to the PodSpec .","title":"Linkerd/sidecar customizations"},{"location":"nais-application/linkerd/#example","text":"apiVersion: nais.io/v1alpha1 kind: Application metadata: name: myapplication namespace: myteam annotations: config.linkerd.io/proxy-memory-limit: \"400M\" config.linkerd.io/proxy-memory-request: \"200M\" spec: ...","title":"Example"},{"location":"nais-application/securitycontext/","text":"Container security context \u00a7 Kubernetes restricts the capabilities of containers by using SecurityContext settings. This feature advances the security in the pods running on Kubernetes. By default we set the following securityContext in the PodSpec for the application container: setting value runAsUser 1069 runAsGroup 1069 allowPrivilegeEscalation false readOnlyRootFilesystem true runAsNonRoot true privileged false capabilities drop: [\"all\"] Enable specific kernel capabilities \u00a7 Enable specific kernel capabilities by adding the following annotation to your Application or NaisJob spec: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/add-kernel-capability : \"NET_RAW\" The annotation supports multiple values separated by comma. Not all capabilities are supported, so if you encounter issues with missing capabilities contact the nais team. A list of capabilities can be found here Disable read-only file system \u00a7 By default, the only writable path on the file system is /tmp . If your application requires writing to another location, it is possible to enable this by setting the following annotation: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/read-only-file-system : \"false\" Note that even though the file system is writable, the default user 1069 (or whatever you override it with) needs write permission inside the docker image. Overriding runAsUser / runAsGroup \u00a7 By default the container runs with user and group id 1069 . If you need to override this for your container, you can add the following annotations to your Application . apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/run-as-user : \"1001\" nais.io/run-as-group : \"1002\" The nais.io/run-as-group will default to what you specify as nais.io/run-as-user . Relevant information \u00a7 Configure security context Docker security best practices","title":"Container security context / kernel capabilities"},{"location":"nais-application/securitycontext/#container-security-context","text":"Kubernetes restricts the capabilities of containers by using SecurityContext settings. This feature advances the security in the pods running on Kubernetes. By default we set the following securityContext in the PodSpec for the application container: setting value runAsUser 1069 runAsGroup 1069 allowPrivilegeEscalation false readOnlyRootFilesystem true runAsNonRoot true privileged false capabilities drop: [\"all\"]","title":"Container security context"},{"location":"nais-application/securitycontext/#enable-specific-kernel-capabilities","text":"Enable specific kernel capabilities by adding the following annotation to your Application or NaisJob spec: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/add-kernel-capability : \"NET_RAW\" The annotation supports multiple values separated by comma. Not all capabilities are supported, so if you encounter issues with missing capabilities contact the nais team. A list of capabilities can be found here","title":"Enable specific kernel capabilities"},{"location":"nais-application/securitycontext/#disable-read-only-file-system","text":"By default, the only writable path on the file system is /tmp . If your application requires writing to another location, it is possible to enable this by setting the following annotation: apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/read-only-file-system : \"false\" Note that even though the file system is writable, the default user 1069 (or whatever you override it with) needs write permission inside the docker image.","title":"Disable read-only file system"},{"location":"nais-application/securitycontext/#overriding-runasuser-runasgroup","text":"By default the container runs with user and group id 1069 . If you need to override this for your container, you can add the following annotations to your Application . apiVersion : nais.io/v1alpha1 kind : Application metadata : annotations : nais.io/run-as-user : \"1001\" nais.io/run-as-group : \"1002\" The nais.io/run-as-group will default to what you specify as nais.io/run-as-user .","title":"Overriding runAsUser / runAsGroup"},{"location":"nais-application/securitycontext/#relevant-information","text":"Configure security context Docker security best practices","title":"Relevant information"},{"location":"naisjob/","text":"Naisjob \u00a7 A Naisjob is very similar to a NAIS Application , except that it only runs once by default. It is built on the Kubernetes Job and CronJob resources. A CronJob runs Job s on a time-based schedule, as denoted in .spec.schedule . Restart policies Naisjobs have restart policies set to \"Never\", and it is currently not possible to set the restart policy of a Naisjob directly. It is possible to achieve automatic restarts on failure by using startup probes however, though this requires the Pod to expose an HTTP endpoint to answer the probe. Below you can find a minimal Naisjob example with a schedule for running a Job every minute. If you don't need to have a recurring Naisjob, remove the schedule field from the configuration. apiVersion: nais.io/v1 kind: Naisjob metadata: labels: team: myteam name: myjob namespace: myteam spec: image: ghcr.io/navikt/myapp:mytag schedule: \"*/1 * * * *\" Behind the scenes, when you make a Naisjob , Kubernetes makes a bunch of different resources for you. Naisjob is one resource that has been defined by the NAIS team. When you deploy a Naisjob with spec.schedule , Kubernetes will create a CronJob for you. A Cronjob is a Kubernetes native resource. When the Cronjob - schedule triggers, Kubernetes will create a Job -ressource, which creates a Pod -resource. The Pod is what actually runs your code. If you make a Naisjob without schedule , Kubernetes will create a Job -resource directly, which will create a Pod -resource. Below you can see some of the resources the Naisjob tiltak-okonomi-avstemming creates. \u279c k tree naisjob tiltak-okonomi-avstemming NAMESPACE NAME READY REASON AGE arbeidsgiver Naisjob/tiltak-okonomi-avstemming - 109d arbeidsgiver \u251c\u2500Job/tiltak-okonomi-avstemming - 109d arbeidsgiver \u2502 \u2514\u2500Pod/tiltak-okonomi-avstemming-brb7c True 12d Re-run Naisjob \u00a7 If you don't want to run your job at a schedule, but still want to re-run your Naisjob, you either have to delete the old Naisjob first, og run under a different name. Applying your Naisjob to Kubernetes \u00a7 You can deploy your Naisjob just as you would deploy your Application using NAIS deploy . Shutting down extra containers \u00a7 Due to the nature of NAIS, your Naisjob might end up having a few sidecars. Since a sidecar typically has little to no logic to look at sibling containers, these would normally not turn off once your app completes. As a result, the Naisjob would continue running indefinitely unless some entity from above sorts it out. To remediate this, an operator named Ginuudan has been created to run in every cluster and observe every Naisjob. Once the \"main\" container in a Naisjob completes, Ginuudan will shut down the surrounding sidecars and thus make the Naisjob run to completion. You can look at what Ginuudan has done to complete your Naisjob by running kubectl describe pod my-naisjob-3f91a0 and viewing the Events section. My Naisjob hasn't completed If you see no events from Ginuudan in your Naisjob, you might want to redeploy it. Naisjobs deployed before Ginuudan was published will not be observed by Ginuudan. If you do see events from Ginuudan but your Naisjob didn't complete, we'd be happy if you could tell us in the #nais channel on Slack. My Naisjob deployment is old, and I want to manually shut down the sidecars to make the Naisjob complete Here's some legacy documentation on how to shut down most sidecars. Linkerd \u00a7 Linkerd exposes an endpoint to shut itself down. curl -X POST http://127.0.0.1:4191/shutdown This is done from inside your pod-container. CloudSQL-proxy sidecar \u00a7 CloudSQL-proxy sidecar does not support turning off remotely. It can be shut down by running exec into the container. kubectl exec yourpod-12345 -c cloudsql-proxy -- kill -s INT 1 Securelogs \u00a7 Securelogs runs on Fluentd, and Fluentd exposes an endpoint to shut itself down. curl http://127.0.0.1:24444/api/processes.killWorkers This is done from inside your pod-container. Additionally, Securelogs runs a second sidecar called secure-logs-configmap-reload . This can be shut down by running exec into the container. kubectl exec yourpod-12345 -c secure-logs-configmap-reload -- /bin/killall configmap-reload Vault sidecar \u00a7 Vault sidecar does not support turning off remotely. It can be shut down by running exec into the container. kubectl exec yourpod-12345 -c vks-sidecar -- /bin/kill -s INT 1 Troubleshooting / FAQ \u00a7 My NaisJob doesn't want to start any scheduled Jobs \u00a7 Answer Sometimes, if the defined schedule attempts to start Jobs frequently without succeeding, Kubernetes will eventually stop attempting to start the Job. You might see the following event or message for the related CronJob ( kubectl describe cronjob <name> ): Cannot determine if job needs to be started: too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew Currently, the only way to fix this is to delete the CronJob and NaisJob, and then re-apply or -deploy the NaisJob: kubectl delete cronjob <name> kubectl delete naisjob <name> kubectl apply -f <path-to-naisjob.yaml> How to I force run my scheduled job \u00a7 Answer Run the following command to create a ad-hoc job based on your cronjob: kubectl create job --name = <newname> --from = cronjobs/<naisjobname>","title":"Overview"},{"location":"naisjob/#naisjob","text":"A Naisjob is very similar to a NAIS Application , except that it only runs once by default. It is built on the Kubernetes Job and CronJob resources. A CronJob runs Job s on a time-based schedule, as denoted in .spec.schedule . Restart policies Naisjobs have restart policies set to \"Never\", and it is currently not possible to set the restart policy of a Naisjob directly. It is possible to achieve automatic restarts on failure by using startup probes however, though this requires the Pod to expose an HTTP endpoint to answer the probe. Below you can find a minimal Naisjob example with a schedule for running a Job every minute. If you don't need to have a recurring Naisjob, remove the schedule field from the configuration. apiVersion: nais.io/v1 kind: Naisjob metadata: labels: team: myteam name: myjob namespace: myteam spec: image: ghcr.io/navikt/myapp:mytag schedule: \"*/1 * * * *\" Behind the scenes, when you make a Naisjob , Kubernetes makes a bunch of different resources for you. Naisjob is one resource that has been defined by the NAIS team. When you deploy a Naisjob with spec.schedule , Kubernetes will create a CronJob for you. A Cronjob is a Kubernetes native resource. When the Cronjob - schedule triggers, Kubernetes will create a Job -ressource, which creates a Pod -resource. The Pod is what actually runs your code. If you make a Naisjob without schedule , Kubernetes will create a Job -resource directly, which will create a Pod -resource. Below you can see some of the resources the Naisjob tiltak-okonomi-avstemming creates. \u279c k tree naisjob tiltak-okonomi-avstemming NAMESPACE NAME READY REASON AGE arbeidsgiver Naisjob/tiltak-okonomi-avstemming - 109d arbeidsgiver \u251c\u2500Job/tiltak-okonomi-avstemming - 109d arbeidsgiver \u2502 \u2514\u2500Pod/tiltak-okonomi-avstemming-brb7c True 12d","title":"Naisjob"},{"location":"naisjob/#re-run-naisjob","text":"If you don't want to run your job at a schedule, but still want to re-run your Naisjob, you either have to delete the old Naisjob first, og run under a different name.","title":"Re-run Naisjob"},{"location":"naisjob/#applying-your-naisjob-to-kubernetes","text":"You can deploy your Naisjob just as you would deploy your Application using NAIS deploy .","title":"Applying your Naisjob to Kubernetes"},{"location":"naisjob/#shutting-down-extra-containers","text":"Due to the nature of NAIS, your Naisjob might end up having a few sidecars. Since a sidecar typically has little to no logic to look at sibling containers, these would normally not turn off once your app completes. As a result, the Naisjob would continue running indefinitely unless some entity from above sorts it out. To remediate this, an operator named Ginuudan has been created to run in every cluster and observe every Naisjob. Once the \"main\" container in a Naisjob completes, Ginuudan will shut down the surrounding sidecars and thus make the Naisjob run to completion. You can look at what Ginuudan has done to complete your Naisjob by running kubectl describe pod my-naisjob-3f91a0 and viewing the Events section. My Naisjob hasn't completed If you see no events from Ginuudan in your Naisjob, you might want to redeploy it. Naisjobs deployed before Ginuudan was published will not be observed by Ginuudan. If you do see events from Ginuudan but your Naisjob didn't complete, we'd be happy if you could tell us in the #nais channel on Slack. My Naisjob deployment is old, and I want to manually shut down the sidecars to make the Naisjob complete Here's some legacy documentation on how to shut down most sidecars.","title":"Shutting down extra containers"},{"location":"naisjob/#linkerd","text":"Linkerd exposes an endpoint to shut itself down. curl -X POST http://127.0.0.1:4191/shutdown This is done from inside your pod-container.","title":"Linkerd"},{"location":"naisjob/#cloudsql-proxy-sidecar","text":"CloudSQL-proxy sidecar does not support turning off remotely. It can be shut down by running exec into the container. kubectl exec yourpod-12345 -c cloudsql-proxy -- kill -s INT 1","title":"CloudSQL-proxy sidecar"},{"location":"naisjob/#securelogs","text":"Securelogs runs on Fluentd, and Fluentd exposes an endpoint to shut itself down. curl http://127.0.0.1:24444/api/processes.killWorkers This is done from inside your pod-container. Additionally, Securelogs runs a second sidecar called secure-logs-configmap-reload . This can be shut down by running exec into the container. kubectl exec yourpod-12345 -c secure-logs-configmap-reload -- /bin/killall configmap-reload","title":"Securelogs"},{"location":"naisjob/#vault-sidecar","text":"Vault sidecar does not support turning off remotely. It can be shut down by running exec into the container. kubectl exec yourpod-12345 -c vks-sidecar -- /bin/kill -s INT 1","title":"Vault sidecar"},{"location":"naisjob/#troubleshooting-faq","text":"","title":"Troubleshooting / FAQ"},{"location":"naisjob/#my-naisjob-doesnt-want-to-start-any-scheduled-jobs","text":"Answer Sometimes, if the defined schedule attempts to start Jobs frequently without succeeding, Kubernetes will eventually stop attempting to start the Job. You might see the following event or message for the related CronJob ( kubectl describe cronjob <name> ): Cannot determine if job needs to be started: too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew Currently, the only way to fix this is to delete the CronJob and NaisJob, and then re-apply or -deploy the NaisJob: kubectl delete cronjob <name> kubectl delete naisjob <name> kubectl apply -f <path-to-naisjob.yaml>","title":"My NaisJob doesn't want to start any scheduled Jobs"},{"location":"naisjob/#how-to-i-force-run-my-scheduled-job","text":"Answer Run the following command to create a ad-hoc job based on your cronjob: kubectl create job --name = <newname> --from = cronjobs/<naisjobname>","title":"How to I force run my scheduled job"},{"location":"naisjob/example/","text":"NAIS Job example YAML \u00a7 This is a complete example of an Naisjob resource. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1 kind : Naisjob metadata : creationTimestamp : null labels : team : myteam name : myjob namespace : myteam spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 activeDeadlineSeconds : 60 azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no backoffLimit : 5 command : - /app/myapplication - --param - value - --other-param - other-value completions : 1 concurrencyPolicy : Allow elastic : access : readwrite instance : my-elastic-instance env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs failedJobsHistoryLimit : 2 filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 image : navikt/testapp:69.0.0 influx : instance : influx-instance kafka : pool : nav-dev streams : true liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 logformat : accesslog_with_referer_useragent logtransform : http_loglevel maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch : access : readwrite instance : my-open-search-instance parallelism : 1 preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi restartPolicy : Never schedule : '*/15 0 0 0 0' secureLogs : enabled : true skipCaBundle : true startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 successfulJobsHistoryLimit : 2 ttlSecondsAfterFinished : 60 vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true webproxy : true status : {}","title":"Full example"},{"location":"naisjob/example/#nais-job-example-yaml","text":"This is a complete example of an Naisjob resource. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1 kind : Naisjob metadata : creationTimestamp : null labels : team : myteam name : myjob namespace : myteam spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 activeDeadlineSeconds : 60 azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no backoffLimit : 5 command : - /app/myapplication - --param - value - --other-param - other-value completions : 1 concurrencyPolicy : Allow elastic : access : readwrite instance : my-elastic-instance env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs failedJobsHistoryLimit : 2 filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 image : navikt/testapp:69.0.0 influx : instance : influx-instance kafka : pool : nav-dev streams : true liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 logformat : accesslog_with_referer_useragent logtransform : http_loglevel maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch : access : readwrite instance : my-open-search-instance parallelism : 1 preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi restartPolicy : Never schedule : '*/15 0 0 0 0' secureLogs : enabled : true skipCaBundle : true startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 successfulJobsHistoryLimit : 2 ttlSecondsAfterFinished : 60 vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true webproxy : true status : {}","title":"NAIS Job example YAML"},{"location":"naisjob/reference/","text":"NAIS Job reference \u00a7 This document describes all possible configuration values in the Naisjob spec. accessPolicy \u00a7 By default, no traffic is allowed between naisjobs inside the cluster. Configure access policies to explicitly allow communication between naisjobs. This is also used for granting inbound access in the context of Azure AD and TokenX clients. Relevant information: https://doc.nais.io/appendix/zero-trust/ https://doc.nais.io/security/auth/azure-ad/access-policy https://doc.nais.io/security/auth/tokenx/#access-policies Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.inbound \u00a7 Configures inbound access for your application. Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules \u00a7 List of NAIS applications that may access your application. These settings apply both to Zero Trust network connectivity and token validity for Azure AD and TokenX tokens. Type: array Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].application \u00a7 The application's name. Type: string Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].cluster \u00a7 The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].namespace \u00a7 The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions \u00a7 Permissions contains a set of permissions that are granted to the given application. Currently only applicable for Azure AD clients. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#fine-grained-access-control Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions.roles \u00a7 Roles is a set of custom permission roles that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-roles Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.inbound.rules[].permissions.scopes \u00a7 Scopes is a set of custom permission scopes that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-scopes Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope accessPolicy.outbound \u00a7 Configures outbound access for your application. Type: object Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.external \u00a7 List of external resources that your applications should be able to reach. Type: array Required: false Availability: GCP Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].host \u00a7 The host that your application should be able to reach, i.e. without the protocol (e.g. https:// ). Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports \u00a7 List of port rules for external communication. Must be specified if using protocols other than HTTPS. Type: array Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].name \u00a7 Human-readable identifier for this rule. Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].port \u00a7 The port used for communication. Type: integer Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.external[].ports[].protocol \u00a7 The protocol used for communication. Type: enum Required: true Allowed values: GRPC , HTTP , HTTP2 , HTTPS , MONGO , TCP , TLS Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP accessPolicy.outbound.rules \u00a7 List of NAIS applications that your application needs to access. These settings apply to Zero Trust network connectivity. Type: array Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].application \u00a7 The application's name. Type: string Required: true Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].cluster \u00a7 The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 accessPolicy.outbound.rules[].namespace \u00a7 The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 activeDeadlineSeconds \u00a7 Once a Naisjob reaches activeDeadlineSeconds, all of its running Pods are terminated and the Naisjob status will become type: Failed with reason: DeadlineExceeded. If set, this takes presedence over BackoffLimit. Type: integer Required: false Example spec : activeDeadlineSeconds : 60 azure \u00a7 Provisions and configures Azure resources. Type: object Required: false Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no azure.application \u00a7 Configures an Azure AD client for this application. Relevant information: https://doc.nais.io/security/auth/azure-ad/ Type: object Required: true Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no azure.application.allowAllUsers \u00a7 AllowAllUsers denotes whether or not all users within the tenant should be allowed to access this AzureAdApplication. If undefined will default to true when Spec.Claims.Groups is undefined, and false if Spec,Claims.Groups is defined. Type: boolean Required: false Example spec : azure : application : allowAllUsers : true azure.application.claims \u00a7 Claims defines additional configuration of the emitted claims in tokens returned to the Azure AD application. Type: object Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.claims.extra \u00a7 Extra is a list of additional claims to be mapped from an associated claim-mapping policy. Currently, the only supported values are NAVident and azp_name . Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#extra Type: array Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name azure.application.claims.groups \u00a7 Groups is a list of Azure AD group IDs to be emitted in the 'Groups' claim. This also restricts access to only contain users of the defined groups unless overridden by Spec.AllowAllUsers. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#groups Type: array Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.claims.groups[].id \u00a7 ID is the actual object ID associated with the given group in Azure AD. Type: string Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000 azure.application.enabled \u00a7 Whether to enable provisioning of an Azure AD application. If enabled, an Azure AD application will be provisioned. Type: boolean Required: true Default value: false Example spec : azure : application : enabled : true azure.application.replyURLs \u00a7 ReplyURLs is a list of allowed redirect URLs used when performing OpenID Connect flows for authenticating end-users. Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#reply-urls Type: array Required: false Example spec : azure : application : replyURLs : - https://myapplication.nav.no/oauth2/callback azure.application.singlePageApplication \u00a7 SinglePageApplication denotes whether or not this Azure AD application should be registered as a single-page-application. Type: boolean Required: false Example spec : azure : application : singlePageApplication : true azure.application.tenant \u00a7 A Tenant represents an organization in Azure AD. If unspecified, will default to trygdeetaten.no for development clusters and nav.no for production clusters. Relevant information: https://doc.nais.io/security/auth/azure-ad/concepts#tenants Type: enum Required: false Allowed values: nav.no , trygdeetaten.no Example spec : azure : application : tenant : nav.no backoffLimit \u00a7 Specify the number of retries before considering a Naisjob as failed Type: integer Required: false Default value: 6 Example spec : backoffLimit : 5 command \u00a7 Override command when starting Docker image. Type: array Required: false Example spec : command : - /app/myapplication - --param - value - --other-param - other-value completions \u00a7 A Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Relevant information: https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-patterns Type: integer Required: false Default value: 1 Example spec : completions : 1 concurrencyPolicy \u00a7 Specifies how to treat concurrent executions of a job that is created by this Naisjob-cron. Relevant information: https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#concurrency-policy Type: enum Required: false Default value: Allow Allowed values: Allow , Forbid , Replace Example spec : concurrencyPolicy : Allow elastic \u00a7 To get your own Elastic Search instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository Type: object Required: false Example spec : elastic : access : readwrite instance : my-elastic-instance elastic.access \u00a7 Access level for elastic user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : elastic : access : readwrite elastic.instance \u00a7 Provisions an Elasticsearch instance and configures your application so it can access it. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : elastic : instance : my-elastic-instance env \u00a7 Custom environment variables injected into your container. Specify either value or valueFrom , but not both. Type: array Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].name \u00a7 Environment variable name. May only contain letters, digits, and the underscore _ character. Type: string Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].value \u00a7 Environment variable value. Numbers and boolean values must be quoted. Required unless valueFrom is specified. Type: string Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom \u00a7 Dynamically set environment variables based on fields found in the Pod spec. Relevant information: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ Type: object Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom.fieldRef \u00a7 Type: object Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name env[].valueFrom.fieldRef.fieldPath \u00a7 Field value from the Pod spec that should be copied into the environment variable. Type: enum Required: true Allowed values: (empty string) , metadata.annotations , metadata.labels , metadata.name , metadata.namespace , spec.nodeName , spec.serviceAccountName , status.hostIP , status.podIP Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name envFrom \u00a7 EnvFrom exposes all variables in the ConfigMap or Secret resources as environment variables. One of configMap or secret is required. Environment variables will take the form KEY=VALUE , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Naisjob resource. Type: array Required: false Availability: team namespaces Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs envFrom[].configmap \u00a7 Name of the ConfigMap where environment variables are specified. Required unless secret is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs envFrom[].secret \u00a7 Name of the Secret where environment variables are specified. Required unless configMap is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs failedJobsHistoryLimit \u00a7 Specify how many failed Jobs should be kept. Type: integer Required: false Default value: 1 Example spec : failedJobsHistoryLimit : 2 filesFrom \u00a7 List of ConfigMap or Secret resources that will have their contents mounted into the containers as files. Either configMap or secret is required. Files will take the path <mountPath>/<key> , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret, and they will all be mounted to the same directory. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Naisjob resource. Type: array Required: false Availability: team namespaces Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].configmap \u00a7 Name of the ConfigMap that contains files that should be mounted into the container. Required unless secret or persistentVolumeClaim is set. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].mountPath \u00a7 Filesystem path inside the pod where files are mounted. The directory will be created if it does not exist. If the directory exists, any files in the directory will be made unaccessible. Defaults to /var/run/configmaps/<NAME> , /var/run/secrets , or /var/run/pvc/<NAME> , depending on which of them is specified. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].persistentVolumeClaim \u00a7 Name of the PersistentVolumeClaim that should be mounted into the container. Required unless configMap or secret is set. This feature requires coordination with the NAIS team. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name filesFrom[].secret \u00a7 Name of the Secret that contains files that should be mounted into the container. Required unless configMap or persistentVolumeClaim is set. If mounting multiple secrets, mountPath MUST be set to avoid collisions. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name gcp \u00a7 Type: object Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.bigQueryDatasets \u00a7 Provision BigQuery datasets and give your application's pod mountable secrets for connecting to each dataset. Datasets are immutable and cannot be changed. Relevant information: https://cloud.google.com/bigquery/docs Type: array Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].cascadingDelete \u00a7 When set to true will delete the dataset, when the application resource is deleted. NB: If no tables exist in the bigquery dataset, it will delete the dataset even if this value is set/defaulted to false . Default value is false . Type: boolean Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].description \u00a7 Human-readable description of what this BigQuery dataset contains, or is used for. Will be visible in the GCP Console. Type: string Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].name \u00a7 Name of the BigQuery Dataset. The canonical name of the dataset will be <TEAM_PROJECT_ID>:<NAME> . Type: string Required: true Immutable: true Pattern: ^[a-z0-9][a-z0-9_]+$ Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.bigQueryDatasets[].permission \u00a7 Permission level given to application. Type: enum Required: true Immutable: true Allowed values: READ , READWRITE Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ gcp.buckets \u00a7 Provision cloud storage buckets and connect them to your application. Relevant information: https://doc.nais.io/persistence/buckets/ Type: array Required: false Availability: GCP Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].cascadingDelete \u00a7 Allows deletion of bucket. Set to true if you want to delete the bucket. Type: boolean Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition \u00a7 Conditions for the bucket to use when selecting objects to delete in cleanup. Relevant information: https://cloud.google.com/storage/docs/lifecycle Type: object Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.age \u00a7 Condition is satisfied when the object reaches the specified age in days. These will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.createdBefore \u00a7 Condition is satisfied when the object is created before midnight on the specified date. These will be deleted. Type: string Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.numNewerVersions \u00a7 Condition is satisfied when the object has the specified number of newer versions. The older versions will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].lifecycleCondition.withState \u00a7 Condition is satisfied when the object has the specified state. Type: enum Required: false Allowed values: (empty string) , ANY , ARCHIVED , LIVE Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].name \u00a7 The name of the bucket Type: string Required: true Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].retentionPeriodDays \u00a7 The number of days to hold objects in the bucket before it is allowed to delete them. Type: integer Required: false Value range: 1 - 36500 Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.buckets[].uniformBucketLevelAccess \u00a7 Allows you to uniformly control access to your Cloud Storage resources. When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled, and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains. Uniform access control can not be reversed after 90 days! This is controlled by Google. Relevant information: https://cloud.google.com/storage/docs/uniform-bucket-level-access Type: boolean Required: false Default value: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true gcp.permissions \u00a7 List of additional permissions that should be granted to your application for accessing external GCP resources that have not been provisioned through NAIS. Relevant information: https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember#external_organization_level_policy_member Type: array Required: false Availability: GCP Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource \u00a7 IAM resource to bind the role to. Type: object Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.apiVersion \u00a7 Kubernetes APIVersion . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.kind \u00a7 Kubernetes Kind . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].resource.name \u00a7 Kubernetes Name . Type: string Required: false Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.permissions[].role \u00a7 Name of the GCP role to bind the resource to. Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client gcp.sqlInstances \u00a7 Provision database instances and connect them to your application. Relevant information: https://doc.nais.io/persistence/postgres/ https://cloud.google.com/sql/docs/postgres/instance-settings#impact Type: array Required: false Availability: GCP Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].autoBackupHour \u00a7 If specified, run automatic backups of the SQL database at the given hour. Note that this will backup the whole SQL instance, and not separate databases. Restores are done using the Google Cloud Console. Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].cascadingDelete \u00a7 Remove the entire Postgres server including all data when the Kubernetes resource is deleted. THIS IS A DESTRUCTIVE OPERATION ! Set cascading delete only when you want to remove data forever. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].collation \u00a7 Sort order for ORDER BY ... clauses. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases \u00a7 List of databases that should be created on this Postgres server. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].envVarPrefix \u00a7 Prefix to add to environment variables made available for database connection. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].name \u00a7 Database name. Type: string Required: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].users \u00a7 Add extra users for database access. These users need to be manually given access to database tables. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].databases[].users[].name \u00a7 User name. Type: string Required: true Pattern: ^[_a-zA-Z][-_a-zA-Z0-9]+$ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskAutoresize \u00a7 When set to true, GCP will automatically increase storage by XXX for the database when disk usage is above the high water mark. Relevant information: https://cloud.google.com/sql/docs/postgres/instance-settings#threshold Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskSize \u00a7 How much hard drive space to allocate for the SQL server, in gigabytes. Type: integer Required: false Minimum value: 10 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].diskType \u00a7 Disk type to use for storage in the database. Type: enum Required: false Allowed values: HDD , SSD Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].highAvailability \u00a7 When set to true this will set up standby database for failover. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights \u00a7 Configures query insights which are now default for new sql instances. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.enabled \u00a7 True if Query Insights feature is enabled. Type: boolean Required: false Default value: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.queryStringLength \u00a7 Maximum query length stored in bytes. Between 256 and 4500. Default to 1024. Type: integer Required: false Value range: 256 - 4500 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.recordApplicationTags \u00a7 True if Query Insights will record application tags from query when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].insights.recordClientAddress \u00a7 True if Query Insights will record client address when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance \u00a7 Desired maintenance window for database updates. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance.day \u00a7 Type: integer Required: false Value range: 1 - 7 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].maintenance.hour \u00a7 Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].name \u00a7 The name of the instance, if omitted the application name will be used. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].pointInTimeRecovery \u00a7 Enables point-in-time recovery for sql instances using write-ahead logs. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].tier \u00a7 Server tier, i.e. how much CPU and memory allocated. Available tiers are db-f1-micro , db-g1-small and custom db-custom-CPU-RAM . Custom memory must be mulitple of 256 MB and at least 3.75 GB (e.g. db-custom-1-3840 for 1 cpu, 3840 MB ram) Also check out sizing your database . Type: string Required: false Default value: db-f1-micro Pattern: db-.+ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 gcp.sqlInstances[].type \u00a7 PostgreSQL version. Type: enum Required: true Allowed values: POSTGRES_11 , POSTGRES_12 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12 image \u00a7 Your Naisjob's Docker image location and tag. Type: string Required: true Example spec : image : navikt/testapp:69.0.0 influx \u00a7 An Influxdb via Aiven. A typical use case is to store metrics from your application and visualize them in Grafana. See navikt/aiven-iac repository Type: object Required: false Availability: GCP Example spec : influx : instance : influx-instance influx.instance \u00a7 Provisions an InfluxDB instance and configures your application to access it. Use the prefix: influx- + team that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : influx : instance : influx-instance kafka \u00a7 Enable Aiven Kafka for your Naisjob. Type: object Required: false Example spec : kafka : pool : nav-dev streams : true kafka.pool \u00a7 Configures your application to access an Aiven Kafka cluster. Type: enum Required: true Allowed values: nav-dev , nav-infrastructure , nav-prod Example spec : kafka : pool : nav-dev kafka.streams \u00a7 Allow this app to use kafka streams Relevant information: https://doc.nais.io/persistence/kafka/application/#using-kafka-streams-with-internal-topics Type: boolean Required: false Default value: false Availability: GCP Example spec : kafka : streams : true liveness \u00a7 Many Naisjobs running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations. Read more about this over at the Kubernetes probes documentation . Type: object Required: false Example spec : liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1 liveness.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Default value: 3 Example spec : liveness : failureThreshold : 10 liveness.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : liveness : initialDelay : 20 liveness.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : liveness : path : /isalive liveness.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Default value: 10 Example spec : liveness : periodSeconds : 5 liveness.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : liveness : port : 8080 liveness.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Default value: 1 Example spec : liveness : timeout : 1 logformat \u00a7 Format of the logs from the container. Use this if the container doesn't support JSON logging and the log is in a special format that need to be parsed. Type: enum Required: false Allowed values: (empty string) , accesslog , accesslog_with_processing_time , accesslog_with_referer_useragent , capnslog , glog , gokit , influxdb , log15 , logrus , redis , simple Example spec : logformat : accesslog_with_referer_useragent logtransform \u00a7 Extra filters for modifying log content. This can e.g. be used for setting loglevel based on http status code. Type: enum Required: false Allowed values: dns_loglevel , http_loglevel Example spec : logtransform : http_loglevel maskinporten \u00a7 Configures a Maskinporten client for this Naisjob. See Maskinporten for more details. Type: object Required: false Example spec : maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.enabled \u00a7 If enabled, provisions and configures a Maskinporten client with consumed scopes and/or Exposed scopes with DigDir. Type: boolean Required: true Default value: false Availability: team namespaces Example spec : maskinporten : enabled : true maskinporten.scopes \u00a7 Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes. Type: object Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.consumes \u00a7 This is the Schema for the consumes and exposes API. consumes is a list of scopes that your client can request access to. Type: array Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read maskinporten.scopes.consumes[].name \u00a7 The scope consumed by the application to gain access to an external organization API. Ensure that the NAV organization has been granted access to the scope prior to requesting access. Relevant information: https://doc.nais.io/security/auth/maskinporten/#consume-scopes Type: string Required: true Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read maskinporten.scopes.exposes \u00a7 exposes is a list of scopes your application want to expose to other organization where access to the scope is based on organization number. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].allowedIntegrations \u00a7 Whitelisting of integration's allowed. Default is maskinporten Relevant information: https://docs.digdir.no/maskinporten_guide_apitilbyder.html#scope-begrensninger Type: array Required: false Default value: maskinporten Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].atMaxAge \u00a7 Max time in seconds for a issued access_token. Default is 30 sec. Type: integer Required: false Default value: 30 Value range: 30 - 680 Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers \u00a7 External consumers granted access to this scope and able to request access_token. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers[].name \u00a7 This is a describing field intended for clarity not used for any other purpose. Type: string Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].consumers[].orgno \u00a7 The external business/organization number. Type: string Required: true Pattern: ^\\d{9}$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].enabled \u00a7 If Enabled the configured scope is available to be used and consumed by organizations granted access. Relevant information: https://doc.nais.io/naisjob/reference/#maskinportenscopesexposesconsumers Type: boolean Required: true Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].name \u00a7 The actual subscope combined with Product . Ensure that <Product><Name> matches Pattern . Type: string Required: true Default value: false Pattern: ^([a-z\u00e6\u00f8\u00e50-9]+\\/?)+(\\:[a-z\u00e6\u00f8\u00e50-9]+)*[a-z\u00e6\u00f8\u00e50-9]+(\\.[a-z\u00e6\u00f8\u00e50-9]+)*$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid maskinporten.scopes.exposes[].product \u00a7 The product-area your application belongs to e.g. arbeid, helse ... This will be included in the final scope nav:<Product><Name> . Type: string Required: true Pattern: ^[a-z0-9]+$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid openSearch \u00a7 To get your own OpenSearch instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : openSearch : access : readwrite instance : my-open-search-instance openSearch.access \u00a7 Access level for OpenSearch user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : openSearch : access : readwrite openSearch.instance \u00a7 Configure your application to access your OpenSearch instance. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : openSearch : instance : my-open-search-instance parallelism \u00a7 For running pods in parallel. If it is specified as 0, then the Job is effectively paused until it is increased. Relevant information: https://kubernetes.io/docs/concepts/workloads/controllers/job/#controlling-parallelism Type: integer Required: false Default value: 1 Example spec : parallelism : 1 preStopHook \u00a7 PreStopHook is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits by itself. The reason for termination is passed to the handler. Relevant information: https://doc.nais.io/naisjob/#handles-termination-gracefully https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080 preStopHook.exec \u00a7 Command that should be run inside the main container just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script preStopHook.exec.command \u00a7 Command is the command line to execute inside the container before the pod is shut down. The command is not run inside a shell, so traditional shell instructions (pipes, redirects, etc.) won't work. To use a shell, you need to explicitly call out to that shell. If the exit status is non-zero, the pod will still be shut down, and marked as Failed . Type: array Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script preStopHook.http \u00a7 HTTP GET request that is called just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : http : path : /internal/stop port : 8080 preStopHook.http.path \u00a7 Path to access on the HTTP server. Type: string Required: true Example spec : preStopHook : http : path : /internal/stop preStopHook.http.port \u00a7 Port to access on the container. Defaults to application port, as defined in .spec.port . Type: integer Required: false Value range: 1 - 65535 Example spec : preStopHook : http : port : 8080 readiness \u00a7 Sometimes, Naisjobs are temporarily unable to serve traffic. For example, an Naisjob might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the Naisjob, but you don\u2019t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services. Read more about this over at the Kubernetes readiness documentation . Type: object Required: false Example spec : readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1 readiness.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : readiness : failureThreshold : 10 readiness.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : readiness : initialDelay : 20 readiness.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : readiness : path : /isready readiness.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Example spec : readiness : periodSeconds : 5 readiness.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : readiness : port : 8080 readiness.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Example spec : readiness : timeout : 1 resources \u00a7 When Containers have resource requests specified, the Kubernetes scheduler can make better decisions about which nodes to place pods on. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi resources.limits \u00a7 Limit defines the maximum amount of resources a container can use before getting evicted. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi resources.limits.cpu \u00a7 Type: string Required: false Default value: 500m Pattern: ^\\d+m?$ Example spec : resources : limits : cpu : 500m resources.limits.memory \u00a7 Type: string Required: false Default value: 512Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : limits : memory : 512Mi resources.requests \u00a7 Request defines the amount of resources a container is allocated on startup. Type: object Required: false Example spec : resources : requests : cpu : 200m memory : 256Mi resources.requests.cpu \u00a7 Type: string Required: false Default value: 200m Pattern: ^\\d+m?$ Example spec : resources : requests : cpu : 200m resources.requests.memory \u00a7 Type: string Required: false Default value: 256Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : requests : memory : 256Mi restartPolicy \u00a7 RestartPolicy describes how the container should be restarted. Only one of the following restart policies may be specified. If none of the following policies is specified, the default one is Never. Read more about Kubernetes handling pod and container failures Type: enum Required: false Default value: Never Allowed values: Never , OnFailure Example spec : restartPolicy : Never schedule \u00a7 The Cron schedule for running the Naisjob. If not specified, the Naisjob will be run as a one-shot Job. Type: string Required: false Example spec : schedule : '*/15 0 0 0 0' secureLogs \u00a7 Whether or not to enable a sidecar container for secure logging. Type: object Required: false Example spec : secureLogs : enabled : true secureLogs.enabled \u00a7 Whether to enable a sidecar container for secure logging. If enabled, a volume is mounted in the pods where secure logs can be saved. Type: boolean Required: true Default value: false Example spec : secureLogs : enabled : true skipCaBundle \u00a7 Whether to skip injection of NAV certificate authority bundle or not. Defaults to false. Type: boolean Required: false Example spec : skipCaBundle : true startup \u00a7 Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Type: object Required: false Example spec : startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1 startup.failureThreshold \u00a7 When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : startup : failureThreshold : 10 startup.initialDelay \u00a7 Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : startup : initialDelay : 20 startup.path \u00a7 HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : startup : path : /started startup.periodSeconds \u00a7 How often (in seconds) to perform the probe. Type: integer Required: false Example spec : startup : periodSeconds : 5 startup.port \u00a7 Port for the startup probe. Type: integer Required: false Example spec : startup : port : 8080 startup.timeout \u00a7 Number of seconds after which the probe times out. Type: integer Required: false Example spec : startup : timeout : 1 successfulJobsHistoryLimit \u00a7 Specify how many completed Jobs should be kept. Type: integer Required: false Default value: 3 Example spec : successfulJobsHistoryLimit : 2 ttlSecondsAfterFinished \u00a7 Specify the number of seconds to wait before removing the Job after it has finished (either Completed or Failed). If the field is unset, this Job won't be cleaned up by the TTL controller after it finishes. Type: integer Required: false Availability: on-premises Example spec : ttlSecondsAfterFinished : 60 vault \u00a7 Provides secrets management, identity-based access, and encrypting application data for auditing of secrets for applications, systems, and users. Relevant information: https://github.com/navikt/vault-iac/tree/master/doc Type: object Required: false Availability: on-premises Example spec : vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true vault.enabled \u00a7 If set to true, fetch secrets from Vault and inject into the pods. Type: boolean Required: false Example spec : vault : enabled : true vault.paths \u00a7 List of secret paths to be read from Vault and injected into the pod's filesystem. Overriding the paths array is optional, and will give you fine-grained control over which Vault paths that will be mounted on the file system. By default, the list will contain an entry with kvPath: /kv/<environment>/<zone>/<application>/<namespace> mountPath: /var/run/secrets/nais.io/vault that will always be attempted to be mounted. Type: array Required: false Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].format \u00a7 Format of the secret that should be processed. Type: enum Required: false Allowed values: (empty string) , env , flatten , json , properties , yaml Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].kvPath \u00a7 Path to Vault key/value store that should be mounted into the file system. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.paths[].mountPath \u00a7 File system path that the secret will be mounted into. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault vault.sidecar \u00a7 If enabled, the sidecar will automatically refresh the token's Time-To-Live before it expires. Type: boolean Required: false Example spec : vault : sidecar : true webproxy \u00a7 Inject on-premises web proxy configuration into the job container. Most Linux applications should auto-detect these settings from the $HTTP_PROXY , $HTTPS_PROXY and $NO_PROXY environment variables (and their lowercase counterparts). Java applications can start the JVM using parameters from the $JAVA_PROXY_OPTIONS environment variable. Type: boolean Required: false Availability: on-premises Example spec : webproxy : true","title":"Reference"},{"location":"naisjob/reference/#nais-job-reference","text":"This document describes all possible configuration values in the Naisjob spec.","title":"NAIS Job reference"},{"location":"naisjob/reference/#accesspolicy","text":"By default, no traffic is allowed between naisjobs inside the cluster. Configure access policies to explicitly allow communication between naisjobs. This is also used for granting inbound access in the context of Azure AD and TokenX clients. Relevant information: https://doc.nais.io/appendix/zero-trust/ https://doc.nais.io/security/auth/azure-ad/access-policy https://doc.nais.io/security/auth/tokenx/#access-policies Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy"},{"location":"naisjob/reference/#accesspolicyinbound","text":"Configures inbound access for your application. Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound"},{"location":"naisjob/reference/#accesspolicyinboundrules","text":"List of NAIS applications that may access your application. These settings apply both to Zero Trust network connectivity and token validity for Azure AD and TokenX tokens. Type: array Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules"},{"location":"naisjob/reference/#accesspolicyinboundrulesapplication","text":"The application's name. Type: string Required: true Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].application"},{"location":"naisjob/reference/#accesspolicyinboundrulescluster","text":"The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].cluster"},{"location":"naisjob/reference/#accesspolicyinboundrulesnamespace","text":"The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].namespace"},{"location":"naisjob/reference/#accesspolicyinboundrulespermissions","text":"Permissions contains a set of permissions that are granted to the given application. Currently only applicable for Azure AD clients. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#fine-grained-access-control Type: object Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions"},{"location":"naisjob/reference/#accesspolicyinboundrulespermissionsroles","text":"Roles is a set of custom permission roles that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-roles Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions.roles"},{"location":"naisjob/reference/#accesspolicyinboundrulespermissionsscopes","text":"Scopes is a set of custom permission scopes that are granted to a given application. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#custom-scopes Type: array Required: false Example spec : accessPolicy : inbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3 - application : app4 permissions : scopes : - custom-scope - application : app5 permissions : roles : - custom-role - application : app6 permissions : roles : - custom-role scopes : - custom-scope","title":"accessPolicy.inbound.rules[].permissions.scopes"},{"location":"naisjob/reference/#accesspolicyoutbound","text":"Configures outbound access for your application. Type: object Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound"},{"location":"naisjob/reference/#accesspolicyoutboundexternal","text":"List of external resources that your applications should be able to reach. Type: array Required: false Availability: GCP Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external"},{"location":"naisjob/reference/#accesspolicyoutboundexternalhost","text":"The host that your application should be able to reach, i.e. without the protocol (e.g. https:// ). Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].host"},{"location":"naisjob/reference/#accesspolicyoutboundexternalports","text":"List of port rules for external communication. Must be specified if using protocols other than HTTPS. Type: array Required: false Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports"},{"location":"naisjob/reference/#accesspolicyoutboundexternalportsname","text":"Human-readable identifier for this rule. Type: string Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].name"},{"location":"naisjob/reference/#accesspolicyoutboundexternalportsport","text":"The port used for communication. Type: integer Required: true Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].port"},{"location":"naisjob/reference/#accesspolicyoutboundexternalportsprotocol","text":"The protocol used for communication. Type: enum Required: true Allowed values: GRPC , HTTP , HTTP2 , HTTPS , MONGO , TCP , TLS Example spec : accessPolicy : outbound : external : - host : external-application.example.com - host : non-http-service.example.com ports : - name : kafka port : 9200 protocol : TCP","title":"accessPolicy.outbound.external[].ports[].protocol"},{"location":"naisjob/reference/#accesspolicyoutboundrules","text":"List of NAIS applications that your application needs to access. These settings apply to Zero Trust network connectivity. Type: array Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules"},{"location":"naisjob/reference/#accesspolicyoutboundrulesapplication","text":"The application's name. Type: string Required: true Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].application"},{"location":"naisjob/reference/#accesspolicyoutboundrulescluster","text":"The application's cluster. May be omitted if it should be in the same cluster as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].cluster"},{"location":"naisjob/reference/#accesspolicyoutboundrulesnamespace","text":"The application's namespace. May be omitted if it should be in the same namespace as your application. Type: string Required: false Example spec : accessPolicy : outbound : rules : - application : app1 - application : app2 namespace : q1 - application : app3 cluster : dev-gcp namespace : q2 - application : '*' namespace : q3","title":"accessPolicy.outbound.rules[].namespace"},{"location":"naisjob/reference/#activedeadlineseconds","text":"Once a Naisjob reaches activeDeadlineSeconds, all of its running Pods are terminated and the Naisjob status will become type: Failed with reason: DeadlineExceeded. If set, this takes presedence over BackoffLimit. Type: integer Required: false Example spec : activeDeadlineSeconds : 60","title":"activeDeadlineSeconds"},{"location":"naisjob/reference/#azure","text":"Provisions and configures Azure resources. Type: object Required: false Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no","title":"azure"},{"location":"naisjob/reference/#azureapplication","text":"Configures an Azure AD client for this application. Relevant information: https://doc.nais.io/security/auth/azure-ad/ Type: object Required: true Example spec : azure : application : allowAllUsers : true claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000 enabled : true replyURLs : - https://myapplication.nav.no/oauth2/callback singlePageApplication : true tenant : nav.no","title":"azure.application"},{"location":"naisjob/reference/#azureapplicationallowallusers","text":"AllowAllUsers denotes whether or not all users within the tenant should be allowed to access this AzureAdApplication. If undefined will default to true when Spec.Claims.Groups is undefined, and false if Spec,Claims.Groups is defined. Type: boolean Required: false Example spec : azure : application : allowAllUsers : true","title":"azure.application.allowAllUsers"},{"location":"naisjob/reference/#azureapplicationclaims","text":"Claims defines additional configuration of the emitted claims in tokens returned to the Azure AD application. Type: object Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims"},{"location":"naisjob/reference/#azureapplicationclaimsextra","text":"Extra is a list of additional claims to be mapped from an associated claim-mapping policy. Currently, the only supported values are NAVident and azp_name . Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#extra Type: array Required: false Example spec : azure : application : claims : extra : - NAVident - azp_name","title":"azure.application.claims.extra"},{"location":"naisjob/reference/#azureapplicationclaimsgroups","text":"Groups is a list of Azure AD group IDs to be emitted in the 'Groups' claim. This also restricts access to only contain users of the defined groups unless overridden by Spec.AllowAllUsers. Relevant information: https://doc.nais.io/security/auth/azure-ad/access-policy#groups Type: array Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims.groups"},{"location":"naisjob/reference/#azureapplicationclaimsgroupsid","text":"ID is the actual object ID associated with the given group in Azure AD. Type: string Required: false Example spec : azure : application : claims : groups : - id : 00000000-0000-0000-0000-000000000000","title":"azure.application.claims.groups[].id"},{"location":"naisjob/reference/#azureapplicationenabled","text":"Whether to enable provisioning of an Azure AD application. If enabled, an Azure AD application will be provisioned. Type: boolean Required: true Default value: false Example spec : azure : application : enabled : true","title":"azure.application.enabled"},{"location":"naisjob/reference/#azureapplicationreplyurls","text":"ReplyURLs is a list of allowed redirect URLs used when performing OpenID Connect flows for authenticating end-users. Relevant information: https://doc.nais.io/security/auth/azure-ad/configuration#reply-urls Type: array Required: false Example spec : azure : application : replyURLs : - https://myapplication.nav.no/oauth2/callback","title":"azure.application.replyURLs"},{"location":"naisjob/reference/#azureapplicationsinglepageapplication","text":"SinglePageApplication denotes whether or not this Azure AD application should be registered as a single-page-application. Type: boolean Required: false Example spec : azure : application : singlePageApplication : true","title":"azure.application.singlePageApplication"},{"location":"naisjob/reference/#azureapplicationtenant","text":"A Tenant represents an organization in Azure AD. If unspecified, will default to trygdeetaten.no for development clusters and nav.no for production clusters. Relevant information: https://doc.nais.io/security/auth/azure-ad/concepts#tenants Type: enum Required: false Allowed values: nav.no , trygdeetaten.no Example spec : azure : application : tenant : nav.no","title":"azure.application.tenant"},{"location":"naisjob/reference/#backofflimit","text":"Specify the number of retries before considering a Naisjob as failed Type: integer Required: false Default value: 6 Example spec : backoffLimit : 5","title":"backoffLimit"},{"location":"naisjob/reference/#command","text":"Override command when starting Docker image. Type: array Required: false Example spec : command : - /app/myapplication - --param - value - --other-param - other-value","title":"command"},{"location":"naisjob/reference/#completions","text":"A Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Relevant information: https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-patterns Type: integer Required: false Default value: 1 Example spec : completions : 1","title":"completions"},{"location":"naisjob/reference/#concurrencypolicy","text":"Specifies how to treat concurrent executions of a job that is created by this Naisjob-cron. Relevant information: https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#concurrency-policy Type: enum Required: false Default value: Allow Allowed values: Allow , Forbid , Replace Example spec : concurrencyPolicy : Allow","title":"concurrencyPolicy"},{"location":"naisjob/reference/#elastic","text":"To get your own Elastic Search instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository Type: object Required: false Example spec : elastic : access : readwrite instance : my-elastic-instance","title":"elastic"},{"location":"naisjob/reference/#elasticaccess","text":"Access level for elastic user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : elastic : access : readwrite","title":"elastic.access"},{"location":"naisjob/reference/#elasticinstance","text":"Provisions an Elasticsearch instance and configures your application so it can access it. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : elastic : instance : my-elastic-instance","title":"elastic.instance"},{"location":"naisjob/reference/#env","text":"Custom environment variables injected into your container. Specify either value or valueFrom , but not both. Type: array Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env"},{"location":"naisjob/reference/#envname","text":"Environment variable name. May only contain letters, digits, and the underscore _ character. Type: string Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].name"},{"location":"naisjob/reference/#envvalue","text":"Environment variable value. Numbers and boolean values must be quoted. Required unless valueFrom is specified. Type: string Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].value"},{"location":"naisjob/reference/#envvaluefrom","text":"Dynamically set environment variables based on fields found in the Pod spec. Relevant information: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ Type: object Required: false Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom"},{"location":"naisjob/reference/#envvaluefromfieldref","text":"Type: object Required: true Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom.fieldRef"},{"location":"naisjob/reference/#envvaluefromfieldreffieldpath","text":"Field value from the Pod spec that should be copied into the environment variable. Type: enum Required: true Allowed values: (empty string) , metadata.annotations , metadata.labels , metadata.name , metadata.namespace , spec.nodeName , spec.serviceAccountName , status.hostIP , status.podIP Example spec : env : - name : MY_CUSTOM_VAR value : some_value - name : MY_APPLICATION_NAME valueFrom : fieldRef : fieldPath : metadata.name","title":"env[].valueFrom.fieldRef.fieldPath"},{"location":"naisjob/reference/#envfrom","text":"EnvFrom exposes all variables in the ConfigMap or Secret resources as environment variables. One of configMap or secret is required. Environment variables will take the form KEY=VALUE , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Naisjob resource. Type: array Required: false Availability: team namespaces Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom"},{"location":"naisjob/reference/#envfromconfigmap","text":"Name of the ConfigMap where environment variables are specified. Required unless secret is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom[].configmap"},{"location":"naisjob/reference/#envfromsecret","text":"Name of the Secret where environment variables are specified. Required unless configMap is set. Type: string Required: false Example spec : envFrom : - secret : my-secret-with-envs - configmap : my-configmap-with-envs","title":"envFrom[].secret"},{"location":"naisjob/reference/#failedjobshistorylimit","text":"Specify how many failed Jobs should be kept. Type: integer Required: false Default value: 1 Example spec : failedJobsHistoryLimit : 2","title":"failedJobsHistoryLimit"},{"location":"naisjob/reference/#filesfrom","text":"List of ConfigMap or Secret resources that will have their contents mounted into the containers as files. Either configMap or secret is required. Files will take the path <mountPath>/<key> , where key is the ConfigMap or Secret key. You can specify as many keys as you like in a single ConfigMap or Secret, and they will all be mounted to the same directory. The ConfigMap and Secret resources must live in the same Kubernetes namespace as the Naisjob resource. Type: array Required: false Availability: team namespaces Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom"},{"location":"naisjob/reference/#filesfromconfigmap","text":"Name of the ConfigMap that contains files that should be mounted into the container. Required unless secret or persistentVolumeClaim is set. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].configmap"},{"location":"naisjob/reference/#filesfrommountpath","text":"Filesystem path inside the pod where files are mounted. The directory will be created if it does not exist. If the directory exists, any files in the directory will be made unaccessible. Defaults to /var/run/configmaps/<NAME> , /var/run/secrets , or /var/run/pvc/<NAME> , depending on which of them is specified. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].mountPath"},{"location":"naisjob/reference/#filesfrompersistentvolumeclaim","text":"Name of the PersistentVolumeClaim that should be mounted into the container. Required unless configMap or secret is set. This feature requires coordination with the NAIS team. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].persistentVolumeClaim"},{"location":"naisjob/reference/#filesfromsecret","text":"Name of the Secret that contains files that should be mounted into the container. Required unless configMap or persistentVolumeClaim is set. If mounting multiple secrets, mountPath MUST be set to avoid collisions. Type: string Required: false Example spec : filesFrom : - configmap : example-files-configmap mountPath : /var/run/configmaps - mountPath : /var/run/secrets secret : my-secret-file - mountPath : /var/run/pvc persistentVolumeClaim : pvc-name","title":"filesFrom[].secret"},{"location":"naisjob/reference/#gcp","text":"Type: object Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp"},{"location":"naisjob/reference/#gcpbigquerydatasets","text":"Provision BigQuery datasets and give your application's pod mountable secrets for connecting to each dataset. Datasets are immutable and cannot be changed. Relevant information: https://cloud.google.com/bigquery/docs Type: array Required: false Availability: GCP Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets"},{"location":"naisjob/reference/#gcpbigquerydatasetscascadingdelete","text":"When set to true will delete the dataset, when the application resource is deleted. NB: If no tables exist in the bigquery dataset, it will delete the dataset even if this value is set/defaulted to false . Default value is false . Type: boolean Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].cascadingDelete"},{"location":"naisjob/reference/#gcpbigquerydatasetsdescription","text":"Human-readable description of what this BigQuery dataset contains, or is used for. Will be visible in the GCP Console. Type: string Required: false Immutable: true Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].description"},{"location":"naisjob/reference/#gcpbigquerydatasetsname","text":"Name of the BigQuery Dataset. The canonical name of the dataset will be <TEAM_PROJECT_ID>:<NAME> . Type: string Required: true Immutable: true Pattern: ^[a-z0-9][a-z0-9_]+$ Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].name"},{"location":"naisjob/reference/#gcpbigquerydatasetspermission","text":"Permission level given to application. Type: enum Required: true Immutable: true Allowed values: READ , READWRITE Example spec : gcp : bigQueryDatasets : - cascadingDelete : true description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset1 permission : READWRITE - description : Contains big data, supporting big queries, for use in big ideas. name : my_bigquery_dataset2 permission : READ","title":"gcp.bigQueryDatasets[].permission"},{"location":"naisjob/reference/#gcpbuckets","text":"Provision cloud storage buckets and connect them to your application. Relevant information: https://doc.nais.io/persistence/buckets/ Type: array Required: false Availability: GCP Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets"},{"location":"naisjob/reference/#gcpbucketscascadingdelete","text":"Allows deletion of bucket. Set to true if you want to delete the bucket. Type: boolean Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].cascadingDelete"},{"location":"naisjob/reference/#gcpbucketslifecyclecondition","text":"Conditions for the bucket to use when selecting objects to delete in cleanup. Relevant information: https://cloud.google.com/storage/docs/lifecycle Type: object Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition"},{"location":"naisjob/reference/#gcpbucketslifecycleconditionage","text":"Condition is satisfied when the object reaches the specified age in days. These will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.age"},{"location":"naisjob/reference/#gcpbucketslifecycleconditioncreatedbefore","text":"Condition is satisfied when the object is created before midnight on the specified date. These will be deleted. Type: string Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.createdBefore"},{"location":"naisjob/reference/#gcpbucketslifecycleconditionnumnewerversions","text":"Condition is satisfied when the object has the specified number of newer versions. The older versions will be deleted. Type: integer Required: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.numNewerVersions"},{"location":"naisjob/reference/#gcpbucketslifecycleconditionwithstate","text":"Condition is satisfied when the object has the specified state. Type: enum Required: false Allowed values: (empty string) , ANY , ARCHIVED , LIVE Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].lifecycleCondition.withState"},{"location":"naisjob/reference/#gcpbucketsname","text":"The name of the bucket Type: string Required: true Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].name"},{"location":"naisjob/reference/#gcpbucketsretentionperioddays","text":"The number of days to hold objects in the bucket before it is allowed to delete them. Type: integer Required: false Value range: 1 - 36500 Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].retentionPeriodDays"},{"location":"naisjob/reference/#gcpbucketsuniformbucketlevelaccess","text":"Allows you to uniformly control access to your Cloud Storage resources. When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled, and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains. Uniform access control can not be reversed after 90 days! This is controlled by Google. Relevant information: https://cloud.google.com/storage/docs/uniform-bucket-level-access Type: boolean Required: false Default value: false Example spec : gcp : buckets : - cascadingDelete : true lifecycleCondition : age : 10 createdBefore : \"2020-01-01\" numNewerVersions : 2 withState : ARCHIVED name : my-cloud-storage-bucket retentionPeriodDays : 30 uniformBucketLevelAccess : true","title":"gcp.buckets[].uniformBucketLevelAccess"},{"location":"naisjob/reference/#gcppermissions","text":"List of additional permissions that should be granted to your application for accessing external GCP resources that have not been provisioned through NAIS. Relevant information: https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember#external_organization_level_policy_member Type: array Required: false Availability: GCP Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions"},{"location":"naisjob/reference/#gcppermissionsresource","text":"IAM resource to bind the role to. Type: object Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource"},{"location":"naisjob/reference/#gcppermissionsresourceapiversion","text":"Kubernetes APIVersion . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.apiVersion"},{"location":"naisjob/reference/#gcppermissionsresourcekind","text":"Kubernetes Kind . Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.kind"},{"location":"naisjob/reference/#gcppermissionsresourcename","text":"Kubernetes Name . Type: string Required: false Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].resource.name"},{"location":"naisjob/reference/#gcppermissionsrole","text":"Name of the GCP role to bind the resource to. Type: string Required: true Example spec : gcp : permissions : - resource : apiVersion : resourcemanager.cnrm.cloud.google.com/v1beta1 kind : Project name : myteam-dev-ab23 role : roles/cloudsql.client","title":"gcp.permissions[].role"},{"location":"naisjob/reference/#gcpsqlinstances","text":"Provision database instances and connect them to your application. Relevant information: https://doc.nais.io/persistence/postgres/ https://cloud.google.com/sql/docs/postgres/instance-settings#impact Type: array Required: false Availability: GCP Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances"},{"location":"naisjob/reference/#gcpsqlinstancesautobackuphour","text":"If specified, run automatic backups of the SQL database at the given hour. Note that this will backup the whole SQL instance, and not separate databases. Restores are done using the Google Cloud Console. Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].autoBackupHour"},{"location":"naisjob/reference/#gcpsqlinstancescascadingdelete","text":"Remove the entire Postgres server including all data when the Kubernetes resource is deleted. THIS IS A DESTRUCTIVE OPERATION ! Set cascading delete only when you want to remove data forever. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].cascadingDelete"},{"location":"naisjob/reference/#gcpsqlinstancescollation","text":"Sort order for ORDER BY ... clauses. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].collation"},{"location":"naisjob/reference/#gcpsqlinstancesdatabases","text":"List of databases that should be created on this Postgres server. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases"},{"location":"naisjob/reference/#gcpsqlinstancesdatabasesenvvarprefix","text":"Prefix to add to environment variables made available for database connection. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].envVarPrefix"},{"location":"naisjob/reference/#gcpsqlinstancesdatabasesname","text":"Database name. Type: string Required: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].name"},{"location":"naisjob/reference/#gcpsqlinstancesdatabasesusers","text":"Add extra users for database access. These users need to be manually given access to database tables. Type: array Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].users"},{"location":"naisjob/reference/#gcpsqlinstancesdatabasesusersname","text":"User name. Type: string Required: true Pattern: ^[_a-zA-Z][-_a-zA-Z0-9]+$ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].databases[].users[].name"},{"location":"naisjob/reference/#gcpsqlinstancesdiskautoresize","text":"When set to true, GCP will automatically increase storage by XXX for the database when disk usage is above the high water mark. Relevant information: https://cloud.google.com/sql/docs/postgres/instance-settings#threshold Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskAutoresize"},{"location":"naisjob/reference/#gcpsqlinstancesdisksize","text":"How much hard drive space to allocate for the SQL server, in gigabytes. Type: integer Required: false Minimum value: 10 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskSize"},{"location":"naisjob/reference/#gcpsqlinstancesdisktype","text":"Disk type to use for storage in the database. Type: enum Required: false Allowed values: HDD , SSD Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].diskType"},{"location":"naisjob/reference/#gcpsqlinstanceshighavailability","text":"When set to true this will set up standby database for failover. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].highAvailability"},{"location":"naisjob/reference/#gcpsqlinstancesinsights","text":"Configures query insights which are now default for new sql instances. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights"},{"location":"naisjob/reference/#gcpsqlinstancesinsightsenabled","text":"True if Query Insights feature is enabled. Type: boolean Required: false Default value: true Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.enabled"},{"location":"naisjob/reference/#gcpsqlinstancesinsightsquerystringlength","text":"Maximum query length stored in bytes. Between 256 and 4500. Default to 1024. Type: integer Required: false Value range: 256 - 4500 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.queryStringLength"},{"location":"naisjob/reference/#gcpsqlinstancesinsightsrecordapplicationtags","text":"True if Query Insights will record application tags from query when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.recordApplicationTags"},{"location":"naisjob/reference/#gcpsqlinstancesinsightsrecordclientaddress","text":"True if Query Insights will record client address when enabled. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].insights.recordClientAddress"},{"location":"naisjob/reference/#gcpsqlinstancesmaintenance","text":"Desired maintenance window for database updates. Type: object Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance"},{"location":"naisjob/reference/#gcpsqlinstancesmaintenanceday","text":"Type: integer Required: false Value range: 1 - 7 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance.day"},{"location":"naisjob/reference/#gcpsqlinstancesmaintenancehour","text":"Type: integer Required: false Value range: 0 - 23 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].maintenance.hour"},{"location":"naisjob/reference/#gcpsqlinstancesname","text":"The name of the instance, if omitted the application name will be used. Type: string Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].name"},{"location":"naisjob/reference/#gcpsqlinstancespointintimerecovery","text":"Enables point-in-time recovery for sql instances using write-ahead logs. Type: boolean Required: false Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].pointInTimeRecovery"},{"location":"naisjob/reference/#gcpsqlinstancestier","text":"Server tier, i.e. how much CPU and memory allocated. Available tiers are db-f1-micro , db-g1-small and custom db-custom-CPU-RAM . Custom memory must be mulitple of 256 MB and at least 3.75 GB (e.g. db-custom-1-3840 for 1 cpu, 3840 MB ram) Also check out sizing your database . Type: string Required: false Default value: db-f1-micro Pattern: db-.+ Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].tier"},{"location":"naisjob/reference/#gcpsqlinstancestype","text":"PostgreSQL version. Type: enum Required: true Allowed values: POSTGRES_11 , POSTGRES_12 Example spec : gcp : sqlInstances : - autoBackupHour : 1 cascadingDelete : true collation : nb_NO.UTF8 databases : - envVarPrefix : DB name : mydatabase users : - name : extra_user diskAutoresize : true diskSize : 30 diskType : SSD highAvailability : true insights : enabled : true queryStringLength : 4500 recordApplicationTags : true recordClientAddress : true maintenance : day : 1 hour : 4 name : myinstance pointInTimeRecovery : true tier : db-f1-micro type : POSTGRES_12","title":"gcp.sqlInstances[].type"},{"location":"naisjob/reference/#image","text":"Your Naisjob's Docker image location and tag. Type: string Required: true Example spec : image : navikt/testapp:69.0.0","title":"image"},{"location":"naisjob/reference/#influx","text":"An Influxdb via Aiven. A typical use case is to store metrics from your application and visualize them in Grafana. See navikt/aiven-iac repository Type: object Required: false Availability: GCP Example spec : influx : instance : influx-instance","title":"influx"},{"location":"naisjob/reference/#influxinstance","text":"Provisions an InfluxDB instance and configures your application to access it. Use the prefix: influx- + team that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : influx : instance : influx-instance","title":"influx.instance"},{"location":"naisjob/reference/#kafka","text":"Enable Aiven Kafka for your Naisjob. Type: object Required: false Example spec : kafka : pool : nav-dev streams : true","title":"kafka"},{"location":"naisjob/reference/#kafkapool","text":"Configures your application to access an Aiven Kafka cluster. Type: enum Required: true Allowed values: nav-dev , nav-infrastructure , nav-prod Example spec : kafka : pool : nav-dev","title":"kafka.pool"},{"location":"naisjob/reference/#kafkastreams","text":"Allow this app to use kafka streams Relevant information: https://doc.nais.io/persistence/kafka/application/#using-kafka-streams-with-internal-topics Type: boolean Required: false Default value: false Availability: GCP Example spec : kafka : streams : true","title":"kafka.streams"},{"location":"naisjob/reference/#liveness","text":"Many Naisjobs running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations. Read more about this over at the Kubernetes probes documentation . Type: object Required: false Example spec : liveness : failureThreshold : 10 initialDelay : 20 path : /isalive periodSeconds : 5 port : 8080 timeout : 1","title":"liveness"},{"location":"naisjob/reference/#livenessfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Default value: 3 Example spec : liveness : failureThreshold : 10","title":"liveness.failureThreshold"},{"location":"naisjob/reference/#livenessinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : liveness : initialDelay : 20","title":"liveness.initialDelay"},{"location":"naisjob/reference/#livenesspath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : liveness : path : /isalive","title":"liveness.path"},{"location":"naisjob/reference/#livenessperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Default value: 10 Example spec : liveness : periodSeconds : 5","title":"liveness.periodSeconds"},{"location":"naisjob/reference/#livenessport","text":"Port for the startup probe. Type: integer Required: false Example spec : liveness : port : 8080","title":"liveness.port"},{"location":"naisjob/reference/#livenesstimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Default value: 1 Example spec : liveness : timeout : 1","title":"liveness.timeout"},{"location":"naisjob/reference/#logformat","text":"Format of the logs from the container. Use this if the container doesn't support JSON logging and the log is in a special format that need to be parsed. Type: enum Required: false Allowed values: (empty string) , accesslog , accesslog_with_processing_time , accesslog_with_referer_useragent , capnslog , glog , gokit , influxdb , log15 , logrus , redis , simple Example spec : logformat : accesslog_with_referer_useragent","title":"logformat"},{"location":"naisjob/reference/#logtransform","text":"Extra filters for modifying log content. This can e.g. be used for setting loglevel based on http status code. Type: enum Required: false Allowed values: dns_loglevel , http_loglevel Example spec : logtransform : http_loglevel","title":"logtransform"},{"location":"naisjob/reference/#maskinporten","text":"Configures a Maskinporten client for this Naisjob. See Maskinporten for more details. Type: object Required: false Example spec : maskinporten : enabled : true scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten"},{"location":"naisjob/reference/#maskinportenenabled","text":"If enabled, provisions and configures a Maskinporten client with consumed scopes and/or Exposed scopes with DigDir. Type: boolean Required: true Default value: false Availability: team namespaces Example spec : maskinporten : enabled : true","title":"maskinporten.enabled"},{"location":"naisjob/reference/#maskinportenscopes","text":"Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes. Type: object Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes"},{"location":"naisjob/reference/#maskinportenscopesconsumes","text":"This is the Schema for the consumes and exposes API. consumes is a list of scopes that your client can request access to. Type: array Required: false Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read","title":"maskinporten.scopes.consumes"},{"location":"naisjob/reference/#maskinportenscopesconsumesname","text":"The scope consumed by the application to gain access to an external organization API. Ensure that the NAV organization has been granted access to the scope prior to requesting access. Relevant information: https://doc.nais.io/security/auth/maskinporten/#consume-scopes Type: string Required: true Example spec : maskinporten : scopes : consumes : - name : skatt:scope.read","title":"maskinporten.scopes.consumes[].name"},{"location":"naisjob/reference/#maskinportenscopesexposes","text":"exposes is a list of scopes your application want to expose to other organization where access to the scope is based on organization number. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes"},{"location":"naisjob/reference/#maskinportenscopesexposesallowedintegrations","text":"Whitelisting of integration's allowed. Default is maskinporten Relevant information: https://docs.digdir.no/maskinporten_guide_apitilbyder.html#scope-begrensninger Type: array Required: false Default value: maskinporten Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].allowedIntegrations"},{"location":"naisjob/reference/#maskinportenscopesexposesatmaxage","text":"Max time in seconds for a issued access_token. Default is 30 sec. Type: integer Required: false Default value: 30 Value range: 30 - 680 Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].atMaxAge"},{"location":"naisjob/reference/#maskinportenscopesexposesconsumers","text":"External consumers granted access to this scope and able to request access_token. Type: array Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers"},{"location":"naisjob/reference/#maskinportenscopesexposesconsumersname","text":"This is a describing field intended for clarity not used for any other purpose. Type: string Required: false Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers[].name"},{"location":"naisjob/reference/#maskinportenscopesexposesconsumersorgno","text":"The external business/organization number. Type: string Required: true Pattern: ^\\d{9}$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].consumers[].orgno"},{"location":"naisjob/reference/#maskinportenscopesexposesenabled","text":"If Enabled the configured scope is available to be used and consumed by organizations granted access. Relevant information: https://doc.nais.io/naisjob/reference/#maskinportenscopesexposesconsumers Type: boolean Required: true Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].enabled"},{"location":"naisjob/reference/#maskinportenscopesexposesname","text":"The actual subscope combined with Product . Ensure that <Product><Name> matches Pattern . Type: string Required: true Default value: false Pattern: ^([a-z\u00e6\u00f8\u00e50-9]+\\/?)+(\\:[a-z\u00e6\u00f8\u00e50-9]+)*[a-z\u00e6\u00f8\u00e50-9]+(\\.[a-z\u00e6\u00f8\u00e50-9]+)*$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].name"},{"location":"naisjob/reference/#maskinportenscopesexposesproduct","text":"The product-area your application belongs to e.g. arbeid, helse ... This will be included in the final scope nav:<Product><Name> . Type: string Required: true Pattern: ^[a-z0-9]+$ Example spec : maskinporten : scopes : exposes : - allowedIntegrations : - maskinporten atMaxAge : 30 consumers : - name : KST orgno : \"123456789\" enabled : true name : scope.read product : arbeid","title":"maskinporten.scopes.exposes[].product"},{"location":"naisjob/reference/#opensearch","text":"To get your own OpenSearch instance head over to the IaC-repo to provision each instance. See navikt/aiven-iac repository. Type: object Required: false Example spec : openSearch : access : readwrite instance : my-open-search-instance","title":"openSearch"},{"location":"naisjob/reference/#opensearchaccess","text":"Access level for OpenSearch user Type: enum Required: false Allowed values: admin , read , readwrite , write Example spec : openSearch : access : readwrite","title":"openSearch.access"},{"location":"naisjob/reference/#opensearchinstance","text":"Configure your application to access your OpenSearch instance. Use the instance_name that you specified in the navikt/aiven-iac repository. Type: string Required: true Example spec : openSearch : instance : my-open-search-instance","title":"openSearch.instance"},{"location":"naisjob/reference/#parallelism","text":"For running pods in parallel. If it is specified as 0, then the Job is effectively paused until it is increased. Relevant information: https://kubernetes.io/docs/concepts/workloads/controllers/job/#controlling-parallelism Type: integer Required: false Default value: 1 Example spec : parallelism : 1","title":"parallelism"},{"location":"naisjob/reference/#prestophook","text":"PreStopHook is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits by itself. The reason for termination is passed to the handler. Relevant information: https://doc.nais.io/naisjob/#handles-termination-gracefully https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script http : path : /internal/stop port : 8080","title":"preStopHook"},{"location":"naisjob/reference/#prestophookexec","text":"Command that should be run inside the main container just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script","title":"preStopHook.exec"},{"location":"naisjob/reference/#prestophookexeccommand","text":"Command is the command line to execute inside the container before the pod is shut down. The command is not run inside a shell, so traditional shell instructions (pipes, redirects, etc.) won't work. To use a shell, you need to explicitly call out to that shell. If the exit status is non-zero, the pod will still be shut down, and marked as Failed . Type: array Required: false Example spec : preStopHook : exec : command : - ./my - --shell - script","title":"preStopHook.exec.command"},{"location":"naisjob/reference/#prestophookhttp","text":"HTTP GET request that is called just before the pod is shut down by Kubernetes. Type: object Required: false Example spec : preStopHook : http : path : /internal/stop port : 8080","title":"preStopHook.http"},{"location":"naisjob/reference/#prestophookhttppath","text":"Path to access on the HTTP server. Type: string Required: true Example spec : preStopHook : http : path : /internal/stop","title":"preStopHook.http.path"},{"location":"naisjob/reference/#prestophookhttpport","text":"Port to access on the container. Defaults to application port, as defined in .spec.port . Type: integer Required: false Value range: 1 - 65535 Example spec : preStopHook : http : port : 8080","title":"preStopHook.http.port"},{"location":"naisjob/reference/#readiness","text":"Sometimes, Naisjobs are temporarily unable to serve traffic. For example, an Naisjob might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the Naisjob, but you don\u2019t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services. Read more about this over at the Kubernetes readiness documentation . Type: object Required: false Example spec : readiness : failureThreshold : 10 initialDelay : 20 path : /isready periodSeconds : 5 port : 8080 timeout : 1","title":"readiness"},{"location":"naisjob/reference/#readinessfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : readiness : failureThreshold : 10","title":"readiness.failureThreshold"},{"location":"naisjob/reference/#readinessinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : readiness : initialDelay : 20","title":"readiness.initialDelay"},{"location":"naisjob/reference/#readinesspath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : readiness : path : /isready","title":"readiness.path"},{"location":"naisjob/reference/#readinessperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Example spec : readiness : periodSeconds : 5","title":"readiness.periodSeconds"},{"location":"naisjob/reference/#readinessport","text":"Port for the startup probe. Type: integer Required: false Example spec : readiness : port : 8080","title":"readiness.port"},{"location":"naisjob/reference/#readinesstimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Example spec : readiness : timeout : 1","title":"readiness.timeout"},{"location":"naisjob/reference/#resources","text":"When Containers have resource requests specified, the Kubernetes scheduler can make better decisions about which nodes to place pods on. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi","title":"resources"},{"location":"naisjob/reference/#resourceslimits","text":"Limit defines the maximum amount of resources a container can use before getting evicted. Type: object Required: false Example spec : resources : limits : cpu : 500m memory : 512Mi","title":"resources.limits"},{"location":"naisjob/reference/#resourceslimitscpu","text":"Type: string Required: false Default value: 500m Pattern: ^\\d+m?$ Example spec : resources : limits : cpu : 500m","title":"resources.limits.cpu"},{"location":"naisjob/reference/#resourceslimitsmemory","text":"Type: string Required: false Default value: 512Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : limits : memory : 512Mi","title":"resources.limits.memory"},{"location":"naisjob/reference/#resourcesrequests","text":"Request defines the amount of resources a container is allocated on startup. Type: object Required: false Example spec : resources : requests : cpu : 200m memory : 256Mi","title":"resources.requests"},{"location":"naisjob/reference/#resourcesrequestscpu","text":"Type: string Required: false Default value: 200m Pattern: ^\\d+m?$ Example spec : resources : requests : cpu : 200m","title":"resources.requests.cpu"},{"location":"naisjob/reference/#resourcesrequestsmemory","text":"Type: string Required: false Default value: 256Mi Pattern: ^\\d+[KMG]i$ Example spec : resources : requests : memory : 256Mi","title":"resources.requests.memory"},{"location":"naisjob/reference/#restartpolicy","text":"RestartPolicy describes how the container should be restarted. Only one of the following restart policies may be specified. If none of the following policies is specified, the default one is Never. Read more about Kubernetes handling pod and container failures Type: enum Required: false Default value: Never Allowed values: Never , OnFailure Example spec : restartPolicy : Never","title":"restartPolicy"},{"location":"naisjob/reference/#schedule","text":"The Cron schedule for running the Naisjob. If not specified, the Naisjob will be run as a one-shot Job. Type: string Required: false Example spec : schedule : '*/15 0 0 0 0'","title":"schedule"},{"location":"naisjob/reference/#securelogs","text":"Whether or not to enable a sidecar container for secure logging. Type: object Required: false Example spec : secureLogs : enabled : true","title":"secureLogs"},{"location":"naisjob/reference/#securelogsenabled","text":"Whether to enable a sidecar container for secure logging. If enabled, a volume is mounted in the pods where secure logs can be saved. Type: boolean Required: true Default value: false Example spec : secureLogs : enabled : true","title":"secureLogs.enabled"},{"location":"naisjob/reference/#skipcabundle","text":"Whether to skip injection of NAV certificate authority bundle or not. Defaults to false. Type: boolean Required: false Example spec : skipCaBundle : true","title":"skipCaBundle"},{"location":"naisjob/reference/#startup","text":"Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Type: object Required: false Example spec : startup : failureThreshold : 10 initialDelay : 20 path : /started periodSeconds : 5 port : 8080 timeout : 1","title":"startup"},{"location":"naisjob/reference/#startupfailurethreshold","text":"When a Pod starts, and the probe fails, Kubernetes will try failureThreshold times before giving up. Giving up in case of a startup probe means restarting the Pod. Type: integer Required: false Example spec : startup : failureThreshold : 10","title":"startup.failureThreshold"},{"location":"naisjob/reference/#startupinitialdelay","text":"Number of seconds after the container has started before startup probes are initiated. Type: integer Required: false Example spec : startup : initialDelay : 20","title":"startup.initialDelay"},{"location":"naisjob/reference/#startuppath","text":"HTTP endpoint path that signals 200 OK if the application has started successfully. Type: string Required: true Example spec : startup : path : /started","title":"startup.path"},{"location":"naisjob/reference/#startupperiodseconds","text":"How often (in seconds) to perform the probe. Type: integer Required: false Example spec : startup : periodSeconds : 5","title":"startup.periodSeconds"},{"location":"naisjob/reference/#startupport","text":"Port for the startup probe. Type: integer Required: false Example spec : startup : port : 8080","title":"startup.port"},{"location":"naisjob/reference/#startuptimeout","text":"Number of seconds after which the probe times out. Type: integer Required: false Example spec : startup : timeout : 1","title":"startup.timeout"},{"location":"naisjob/reference/#successfuljobshistorylimit","text":"Specify how many completed Jobs should be kept. Type: integer Required: false Default value: 3 Example spec : successfulJobsHistoryLimit : 2","title":"successfulJobsHistoryLimit"},{"location":"naisjob/reference/#ttlsecondsafterfinished","text":"Specify the number of seconds to wait before removing the Job after it has finished (either Completed or Failed). If the field is unset, this Job won't be cleaned up by the TTL controller after it finishes. Type: integer Required: false Availability: on-premises Example spec : ttlSecondsAfterFinished : 60","title":"ttlSecondsAfterFinished"},{"location":"naisjob/reference/#vault","text":"Provides secrets management, identity-based access, and encrypting application data for auditing of secrets for applications, systems, and users. Relevant information: https://github.com/navikt/vault-iac/tree/master/doc Type: object Required: false Availability: on-premises Example spec : vault : enabled : true paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault sidecar : true","title":"vault"},{"location":"naisjob/reference/#vaultenabled","text":"If set to true, fetch secrets from Vault and inject into the pods. Type: boolean Required: false Example spec : vault : enabled : true","title":"vault.enabled"},{"location":"naisjob/reference/#vaultpaths","text":"List of secret paths to be read from Vault and injected into the pod's filesystem. Overriding the paths array is optional, and will give you fine-grained control over which Vault paths that will be mounted on the file system. By default, the list will contain an entry with kvPath: /kv/<environment>/<zone>/<application>/<namespace> mountPath: /var/run/secrets/nais.io/vault that will always be attempted to be mounted. Type: array Required: false Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths"},{"location":"naisjob/reference/#vaultpathsformat","text":"Format of the secret that should be processed. Type: enum Required: false Allowed values: (empty string) , env , flatten , json , properties , yaml Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].format"},{"location":"naisjob/reference/#vaultpathskvpath","text":"Path to Vault key/value store that should be mounted into the file system. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].kvPath"},{"location":"naisjob/reference/#vaultpathsmountpath","text":"File system path that the secret will be mounted into. Type: string Required: true Example spec : vault : paths : - format : env kvPath : /kv/preprod/fss/application/namespace mountPath : /var/run/secrets/nais.io/vault","title":"vault.paths[].mountPath"},{"location":"naisjob/reference/#vaultsidecar","text":"If enabled, the sidecar will automatically refresh the token's Time-To-Live before it expires. Type: boolean Required: false Example spec : vault : sidecar : true","title":"vault.sidecar"},{"location":"naisjob/reference/#webproxy","text":"Inject on-premises web proxy configuration into the job container. Most Linux applications should auto-detect these settings from the $HTTP_PROXY , $HTTPS_PROXY and $NO_PROXY environment variables (and their lowercase counterparts). Java applications can start the JVM using parameters from the $JAVA_PROXY_OPTIONS environment variable. Type: boolean Required: false Availability: on-premises Example spec : webproxy : true","title":"webproxy"},{"location":"observability/metrics/","text":"Metrics \u00a7 Prometheus is used to scrape metrics from the pod. You have to add Prometheus in your NAIS manifest to enable scrape. Each app that have scraping enabled can use the default Grafana dashboard nais-app-dashboard , or create theire own. NAIS manifest config \u00a7 spec: prometheus: enabled: true # default: false. Pod will now be scraped for metrics by Prometheus. path: /metrics # Path where prometheus metrics are served. JVM Application \u00a7 If you are building an app on the JVM you can use Prometheus' own Java client library . Make sure to enable scraping in the NAIS manifest. We also recommend to export the default metrics. DefaultExports.initialize(); Retention \u00a7 When using Prometheus the retention is 16 weeks for prod, and 4 weeks for dev. If you need data stored longer then what Prometheus support, we recommend using your own Aiven Influxdb . Then you have full control of the database and retention. Data via push metrics/sensu is stored for one year. Push metrics \u00a7 If you don't want to just rely on pull metrics, you can push data directly to InfluxDB via Sensu . This is easily done by writing to the Sensu socket. sensu.nais:3030 Example message: { \"name\" : \"myapp_metrics\" , \"type\" : \"metric\" , \"handlers\" : [ \"events_nano\" ], \"output\" : \"myapp.event1,tag1=x,tag2=y value=1,value2=2 1571402276000000000\\nmyapp.event2,tag1=xx,tag2=yy value=42,value2=69 1571402276000000000\" } The format of the data (The output field in the message) should be formatted as Influxdb Line Protocol . Warning Note that each variation of tag values will create a new time series, so avoid using tags for data that varies a lot. Read more about best practices here: InfluxDB schema design and data layout Overview \u00a7","title":"Metrics"},{"location":"observability/metrics/#metrics","text":"Prometheus is used to scrape metrics from the pod. You have to add Prometheus in your NAIS manifest to enable scrape. Each app that have scraping enabled can use the default Grafana dashboard nais-app-dashboard , or create theire own.","title":"Metrics"},{"location":"observability/metrics/#nais-manifest-config","text":"spec: prometheus: enabled: true # default: false. Pod will now be scraped for metrics by Prometheus. path: /metrics # Path where prometheus metrics are served.","title":"NAIS manifest config"},{"location":"observability/metrics/#jvm-application","text":"If you are building an app on the JVM you can use Prometheus' own Java client library . Make sure to enable scraping in the NAIS manifest. We also recommend to export the default metrics. DefaultExports.initialize();","title":"JVM Application"},{"location":"observability/metrics/#retention","text":"When using Prometheus the retention is 16 weeks for prod, and 4 weeks for dev. If you need data stored longer then what Prometheus support, we recommend using your own Aiven Influxdb . Then you have full control of the database and retention. Data via push metrics/sensu is stored for one year.","title":"Retention"},{"location":"observability/metrics/#push-metrics","text":"If you don't want to just rely on pull metrics, you can push data directly to InfluxDB via Sensu . This is easily done by writing to the Sensu socket. sensu.nais:3030 Example message: { \"name\" : \"myapp_metrics\" , \"type\" : \"metric\" , \"handlers\" : [ \"events_nano\" ], \"output\" : \"myapp.event1,tag1=x,tag2=y value=1,value2=2 1571402276000000000\\nmyapp.event2,tag1=xx,tag2=yy value=42,value2=69 1571402276000000000\" } The format of the data (The output field in the message) should be formatted as Influxdb Line Protocol . Warning Note that each variation of tag values will create a new time series, so avoid using tags for data that varies a lot. Read more about best practices here: InfluxDB schema design and data layout","title":"Push metrics"},{"location":"observability/metrics/#overview","text":"","title":"Overview"},{"location":"observability/tracing/","text":"Tracing \u00a7 With program flow distributed across many microservices, the ability to observe the different services in context can become a valuable tool for development and operations. For the GCP clusters, NAIS offers a rich toolkit for this. The simplest way to get an overview is to observe the linkerd service mesh using the dashboard. Visualizing service mesh with Linkerd dashboard \u00a7 Linkerd can be reached at linkerd. cluster-name .nais.io, eg. linkerd.dev-gcp.nais.io . NAIS leverages the Linkerd distributed tracing and Linkerd dashboard to give developers a visualization of the service mesh and its general health. Linkerd will also give a quick graphical overview of failing HTTP calls.","title":"Tracing"},{"location":"observability/tracing/#tracing","text":"With program flow distributed across many microservices, the ability to observe the different services in context can become a valuable tool for development and operations. For the GCP clusters, NAIS offers a rich toolkit for this. The simplest way to get an overview is to observe the linkerd service mesh using the dashboard.","title":"Tracing"},{"location":"observability/tracing/#visualizing-service-mesh-with-linkerd-dashboard","text":"Linkerd can be reached at linkerd. cluster-name .nais.io, eg. linkerd.dev-gcp.nais.io . NAIS leverages the Linkerd distributed tracing and Linkerd dashboard to give developers a visualization of the service mesh and its general health. Linkerd will also give a quick graphical overview of failing HTTP calls.","title":"Visualizing service mesh with Linkerd dashboard"},{"location":"observability/alerts/","text":"Alerts \u00a7 We use Prometheus to collect metrics, and can trigger alerts based on these metrics. Alerts are specified in their own Kubernetes resource called alerts as we have made our own operator called Alerterator . Getting started \u00a7 To get started using Alerts we need to have a yaml-file describing our rules. You can start of with our recommended alerts (made by contributions from our users!). Usually it's simplest to just call this file alerts.yaml , which we will do in this example. You can see the content of the file below, and you will see that we only notify Slack, but you can add more receivers. apiVersion : \"nais.io/v1\" kind : \"Alert\" metadata : name : nais-testapp namespace : aura labels : team : aura spec : receivers : # receivers for all alerts below slack : channel : '#nais-alerts-dev' prependText : '<!here> | ' # this text will be prepended to the Slack alert title alerts : - alert : Nais-testapp unavailable expr : 'kube_deployment_status_replicas_unavailable{deployment=\"nais-testapp\"} > 0' for : 2m action : Read app logs(kubectl logs appname). Read Application events (kubectl descibe deployment appname) description : The app might crash sometimes due to startup errors documentation : https://github.com/navikt/aura-doc/naisvakt/alerts.md#app_unavailable sla : respond within 1h, during office hours severity : danger - alert : CoreDNS unavailable description : CoreDNS unavailable, there are zero replicas expr : 'kube_deployment_status_replicas_available{namespace=\"kube-system\", deployment=\"coredns\"} == 0' for : 1m action : kubectl describe pod -l app=nais-testapp documentation : https://github.com/navikt/aura-doc/naisvakt/alerts.md#coredns sla : respond within 1h, solve within 4h, around the clock severity : danger Deployment \u00a7 If you already use nais/deploy , adding a new deploy-action is rather simple. Create a file called .github/workflows/alert-deploy.yaml name : Deploy alerts to NAIS on : push : branches : - master paths : - 'alerts.yaml' - '.github/workflows/alerts.yaml' jobs : apply-alerts : name : Apply alerts to cluster runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v2 - name : deploy to dev uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : dev-fss RESOURCE : /path/to/alerts.yaml - name : deploy to prod uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : prod-fss RESOURCE : /path/to/alerts.yaml You can also add /path/to/alerts.yaml to RESOURCE in a previous workflow, just remember to escape your expressive descriptions. Different receivers/notifications \u00a7 Alerterator supports three different types of notification, Slack , E-mail , and SMS . In our spec we follow the naming convention of Alertmanager and call it receivers . Take a look below too see exactly how each receiver is configured. Slack \u00a7 spec: receivers: slack: channel: '#teamname-alerts' prependText: '<!here>' Slack will always notify you when an alert is resolved. E-mail \u00a7 spec: receivers: email: to: 'kari.nordmann@nav.no' send_resolved: 'true' send_resolved can be set to true if you want to receiver notification for when an alert is resolved. SMS \u00a7 spec: receivers: sms: recipients: '12345678,8764321' send_resolved: 'false' send_resolved can be set to true if you want to receiver notification for when an alert is resolved. How to write a good alert \u00a7 Writing the expr \u00a7 In order to minimize the feedback loop we suggest experimenting on the Prometheus server to find the right metric for your alert and the notification threshold. The Prometheus server can be found in each cluster, at https://prometheus.{cluster.ingress} (i.e. https://prometheus.dev-gcp.nais.io ). You can also visit the Alertmanager at https://alertmanager.{cluster.ingress} (i.e. https://alertmanager.dev-gcp.nais.io ) to see which alerts are triggered now (you can also silence already triggered alerts). Expressive descriptions or actions \u00a7 You can also use labels in your notification by referencing them with {{ $labels.<field> }} . For example: {{ $labels.node }} is marked as unschedulable turns into the following when notifying: b27apvl00178.preprod.local is marked as unschedulable Note that the query decides which labels are available, since the labels correnspond to the field names in the document that is the query result. To see which labels are available for your specific query, run your query on the Prometheus server. Here's an example (remember to connect your naisdeveice). You can read more about this over at the Prometheus documentation and at the Kubernetes documentation of exposed metrics . Labels and nais/deploy \u00a7 If you are using labels and variables with nais/deploy, you need to remember to escape the labels used for the alerts. See deployment#escaping-and-raw-resources for how. Value of expression \u00a7 You can get the value of your expression with $value Target several apps or namespaces in a query \u00a7 Using regular expression, you can target multiple apps or namespaces with one query. This saves on repeating the same alert for each your apps. absent(up{app=~\"myapp|otherapp|thirdapp\"}) Here we use =~ to select labels that match the provided string (or substring) using a regular expression. Use !~ to negate the regular expression. When doing this, it can be smart to use labels (mention above), to list which namespace the alert is tied to. Use the kubernetes_namespace label in your action or description by adding {{ $labels.kubernetes_namespace }} , and it will write the namespace for the app that is having problems. Slack \u00a7 Notify @here and @team \u00a7 Slack has their own syntax for notifying @channel or @here , respectively <!channel> and <!here> . Notifying a user group on the other hand is a bit more complicated. The user group @nais-vakt is written <!subteam^SB8KS4WAV|nais-vakt> in a Slack alert message, where SB8KS4WAV is the id for the specific user group, and nais-vakt is the name of the user group. You can find the id by right clicking on the name in the user group list. Where last part in the URL is the id. The URL will look something like the one below: https://nav-it.slack.com/usergroups/SB8KS4WAV Examples of the different Slack/severity colors \u00a7 Slack alerts supports colors. The severity field defines the color ( good is results in a green message, warning in a yellow and danger in a red). If you want to define another color, specify the wanted hex code as severity instead of the pre-defined words. Flow \u00a7","title":"Alerts Overview"},{"location":"observability/alerts/#alerts","text":"We use Prometheus to collect metrics, and can trigger alerts based on these metrics. Alerts are specified in their own Kubernetes resource called alerts as we have made our own operator called Alerterator .","title":"Alerts"},{"location":"observability/alerts/#getting-started","text":"To get started using Alerts we need to have a yaml-file describing our rules. You can start of with our recommended alerts (made by contributions from our users!). Usually it's simplest to just call this file alerts.yaml , which we will do in this example. You can see the content of the file below, and you will see that we only notify Slack, but you can add more receivers. apiVersion : \"nais.io/v1\" kind : \"Alert\" metadata : name : nais-testapp namespace : aura labels : team : aura spec : receivers : # receivers for all alerts below slack : channel : '#nais-alerts-dev' prependText : '<!here> | ' # this text will be prepended to the Slack alert title alerts : - alert : Nais-testapp unavailable expr : 'kube_deployment_status_replicas_unavailable{deployment=\"nais-testapp\"} > 0' for : 2m action : Read app logs(kubectl logs appname). Read Application events (kubectl descibe deployment appname) description : The app might crash sometimes due to startup errors documentation : https://github.com/navikt/aura-doc/naisvakt/alerts.md#app_unavailable sla : respond within 1h, during office hours severity : danger - alert : CoreDNS unavailable description : CoreDNS unavailable, there are zero replicas expr : 'kube_deployment_status_replicas_available{namespace=\"kube-system\", deployment=\"coredns\"} == 0' for : 1m action : kubectl describe pod -l app=nais-testapp documentation : https://github.com/navikt/aura-doc/naisvakt/alerts.md#coredns sla : respond within 1h, solve within 4h, around the clock severity : danger","title":"Getting started"},{"location":"observability/alerts/#deployment","text":"If you already use nais/deploy , adding a new deploy-action is rather simple. Create a file called .github/workflows/alert-deploy.yaml name : Deploy alerts to NAIS on : push : branches : - master paths : - 'alerts.yaml' - '.github/workflows/alerts.yaml' jobs : apply-alerts : name : Apply alerts to cluster runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v2 - name : deploy to dev uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : dev-fss RESOURCE : /path/to/alerts.yaml - name : deploy to prod uses : nais/deploy/actions/deploy@v1 env : APIKEY : ${{ secrets.NAIS_DEPLOY_APIKEY }} CLUSTER : prod-fss RESOURCE : /path/to/alerts.yaml You can also add /path/to/alerts.yaml to RESOURCE in a previous workflow, just remember to escape your expressive descriptions.","title":"Deployment"},{"location":"observability/alerts/#different-receiversnotifications","text":"Alerterator supports three different types of notification, Slack , E-mail , and SMS . In our spec we follow the naming convention of Alertmanager and call it receivers . Take a look below too see exactly how each receiver is configured.","title":"Different receivers/notifications"},{"location":"observability/alerts/#slack","text":"spec: receivers: slack: channel: '#teamname-alerts' prependText: '<!here>' Slack will always notify you when an alert is resolved.","title":"Slack"},{"location":"observability/alerts/#e-mail","text":"spec: receivers: email: to: 'kari.nordmann@nav.no' send_resolved: 'true' send_resolved can be set to true if you want to receiver notification for when an alert is resolved.","title":"E-mail"},{"location":"observability/alerts/#sms","text":"spec: receivers: sms: recipients: '12345678,8764321' send_resolved: 'false' send_resolved can be set to true if you want to receiver notification for when an alert is resolved.","title":"SMS"},{"location":"observability/alerts/#how-to-write-a-good-alert","text":"","title":"How to write a good alert"},{"location":"observability/alerts/#writing-the-expr","text":"In order to minimize the feedback loop we suggest experimenting on the Prometheus server to find the right metric for your alert and the notification threshold. The Prometheus server can be found in each cluster, at https://prometheus.{cluster.ingress} (i.e. https://prometheus.dev-gcp.nais.io ). You can also visit the Alertmanager at https://alertmanager.{cluster.ingress} (i.e. https://alertmanager.dev-gcp.nais.io ) to see which alerts are triggered now (you can also silence already triggered alerts).","title":"Writing the expr"},{"location":"observability/alerts/#expressive-descriptions-or-actions","text":"You can also use labels in your notification by referencing them with {{ $labels.<field> }} . For example: {{ $labels.node }} is marked as unschedulable turns into the following when notifying: b27apvl00178.preprod.local is marked as unschedulable Note that the query decides which labels are available, since the labels correnspond to the field names in the document that is the query result. To see which labels are available for your specific query, run your query on the Prometheus server. Here's an example (remember to connect your naisdeveice). You can read more about this over at the Prometheus documentation and at the Kubernetes documentation of exposed metrics .","title":"Expressive descriptions or actions"},{"location":"observability/alerts/#labels-and-naisdeploy","text":"If you are using labels and variables with nais/deploy, you need to remember to escape the labels used for the alerts. See deployment#escaping-and-raw-resources for how.","title":"Labels and nais/deploy"},{"location":"observability/alerts/#value-of-expression","text":"You can get the value of your expression with $value","title":"Value of expression"},{"location":"observability/alerts/#target-several-apps-or-namespaces-in-a-query","text":"Using regular expression, you can target multiple apps or namespaces with one query. This saves on repeating the same alert for each your apps. absent(up{app=~\"myapp|otherapp|thirdapp\"}) Here we use =~ to select labels that match the provided string (or substring) using a regular expression. Use !~ to negate the regular expression. When doing this, it can be smart to use labels (mention above), to list which namespace the alert is tied to. Use the kubernetes_namespace label in your action or description by adding {{ $labels.kubernetes_namespace }} , and it will write the namespace for the app that is having problems.","title":"Target several apps or namespaces in a query"},{"location":"observability/alerts/#slack_1","text":"","title":"Slack"},{"location":"observability/alerts/#notify-here-and-team","text":"Slack has their own syntax for notifying @channel or @here , respectively <!channel> and <!here> . Notifying a user group on the other hand is a bit more complicated. The user group @nais-vakt is written <!subteam^SB8KS4WAV|nais-vakt> in a Slack alert message, where SB8KS4WAV is the id for the specific user group, and nais-vakt is the name of the user group. You can find the id by right clicking on the name in the user group list. Where last part in the URL is the id. The URL will look something like the one below: https://nav-it.slack.com/usergroups/SB8KS4WAV","title":"Notify @here and @team"},{"location":"observability/alerts/#examples-of-the-different-slackseverity-colors","text":"Slack alerts supports colors. The severity field defines the color ( good is results in a green message, warning in a yellow and danger in a red). If you want to define another color, specify the wanted hex code as severity instead of the pre-defined words.","title":"Examples of the different Slack/severity colors"},{"location":"observability/alerts/#flow","text":"","title":"Flow"},{"location":"observability/alerts/example/","text":"NAIS Alert example YAML \u00a7 This is a complete example of an Alert resource, commonly known as the alert.yaml file. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1 kind : Alert metadata : creationTimestamp : null labels : team : myteam name : myalert namespace : myteam spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ receivers : email : to : myteam@nav.no slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager sms : recipients : \"12345678\" send_resolved : false route : group_by : - label - name groupInterval : 5m groupWait : 30s repeatInterval : 3h status : {}","title":"Full example"},{"location":"observability/alerts/example/#nais-alert-example-yaml","text":"This is a complete example of an Alert resource, commonly known as the alert.yaml file. For an in-depth explanation of each field, head over to the reference documentation . apiVersion : nais.io/v1 kind : Alert metadata : creationTimestamp : null labels : team : myteam name : myalert namespace : myteam spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ receivers : email : to : myteam@nav.no slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager sms : recipients : \"12345678\" send_resolved : false route : group_by : - label - name groupInterval : 5m groupWait : 30s repeatInterval : 3h status : {}","title":"NAIS Alert example YAML"},{"location":"observability/alerts/recommended_alerts/","text":"Recommended \u00a7 # Dette er en liste over anbefalte Alerts, har du forbedringer, eller forslag # til Alerts vi b\u00f8r ha med? Lag en pull request, s\u00e5 merger vi inn! --- apiVersion : \"nais.io/v1\" kind : \"Alert\" metadata : name : <alert-name> namespace : <namespace> labels : team : <teamname> spec : receivers : slack : channel : '<appchannel>' prependText : '<!here> | ' alerts : - alert : applikasjon nede expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} == 0 for : 2m description : \"App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }}\" action : \"`kubectl describe pod -l app={{ $labels.app }} -n {{ $labels.namespace }}` for events, og `kubectl logs -l app={{ $labels.app }} -n {{ $labels.namespace }}` for logger\" - alert : h\u00f8y feilrate i logger expr : (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=\"<appname>\",log_level=~\"Warning|Error\"}[3m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=\"<appname>\"}[3m]))) > 10 for : 3m action : \"Sjekk loggene til app {{ $labels.log_app }} i namespace {{ $labels.log_namespace }}, for \u00e5 se hvorfor det er s\u00e5 mye feil\" - alert : feil i selftest expr : selftests_aggregate_result_status{app=\"<appname>\"} > 0 for : 1m action : \"Sjekk app {{ $labels.app }} i namespace {{ $labels.kubernetes_namespace }} sine selftest for \u00e5 se hva som er galt\" - alert : H\u00f8y andel HTTP serverfeil (5xx responser) severity : danger expr : (100 * (sum by (backend) (rate(traefik_backend_requests_total{code=~\"^5\\\\d\\\\d\", backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])) / sum by (backend) (rate(traefik_backend_requests_total{backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])))) > 1 for : 3m action : \"Sjekk loggene for \u00e5 se hvorfor {{ $labels.backend }} returnerer HTTP feilresponser\" - alert : H\u00f8y andel HTTP klientfeil (4xx responser) severity : warning expr : (100 * (sum by (backend) (rate(traefik_backend_requests_total{code=~\"^4\\\\d\\\\d\", backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])) / sum by (backend) (rate(traefik_backend_requests_total{backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])))) > 10 for : 3m action : \"Sjekk loggene for \u00e5 se hvorfor {{ $labels.backend }} returnerer HTTP feilresponser\"","title":"Recommended"},{"location":"observability/alerts/recommended_alerts/#recommended","text":"# Dette er en liste over anbefalte Alerts, har du forbedringer, eller forslag # til Alerts vi b\u00f8r ha med? Lag en pull request, s\u00e5 merger vi inn! --- apiVersion : \"nais.io/v1\" kind : \"Alert\" metadata : name : <alert-name> namespace : <namespace> labels : team : <teamname> spec : receivers : slack : channel : '<appchannel>' prependText : '<!here> | ' alerts : - alert : applikasjon nede expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} == 0 for : 2m description : \"App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }}\" action : \"`kubectl describe pod -l app={{ $labels.app }} -n {{ $labels.namespace }}` for events, og `kubectl logs -l app={{ $labels.app }} -n {{ $labels.namespace }}` for logger\" - alert : h\u00f8y feilrate i logger expr : (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=\"<appname>\",log_level=~\"Warning|Error\"}[3m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app=\"<appname>\"}[3m]))) > 10 for : 3m action : \"Sjekk loggene til app {{ $labels.log_app }} i namespace {{ $labels.log_namespace }}, for \u00e5 se hvorfor det er s\u00e5 mye feil\" - alert : feil i selftest expr : selftests_aggregate_result_status{app=\"<appname>\"} > 0 for : 1m action : \"Sjekk app {{ $labels.app }} i namespace {{ $labels.kubernetes_namespace }} sine selftest for \u00e5 se hva som er galt\" - alert : H\u00f8y andel HTTP serverfeil (5xx responser) severity : danger expr : (100 * (sum by (backend) (rate(traefik_backend_requests_total{code=~\"^5\\\\d\\\\d\", backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])) / sum by (backend) (rate(traefik_backend_requests_total{backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])))) > 1 for : 3m action : \"Sjekk loggene for \u00e5 se hvorfor {{ $labels.backend }} returnerer HTTP feilresponser\" - alert : H\u00f8y andel HTTP klientfeil (4xx responser) severity : warning expr : (100 * (sum by (backend) (rate(traefik_backend_requests_total{code=~\"^4\\\\d\\\\d\", backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])) / sum by (backend) (rate(traefik_backend_requests_total{backend=~\"mydomain.nais.*/mycontextpath/*\"}[3m])))) > 10 for : 3m action : \"Sjekk loggene for \u00e5 se hvorfor {{ $labels.backend }} returnerer HTTP feilresponser\"","title":"Recommended"},{"location":"observability/alerts/reference/","text":"NAIS Alert reference \u00a7 This document describes all possible configuration values in the Alert spec, commonly known as the alert.yaml file. alerts \u00a7 Type: array Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].action \u00a7 What human actions are needed to resolve or investigate this alert. Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].alert \u00a7 The name of the alert. Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].description \u00a7 Simple description of the triggered alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].documentation \u00a7 URL for documentation for this alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].expr \u00a7 Prometheus expression that triggers an alert. Explore expressions in the Prometheus -interface Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].for \u00a7 Duration before the alert should trigger. Type: string Required: true Pattern: ^\\d+[smhdwy]$ Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].severity \u00a7 Alert level for Slack messages. Type: string Required: false Default value: danger Pattern: ^$|good|warning|danger|#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3}) Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 alerts[].sla \u00a7 Time before a human should resolve the alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16 inhibitRules \u00a7 A list of inhibit rules. An inhibition rule mutes an alert (target) matching a set of matchers when an alert (source) exists that matches another set of matchers. Both target and source alerts must have the same label values for the label names in the labels list. Relevant information: https://prometheus.io/docs/alerting/latest/configuration/#inhibit_rule Type: array Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ inhibitRules[].labels \u00a7 Labels that must have an equal value in the source and target alert for the inhibition to take effect. Type: array Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ inhibitRules[].sources \u00a7 Matchers for which one or more alerts have to exist for the inhibition to take effect. These are key/value pairs. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ inhibitRules[].sourcesRegex \u00a7 Regex matchers for which one or more alerts have to exist for the inhibition to take effect. These are key/value pairs, where the value can be a regex. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ inhibitRules[].targets \u00a7 Matchers that have to be fulfilled in the alerts to be muted. These are key/value pairs. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ inhibitRules[].targetsRegex \u00a7 Regex matchers that have to be fulfilled in the alerts to be muted. These are key/value pairs, where the value can be a regex. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+ receivers \u00a7 A list of notification recievers. You can use one or more of: e-mail, slack, sms. There needs to be at least one receiver. Type: object Required: false Example spec : receivers : email : to : myteam@nav.no slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager sms : recipients : \"12345678\" send_resolved : false receivers.email \u00a7 Alerts via e-mails Type: object Required: false Example spec : receivers : email : to : myteam@nav.no receivers.email.send_resolved \u00a7 Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: false receivers.email.to \u00a7 Type: string Required: true Example spec : receivers : email : to : myteam@nav.no receivers.slack \u00a7 Slack notifications are sent via Slack webhooks. Type: object Required: false Example spec : receivers : slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager receivers.slack.channel \u00a7 The channel or user to send notifications to. Can be specified with and without # . Type: string Required: true Example spec : receivers : slack : channel : '#alert-channel' receivers.slack.icon_emoji \u00a7 Emoji to use as the icon for this message Type: string Required: false Example spec : receivers : slack : icon_emoji : ':chart_with_upwards_trend:' receivers.slack.icon_url \u00a7 URL to an image to use as the icon for this message Type: string Required: false Example spec : receivers : slack : icon_url : http://lorempixel.com/48/48 receivers.slack.prependText \u00a7 Text to prepend every Slack message with severity danger . Type: string Required: false Example spec : receivers : slack : prependText : Oh noes! receivers.slack.send_resolved \u00a7 Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: true Example spec : receivers : slack : send_resolved : true receivers.slack.username \u00a7 Set your bot's user name. Type: string Required: false Example spec : receivers : slack : username : Alertmanager receivers.sms \u00a7 Alerts via SMS Type: object Required: false Example spec : receivers : sms : recipients : \"12345678\" send_resolved : false receivers.sms.recipients \u00a7 Type: string Required: true Example spec : receivers : sms : recipients : \"12345678\" receivers.sms.send_resolved \u00a7 Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: true Example spec : receivers : sms : send_resolved : false route \u00a7 Type: object Required: false Example spec : route : group_by : - label - name groupInterval : 5m groupWait : 30s repeatInterval : 3h route.groupInterval \u00a7 How long to wait before sending a notification about new alerts that are added to a group of alerts for which an initial notification has already been sent. Type: string Required: false Default value: 5m Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : groupInterval : 5m route.groupWait \u00a7 How long to initially wait to send a notification for a group of alerts. Allows to wait for an inhibiting alert to arrive or collect more initial alerts for the same group. Type: string Required: false Default value: 10s Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : groupWait : 30s route.group_by \u00a7 The labels by which incoming alerts are grouped together. Type: array Required: false Example spec : route : group_by : - label - name route.repeatInterval \u00a7 How long to wait before sending a notification again if it has already been sent successfully for an alert. Type: string Required: false Default value: 1h Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : repeatInterval : 3h","title":"Reference"},{"location":"observability/alerts/reference/#nais-alert-reference","text":"This document describes all possible configuration values in the Alert spec, commonly known as the alert.yaml file.","title":"NAIS Alert reference"},{"location":"observability/alerts/reference/#alerts","text":"Type: array Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts"},{"location":"observability/alerts/reference/#alertsaction","text":"What human actions are needed to resolve or investigate this alert. Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].action"},{"location":"observability/alerts/reference/#alertsalert","text":"The name of the alert. Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].alert"},{"location":"observability/alerts/reference/#alertsdescription","text":"Simple description of the triggered alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].description"},{"location":"observability/alerts/reference/#alertsdocumentation","text":"URL for documentation for this alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].documentation"},{"location":"observability/alerts/reference/#alertsexpr","text":"Prometheus expression that triggers an alert. Explore expressions in the Prometheus -interface Type: string Required: true Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].expr"},{"location":"observability/alerts/reference/#alertsfor","text":"Duration before the alert should trigger. Type: string Required: true Pattern: ^\\d+[smhdwy]$ Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].for"},{"location":"observability/alerts/reference/#alertsseverity","text":"Alert level for Slack messages. Type: string Required: false Default value: danger Pattern: ^$|good|warning|danger|#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3}) Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].severity"},{"location":"observability/alerts/reference/#alertssla","text":"Time before a human should resolve the alert. Type: string Required: false Example spec : alerts : - action : kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger alert : applikasjon nede description : App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }} documentation : https://doc.nais.io/observability/alerts/ expr : kube_deployment_status_replicas_available{deployment=\"<appname>\"} > 0 for : 2m severity : danger sla : Mellom 8 og 16","title":"alerts[].sla"},{"location":"observability/alerts/reference/#inhibitrules","text":"A list of inhibit rules. An inhibition rule mutes an alert (target) matching a set of matchers when an alert (source) exists that matches another set of matchers. Both target and source alerts must have the same label values for the label names in the labels list. Relevant information: https://prometheus.io/docs/alerting/latest/configuration/#inhibit_rule Type: array Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules"},{"location":"observability/alerts/reference/#inhibitruleslabels","text":"Labels that must have an equal value in the source and target alert for the inhibition to take effect. Type: array Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules[].labels"},{"location":"observability/alerts/reference/#inhibitrulessources","text":"Matchers for which one or more alerts have to exist for the inhibition to take effect. These are key/value pairs. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules[].sources"},{"location":"observability/alerts/reference/#inhibitrulessourcesregex","text":"Regex matchers for which one or more alerts have to exist for the inhibition to take effect. These are key/value pairs, where the value can be a regex. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules[].sourcesRegex"},{"location":"observability/alerts/reference/#inhibitrulestargets","text":"Matchers that have to be fulfilled in the alerts to be muted. These are key/value pairs. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules[].targets"},{"location":"observability/alerts/reference/#inhibitrulestargetsregex","text":"Regex matchers that have to be fulfilled in the alerts to be muted. These are key/value pairs, where the value can be a regex. Type: object Required: false Example spec : inhibitRules : - labels : - label - name sources : key : value sourcesRegex : key : value(.)? targets : key : value targetsRegex : key : value(.)+","title":"inhibitRules[].targetsRegex"},{"location":"observability/alerts/reference/#receivers","text":"A list of notification recievers. You can use one or more of: e-mail, slack, sms. There needs to be at least one receiver. Type: object Required: false Example spec : receivers : email : to : myteam@nav.no slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager sms : recipients : \"12345678\" send_resolved : false","title":"receivers"},{"location":"observability/alerts/reference/#receiversemail","text":"Alerts via e-mails Type: object Required: false Example spec : receivers : email : to : myteam@nav.no","title":"receivers.email"},{"location":"observability/alerts/reference/#receiversemailsend_resolved","text":"Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: false","title":"receivers.email.send_resolved"},{"location":"observability/alerts/reference/#receiversemailto","text":"Type: string Required: true Example spec : receivers : email : to : myteam@nav.no","title":"receivers.email.to"},{"location":"observability/alerts/reference/#receiversslack","text":"Slack notifications are sent via Slack webhooks. Type: object Required: false Example spec : receivers : slack : channel : '#alert-channel' icon_emoji : ':chart_with_upwards_trend:' icon_url : http://lorempixel.com/48/48 prependText : Oh noes! send_resolved : true username : Alertmanager","title":"receivers.slack"},{"location":"observability/alerts/reference/#receiversslackchannel","text":"The channel or user to send notifications to. Can be specified with and without # . Type: string Required: true Example spec : receivers : slack : channel : '#alert-channel'","title":"receivers.slack.channel"},{"location":"observability/alerts/reference/#receiversslackicon_emoji","text":"Emoji to use as the icon for this message Type: string Required: false Example spec : receivers : slack : icon_emoji : ':chart_with_upwards_trend:'","title":"receivers.slack.icon_emoji"},{"location":"observability/alerts/reference/#receiversslackicon_url","text":"URL to an image to use as the icon for this message Type: string Required: false Example spec : receivers : slack : icon_url : http://lorempixel.com/48/48","title":"receivers.slack.icon_url"},{"location":"observability/alerts/reference/#receiversslackprependtext","text":"Text to prepend every Slack message with severity danger . Type: string Required: false Example spec : receivers : slack : prependText : Oh noes!","title":"receivers.slack.prependText"},{"location":"observability/alerts/reference/#receiversslacksend_resolved","text":"Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: true Example spec : receivers : slack : send_resolved : true","title":"receivers.slack.send_resolved"},{"location":"observability/alerts/reference/#receiversslackusername","text":"Set your bot's user name. Type: string Required: false Example spec : receivers : slack : username : Alertmanager","title":"receivers.slack.username"},{"location":"observability/alerts/reference/#receiverssms","text":"Alerts via SMS Type: object Required: false Example spec : receivers : sms : recipients : \"12345678\" send_resolved : false","title":"receivers.sms"},{"location":"observability/alerts/reference/#receiverssmsrecipients","text":"Type: string Required: true Example spec : receivers : sms : recipients : \"12345678\"","title":"receivers.sms.recipients"},{"location":"observability/alerts/reference/#receiverssmssend_resolved","text":"Whether or not to notify about resolved alerts. Type: boolean Required: false Default value: true Example spec : receivers : sms : send_resolved : false","title":"receivers.sms.send_resolved"},{"location":"observability/alerts/reference/#route","text":"Type: object Required: false Example spec : route : group_by : - label - name groupInterval : 5m groupWait : 30s repeatInterval : 3h","title":"route"},{"location":"observability/alerts/reference/#routegroupinterval","text":"How long to wait before sending a notification about new alerts that are added to a group of alerts for which an initial notification has already been sent. Type: string Required: false Default value: 5m Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : groupInterval : 5m","title":"route.groupInterval"},{"location":"observability/alerts/reference/#routegroupwait","text":"How long to initially wait to send a notification for a group of alerts. Allows to wait for an inhibiting alert to arrive or collect more initial alerts for the same group. Type: string Required: false Default value: 10s Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : groupWait : 30s","title":"route.groupWait"},{"location":"observability/alerts/reference/#routegroup_by","text":"The labels by which incoming alerts are grouped together. Type: array Required: false Example spec : route : group_by : - label - name","title":"route.group_by"},{"location":"observability/alerts/reference/#routerepeatinterval","text":"How long to wait before sending a notification again if it has already been sent successfully for an alert. Type: string Required: false Default value: 1h Pattern: ((([0-9]+)y)?(([0-9]+)w)?(([0-9]+)d)?(([0-9]+)h)?(([0-9]+)m)?(([0-9]+)s)?(([0-9]+)ms)?|0) Example spec : route : repeatInterval : 3h","title":"route.repeatInterval"},{"location":"observability/logs/","text":"Logs \u00a7 Logging \u00a7 Configure your application to log to console (stdout/stderr), it will be scraped by FluentD running inside the cluster and sent to Elasticsearch and made available via Kibana . Visit our Kibana at logs.adeo.no . If you want more information than just the log message (loglevel, MDC, etc), you should log in JSON format; the fields you provide will then be indexed. Gain access to logs.adeo.no \u00a7 In order to get access to logs.adeo.no you need to have the correct access rights added to your AD account. This can be requested through your Personnal Manager. These permissions will give you access: 0000-GA-Logganalyse 0000-GA-Logganalyse_FullVerdikjede_Prod 0000-GA-Logganalyse_FullVerdikjede_UTQ If you're unsure if you already have these permission, or some of them, you can check what permission you currently have at igruppe.adeo.no Secure logs \u00a7 Some applications have logs with information that should not be stored with the normal application logs. To support this a directory for these logs can be mounted in the application, and the content of logs written here will be transferred to separate indices in Elasticsearch. Enabling secure logs \u00a7 Secure logs can be enabled by setting the secureLogs.enabled flag in the application resource. See the nais manifest specification . Log files \u00a7 With secure logs enabled a directory /secure-logs/ will be mounted in the application container. Every *.log file in this directory will be monitored and the content transferred to Elasticsearch. Make sure that these files are readable for the log shipper (the process runs as uid/gid 1065). The /secure-logs/ directory has a size limit of 128Mb, and it's the application responsibility to ensure that this limit is not exceeded. If the limit is exceeded the application pod will be evicted and restarted. Use log rotation on file size to avoid this. Log configuration \u00a7 Log files should be in JSON format as the normal application logs. Here is an example configuration of JSON logging with a size based rolling file appender in Logback: <appender name=\"secureLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>/secure-logs/secure.log</file> <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\"> <fileNamePattern>/secure-logs/secure.log.%i</fileNamePattern> <minIndex>1</minIndex> <maxIndex>1</maxIndex> </rollingPolicy> <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"> <maxFileSize>50MB</maxFileSize> </triggeringPolicy> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\" /> </appender> See logging examples for more information on log configuration. Non-JSON logs \u00a7 If the logging framework used doesn't support JSON logging, it is also possible to use multiline logs in this format: <iso8601 timestamp> <log level> <message> <message cont.> <message cont.> Files on this format must be named *.mlog . Sending logs with HTTP \u00a7 If you do not want to have these logs as files in the pod, it is also possible to use HTTP to write logs. POST your log entry as JSON to http://localhost:19880 curl -X POST -d '{\"log\":\"hello world\",\"field1\":\"value1\"}' -H 'Content-Type: application/json' http://localhost:19880/ Audit logs \u00a7 Most applications where a user processes data related to another user need to log audit statements, detailing which user did what action on which subject. These logs need to follow a specific format and be accessible by ArcSight. See naudit for how to set up the logging, and details on the log format. Overview \u00a7 Gaining access in kibana \u00a7 Once everything is configured, your secure logs will be sent to the tjenestekall-* index in kibana. To gain access to these logs, you need to do the following: 1 Create an AD-group \u00a7 To make sure you gain access to the proper logs, you need an AD-group connected to the nais-team. So the first thing you do is create this group. Go to Porten (service desk) and click Melde sak til IT . The follow the template below. 2 Connect the AD group to your team in Kibana \u00a7 Your app produces logs based on nais-team. So in order for you to get access to the logs, the AD-group must be linked with the nais team, so whoever is in the AD-group can read all logs produced by apps belonging to the nais-team. The easiest way to achieve this is to ask in the #atom slack channel. And ask them to connect AD-group X to team Y. 3 Put people into the AD-group \u00a7 This must be done by \"identansvarlig\". For NAV-IT employees, this is nav.it.identhandtering@nav.no . Send them an email and ask for access with a CC to whoever is your superior. For everyone else, the team lead or who ever is their superior should know. What can go wrong? \u00a7 Basically, the one thing that can go wrong here is that the AD-group is not registered in \"identrutinen\". If this happens, the group cannot be found by \"identansvarlig\". If this happens, make a new JIRA-ticket to the same people and tell them to transfer the group. Sadly this can take a few days.","title":"Logs Overview"},{"location":"observability/logs/#logs","text":"","title":"Logs"},{"location":"observability/logs/#logging","text":"Configure your application to log to console (stdout/stderr), it will be scraped by FluentD running inside the cluster and sent to Elasticsearch and made available via Kibana . Visit our Kibana at logs.adeo.no . If you want more information than just the log message (loglevel, MDC, etc), you should log in JSON format; the fields you provide will then be indexed.","title":"Logging"},{"location":"observability/logs/#gain-access-to-logsadeono","text":"In order to get access to logs.adeo.no you need to have the correct access rights added to your AD account. This can be requested through your Personnal Manager. These permissions will give you access: 0000-GA-Logganalyse 0000-GA-Logganalyse_FullVerdikjede_Prod 0000-GA-Logganalyse_FullVerdikjede_UTQ If you're unsure if you already have these permission, or some of them, you can check what permission you currently have at igruppe.adeo.no","title":"Gain access to logs.adeo.no"},{"location":"observability/logs/#secure-logs","text":"Some applications have logs with information that should not be stored with the normal application logs. To support this a directory for these logs can be mounted in the application, and the content of logs written here will be transferred to separate indices in Elasticsearch.","title":"Secure logs"},{"location":"observability/logs/#enabling-secure-logs","text":"Secure logs can be enabled by setting the secureLogs.enabled flag in the application resource. See the nais manifest specification .","title":"Enabling secure logs"},{"location":"observability/logs/#log-files","text":"With secure logs enabled a directory /secure-logs/ will be mounted in the application container. Every *.log file in this directory will be monitored and the content transferred to Elasticsearch. Make sure that these files are readable for the log shipper (the process runs as uid/gid 1065). The /secure-logs/ directory has a size limit of 128Mb, and it's the application responsibility to ensure that this limit is not exceeded. If the limit is exceeded the application pod will be evicted and restarted. Use log rotation on file size to avoid this.","title":"Log files"},{"location":"observability/logs/#log-configuration","text":"Log files should be in JSON format as the normal application logs. Here is an example configuration of JSON logging with a size based rolling file appender in Logback: <appender name=\"secureLog\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>/secure-logs/secure.log</file> <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\"> <fileNamePattern>/secure-logs/secure.log.%i</fileNamePattern> <minIndex>1</minIndex> <maxIndex>1</maxIndex> </rollingPolicy> <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"> <maxFileSize>50MB</maxFileSize> </triggeringPolicy> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\" /> </appender> See logging examples for more information on log configuration.","title":"Log configuration"},{"location":"observability/logs/#non-json-logs","text":"If the logging framework used doesn't support JSON logging, it is also possible to use multiline logs in this format: <iso8601 timestamp> <log level> <message> <message cont.> <message cont.> Files on this format must be named *.mlog .","title":"Non-JSON logs"},{"location":"observability/logs/#sending-logs-with-http","text":"If you do not want to have these logs as files in the pod, it is also possible to use HTTP to write logs. POST your log entry as JSON to http://localhost:19880 curl -X POST -d '{\"log\":\"hello world\",\"field1\":\"value1\"}' -H 'Content-Type: application/json' http://localhost:19880/","title":"Sending logs with HTTP"},{"location":"observability/logs/#audit-logs","text":"Most applications where a user processes data related to another user need to log audit statements, detailing which user did what action on which subject. These logs need to follow a specific format and be accessible by ArcSight. See naudit for how to set up the logging, and details on the log format.","title":"Audit logs"},{"location":"observability/logs/#overview","text":"","title":"Overview"},{"location":"observability/logs/#gaining-access-in-kibana","text":"Once everything is configured, your secure logs will be sent to the tjenestekall-* index in kibana. To gain access to these logs, you need to do the following:","title":"Gaining access in kibana"},{"location":"observability/logs/#1-create-an-ad-group","text":"To make sure you gain access to the proper logs, you need an AD-group connected to the nais-team. So the first thing you do is create this group. Go to Porten (service desk) and click Melde sak til IT . The follow the template below.","title":"1 Create an AD-group"},{"location":"observability/logs/#2-connect-the-ad-group-to-your-team-in-kibana","text":"Your app produces logs based on nais-team. So in order for you to get access to the logs, the AD-group must be linked with the nais team, so whoever is in the AD-group can read all logs produced by apps belonging to the nais-team. The easiest way to achieve this is to ask in the #atom slack channel. And ask them to connect AD-group X to team Y.","title":"2 Connect the AD group to your team in Kibana"},{"location":"observability/logs/#3-put-people-into-the-ad-group","text":"This must be done by \"identansvarlig\". For NAV-IT employees, this is nav.it.identhandtering@nav.no . Send them an email and ask for access with a CC to whoever is your superior. For everyone else, the team lead or who ever is their superior should know.","title":"3 Put people into the AD-group"},{"location":"observability/logs/#what-can-go-wrong","text":"Basically, the one thing that can go wrong here is that the AD-group is not registered in \"identrutinen\". If this happens, the group cannot be found by \"identansvarlig\". If this happens, make a new JIRA-ticket to the same people and tell them to transfer the group. Sadly this can take a few days.","title":"What can go wrong?"},{"location":"observability/logs/examples/","text":"Examples \u00a7 SLF4J \u00a7 pom.xml \u00a7 <dependencies> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>1.2.3</version> </dependency> <dependency> <groupId>net.logstash.logback</groupId> <artifactId>logstash-logback-encoder</artifactId> <version>5.1</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.7.25</version> </dependency> </dependencies> logback.xml \u00a7 <configuration> <appender name=\"stdout_json\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\" /> </appender> <root level=\"info\"> <appender-ref ref=\"stdout_json\" /> </root> </configuration> Issues with long log messages \u00a7 The max log message size in Docker is 16KB, so if it will be split into parts if it's bigger. Fluentd dosen't support this, so we recommend making stack traces shorter. Read more about this on github.com/logstash . <configuration> <appender name=\"stdout_json\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"> <throwableConverter class=\"net.logstash.logback.stacktrace.ShortenedThrowableConverter\"> <maxDepthPerThrowable>30</maxDepthPerThrowable> <exclude>java\\.util\\.concurrent\\..*</exclude> <exclude>org\\.apache\\.tomcat\\..*</exclude> <exclude>org\\.apache\\.coyote\\..*</exclude> <exclude>org\\.apache\\.catalina\\..*</exclude> <exclude>org\\.springframework\\.web\\..*</exclude> </throwableConverter> </encoder> </appender> <root level=\"info\"> <appender-ref ref=\"stdout_json\"/> </root> </configuration> Log4j2 \u00a7 pom.xml \u00a7 <dependencies> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-layout-template-json</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.7.25</version> </dependency> </dependencies> log4j2.xml \u00a7 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status=\"INFO\"> <Appenders> <Console name=\"ConsoleAppender\" target=\"SYSTEM_OUT\"> <JsonTemplateLayout templateUri=\"classpath:LogstashJsonEventLayoutV1.json\" stackTraceEnabled=\"true\"/> </Console> </Appenders> <Loggers> <Root level=\"info\"> <AppenderRef ref=\"ConsoleAppender\"/> </Root> </Loggers> </Configuration>","title":"Examples"},{"location":"observability/logs/examples/#examples","text":"","title":"Examples"},{"location":"observability/logs/examples/#slf4j","text":"","title":"SLF4J"},{"location":"observability/logs/examples/#pomxml","text":"<dependencies> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>1.2.3</version> </dependency> <dependency> <groupId>net.logstash.logback</groupId> <artifactId>logstash-logback-encoder</artifactId> <version>5.1</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.7.25</version> </dependency> </dependencies>","title":"pom.xml"},{"location":"observability/logs/examples/#logbackxml","text":"<configuration> <appender name=\"stdout_json\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\" /> </appender> <root level=\"info\"> <appender-ref ref=\"stdout_json\" /> </root> </configuration>","title":"logback.xml"},{"location":"observability/logs/examples/#issues-with-long-log-messages","text":"The max log message size in Docker is 16KB, so if it will be split into parts if it's bigger. Fluentd dosen't support this, so we recommend making stack traces shorter. Read more about this on github.com/logstash . <configuration> <appender name=\"stdout_json\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"> <throwableConverter class=\"net.logstash.logback.stacktrace.ShortenedThrowableConverter\"> <maxDepthPerThrowable>30</maxDepthPerThrowable> <exclude>java\\.util\\.concurrent\\..*</exclude> <exclude>org\\.apache\\.tomcat\\..*</exclude> <exclude>org\\.apache\\.coyote\\..*</exclude> <exclude>org\\.apache\\.catalina\\..*</exclude> <exclude>org\\.springframework\\.web\\..*</exclude> </throwableConverter> </encoder> </appender> <root level=\"info\"> <appender-ref ref=\"stdout_json\"/> </root> </configuration>","title":"Issues with long log messages"},{"location":"observability/logs/examples/#log4j2","text":"","title":"Log4j2"},{"location":"observability/logs/examples/#pomxml_1","text":"<dependencies> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-layout-template-json</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>2.15.0</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.7.25</version> </dependency> </dependencies>","title":"pom.xml"},{"location":"observability/logs/examples/#log4j2xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status=\"INFO\"> <Appenders> <Console name=\"ConsoleAppender\" target=\"SYSTEM_OUT\"> <JsonTemplateLayout templateUri=\"classpath:LogstashJsonEventLayoutV1.json\" stackTraceEnabled=\"true\"/> </Console> </Appenders> <Loggers> <Root level=\"info\"> <AppenderRef ref=\"ConsoleAppender\"/> </Root> </Loggers> </Configuration>","title":"log4j2.xml"},{"location":"persistence/bigquery/","text":"Bigquery Datasets \u00a7 Info This feature is only available in GCP clusters. NAIS Application yaml manifest options \u00a7 Full documentation of all available options can be found over at: spec.gcp.bigQueryDatasets[] . Example of an application using a nais.yaml provisioned BigQuery Dataset can be found here: testapp . Minimal Working Example \u00a7 See below for a minimal working example for a NAIS Application manifest apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... gcp : bigQueryDatasets : - name : my_bigquery_dataset permission : READWRITE Connecting to the Dataset \u00a7 When connecting your BigQuery client you need to specify the project ID and the dataset ID. The project ID is available in the GCP_TEAM_PROJECT_ID environment variable. There's no automatic environment variable for the dataset ID. val projectId = System . getenv ( \"GCP_TEAM_PROJECT_ID\" ) val datasetId = \"my_bigquery_dataset\" val bigQueryClient = BigQueryClient . create ( projectId , datasetId ) Caveats to be aware of \u00a7 Automatic Deletion Once a BigQuery Dataset is provisioned, it will not be automatically deleted - unless one explicitly sets spec.gcp.bigQueryDatasets[].cascadingDelete to true . Clean up is done by deleting application resource and deleting the BigQuery instance directly in console.cloud.google.com . When there exist no tables in the specified BigQuery Dataset, deleting the \"nais application\" will delete the whole BigQuery Dataset, even if spec.gcp.bigQueryDatasets[].cascadingDelete is set to false . Unique names The name of your Dataset must be unique within your team's GCP project. Updates/Immutability The NAIS Manifest does not currently support updating any setting of existing BigQuery Datasets. Thus, if you want a read-connection to a already-created BigQuery Dataset, take a look at nais/dp . K8s resource naming Since Kubernetes does not permit underscores ( _ ) in the names of any K8s resource, any underscores will be converted to hyphens ( - ). Example with all configuration options \u00a7 See full example . Troubleshooting \u00a7 If you have problems getting your bucket up and running, check errors in the event log: kubectl describe bigquerydataset my-bigquery-dataset","title":"Google BigQuery"},{"location":"persistence/bigquery/#bigquery-datasets","text":"Info This feature is only available in GCP clusters.","title":"Bigquery Datasets"},{"location":"persistence/bigquery/#nais-application-yaml-manifest-options","text":"Full documentation of all available options can be found over at: spec.gcp.bigQueryDatasets[] . Example of an application using a nais.yaml provisioned BigQuery Dataset can be found here: testapp .","title":"NAIS Application yaml manifest options"},{"location":"persistence/bigquery/#minimal-working-example","text":"See below for a minimal working example for a NAIS Application manifest apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... gcp : bigQueryDatasets : - name : my_bigquery_dataset permission : READWRITE","title":"Minimal Working Example"},{"location":"persistence/bigquery/#connecting-to-the-dataset","text":"When connecting your BigQuery client you need to specify the project ID and the dataset ID. The project ID is available in the GCP_TEAM_PROJECT_ID environment variable. There's no automatic environment variable for the dataset ID. val projectId = System . getenv ( \"GCP_TEAM_PROJECT_ID\" ) val datasetId = \"my_bigquery_dataset\" val bigQueryClient = BigQueryClient . create ( projectId , datasetId )","title":"Connecting to the Dataset"},{"location":"persistence/bigquery/#caveats-to-be-aware-of","text":"Automatic Deletion Once a BigQuery Dataset is provisioned, it will not be automatically deleted - unless one explicitly sets spec.gcp.bigQueryDatasets[].cascadingDelete to true . Clean up is done by deleting application resource and deleting the BigQuery instance directly in console.cloud.google.com . When there exist no tables in the specified BigQuery Dataset, deleting the \"nais application\" will delete the whole BigQuery Dataset, even if spec.gcp.bigQueryDatasets[].cascadingDelete is set to false . Unique names The name of your Dataset must be unique within your team's GCP project. Updates/Immutability The NAIS Manifest does not currently support updating any setting of existing BigQuery Datasets. Thus, if you want a read-connection to a already-created BigQuery Dataset, take a look at nais/dp . K8s resource naming Since Kubernetes does not permit underscores ( _ ) in the names of any K8s resource, any underscores will be converted to hyphens ( - ).","title":"Caveats to be aware of"},{"location":"persistence/bigquery/#example-with-all-configuration-options","text":"See full example .","title":"Example with all configuration options"},{"location":"persistence/bigquery/#troubleshooting","text":"If you have problems getting your bucket up and running, check errors in the event log: kubectl describe bigquerydataset my-bigquery-dataset","title":"Troubleshooting"},{"location":"persistence/buckets/","text":"Buckets \u00a7 Warning There is no automatic backup enabled for buckets. You can request a Google Cloud Storage bucket through the NAIS manifest. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... gcp : buckets : - name : mybucket retentionPeriodDays : 30 lifecycleCondition : age : 7 createdBefore : 2020-01-01 numNewerVersions : 2 withState : ANY Info Once a bucket is provisioned, it will not be automatically deleted unless one explicitly sets spec.gcp.buckets[].cascadingDelete to true . This means that any cleanup must be done manually. Bucket names must be globally unique across the entire Google infrastructure. This can cause provisioning problems if your bucket name is used by someone else. RetentionPeriodDays and lifecycleCondition must be set when creating the bucket as they cannot be changed after the bucket is created. RetentionPeriodDays is set in number of days, if not set; no retention policy will be set and files can be deleted by application or manually from the point they are created. LifecycleCondition can be set to verify which files/objects are subject to permanent deletion based on the conditions set. age specifies days it has existed in the bucket before it can be deleted. createdBefore specifies a date all files created before this date can be deleted. numNewerVersions specifies the number of revisions that must be kept, all older revisions can be deleted. withState specifies which state (LIVE, ARCHIVED, ANY) the object must have before being subject to permanent deletion. The latter two are subject to object versioning being set on objects, as unversioned objects do not have multiple versions and have state value LIVE. If you have problems getting your bucket up and running, check errors in the event log: kubectl describe storagebucket mybucket An example application using workflow identity to access a bucket: testapp","title":"Google Cloud Storage/Buckets"},{"location":"persistence/buckets/#buckets","text":"Warning There is no automatic backup enabled for buckets. You can request a Google Cloud Storage bucket through the NAIS manifest. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... gcp : buckets : - name : mybucket retentionPeriodDays : 30 lifecycleCondition : age : 7 createdBefore : 2020-01-01 numNewerVersions : 2 withState : ANY Info Once a bucket is provisioned, it will not be automatically deleted unless one explicitly sets spec.gcp.buckets[].cascadingDelete to true . This means that any cleanup must be done manually. Bucket names must be globally unique across the entire Google infrastructure. This can cause provisioning problems if your bucket name is used by someone else. RetentionPeriodDays and lifecycleCondition must be set when creating the bucket as they cannot be changed after the bucket is created. RetentionPeriodDays is set in number of days, if not set; no retention policy will be set and files can be deleted by application or manually from the point they are created. LifecycleCondition can be set to verify which files/objects are subject to permanent deletion based on the conditions set. age specifies days it has existed in the bucket before it can be deleted. createdBefore specifies a date all files created before this date can be deleted. numNewerVersions specifies the number of revisions that must be kept, all older revisions can be deleted. withState specifies which state (LIVE, ARCHIVED, ANY) the object must have before being subject to permanent deletion. The latter two are subject to object versioning being set on objects, as unversioned objects do not have multiple versions and have state value LIVE. If you have problems getting your bucket up and running, check errors in the event log: kubectl describe storagebucket mybucket An example application using workflow identity to access a bucket: testapp","title":"Buckets"},{"location":"persistence/elastic-search/","text":"Elastic Search \u00a7 Warning Aiven is migrating to OpenSearch, so new instances of ElasticSearch should be avoided. Use OpenSearch instead. The NAIS platform offers Elastic Search via Aiven . Get your own \u00a7 As there are few teams that need an Elastic Search instance we use a IaC-repo to provision each instance. Head over to aiven-iac to learn how to get your own instance. To make it easier for you, when creating the instance, we will also create four users with read, write, readwrite and admin access. Access from Nais-app \u00a7 If you need access from an application, use the following nais.yaml-reference . When an application requesting an elastic instance is deployed, credentials will be provided as environment variables. The service URI for Elastic is also available. If you specify elastic.access , the credentials will be for the user with those access rights. The available access levels are: 'admin', 'read', 'write', 'readwrite' If not specified, the credentials will be for a user with read access. Environment variable Description ELASTIC_USERNAME Username ELASTIC_PASSWORD Password ELASTIC_URI Service URI Access from laptop \u00a7 With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given. Kibana \u00a7 The URL for Kibana is the same as the Elastic instance, but using port 443 (regular https). Support \u00a7 We do not offer support on Elastic Search as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack. Alerts \u00a7 We recommend that you set up your own alerts so that you can react to problems in your Elastic instance. Aiven uses Telegraf to collect and present metrics, so available metrics can be found in the Telegraf documentation . We have configured our Prometheus instances in GCP to scrape the Elastic clusters in Aiven, so these metrics should be available in Grafana. New Elastic clusters currently need to be manually added to the Prometheus config, so if you can't find your cluster, please poke us. Particularly relevant input plugins are: elasticsearch mem cpu diskio disk O'Reilly has a useful article about metrics to watch in an Elastic cluster. Awesome prometheus alerts is another good source of alerts to look out for. Some metrics that might be useful to watch, based on above article (We need feedback on this list, as we have no practical experience to lean on): elasticsearch_cluster_health_status_code elasticsearch_cluster_health_active_shards elasticsearch_cluster_health_initializing_shards elasticsearch_cluster_health_relocating_shards elasticsearch_cluster_health_unassigned_shards elasticsearch_jvm_mem_heap_used_percent elasticsearch_jvm_gc_collectors_old_collection_time_in_millis elasticsearch_jvm_gc_collectors_young_collection_time_in_millis elasticsearch_jvm_mem_pools_old_used_in_byte vs. elasticsearch_jvm_mem_pools_old_max_in_bytes elasticsearch_jvm_mem_pools_survivor_used_in_byte vs. elasticsearch_jvm_mem_pools_survivor_max_in_bytes elasticsearch_jvm_mem_pools_young_used_in_byte vs. elasticsearch_jvm_mem_pools_young_max_in_bytes elasticsearch_indices_search_query_time_in_millis elasticsearch_indices_fielddata_evictions elasticsearch_indices_fielddata_memory_size_in_bytes elasticsearch_indices_indexing_index_time_in_millis elasticsearch_indices_indexing_index_total elasticsearch_indices_merges_total_time_in_millis elasticsearch_indices_store_size_in_bytes cpu_usage_user diskio_weighted_io_time","title":"ElasticSearch"},{"location":"persistence/elastic-search/#elastic-search","text":"Warning Aiven is migrating to OpenSearch, so new instances of ElasticSearch should be avoided. Use OpenSearch instead. The NAIS platform offers Elastic Search via Aiven .","title":"Elastic Search"},{"location":"persistence/elastic-search/#get-your-own","text":"As there are few teams that need an Elastic Search instance we use a IaC-repo to provision each instance. Head over to aiven-iac to learn how to get your own instance. To make it easier for you, when creating the instance, we will also create four users with read, write, readwrite and admin access.","title":"Get your own"},{"location":"persistence/elastic-search/#access-from-nais-app","text":"If you need access from an application, use the following nais.yaml-reference . When an application requesting an elastic instance is deployed, credentials will be provided as environment variables. The service URI for Elastic is also available. If you specify elastic.access , the credentials will be for the user with those access rights. The available access levels are: 'admin', 'read', 'write', 'readwrite' If not specified, the credentials will be for a user with read access. Environment variable Description ELASTIC_USERNAME Username ELASTIC_PASSWORD Password ELASTIC_URI Service URI","title":"Access from Nais-app"},{"location":"persistence/elastic-search/#access-from-laptop","text":"With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given.","title":"Access from laptop"},{"location":"persistence/elastic-search/#kibana","text":"The URL for Kibana is the same as the Elastic instance, but using port 443 (regular https).","title":"Kibana"},{"location":"persistence/elastic-search/#support","text":"We do not offer support on Elastic Search as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack.","title":"Support"},{"location":"persistence/elastic-search/#alerts","text":"We recommend that you set up your own alerts so that you can react to problems in your Elastic instance. Aiven uses Telegraf to collect and present metrics, so available metrics can be found in the Telegraf documentation . We have configured our Prometheus instances in GCP to scrape the Elastic clusters in Aiven, so these metrics should be available in Grafana. New Elastic clusters currently need to be manually added to the Prometheus config, so if you can't find your cluster, please poke us. Particularly relevant input plugins are: elasticsearch mem cpu diskio disk O'Reilly has a useful article about metrics to watch in an Elastic cluster. Awesome prometheus alerts is another good source of alerts to look out for. Some metrics that might be useful to watch, based on above article (We need feedback on this list, as we have no practical experience to lean on): elasticsearch_cluster_health_status_code elasticsearch_cluster_health_active_shards elasticsearch_cluster_health_initializing_shards elasticsearch_cluster_health_relocating_shards elasticsearch_cluster_health_unassigned_shards elasticsearch_jvm_mem_heap_used_percent elasticsearch_jvm_gc_collectors_old_collection_time_in_millis elasticsearch_jvm_gc_collectors_young_collection_time_in_millis elasticsearch_jvm_mem_pools_old_used_in_byte vs. elasticsearch_jvm_mem_pools_old_max_in_bytes elasticsearch_jvm_mem_pools_survivor_used_in_byte vs. elasticsearch_jvm_mem_pools_survivor_max_in_bytes elasticsearch_jvm_mem_pools_young_used_in_byte vs. elasticsearch_jvm_mem_pools_young_max_in_bytes elasticsearch_indices_search_query_time_in_millis elasticsearch_indices_fielddata_evictions elasticsearch_indices_fielddata_memory_size_in_bytes elasticsearch_indices_indexing_index_time_in_millis elasticsearch_indices_indexing_index_total elasticsearch_indices_merges_total_time_in_millis elasticsearch_indices_store_size_in_bytes cpu_usage_user diskio_weighted_io_time","title":"Alerts"},{"location":"persistence/influxdb/","text":"Influxdb \u00a7 Aiven discontinues InfluxDB Aiven has informed us that they will discontinue support for InfluxDB. The timeline is yet to be confirmed, but could be as early as end-of-year 2022. Talk to us in NAIS before starting to use InfluxDB The NAIS platform offers Influxdb via Aiven . A typical use case is to store metrics from your app and visualize them in Grafana . Get your own instance \u00a7 As there are many teams that need Influxdb we use an IaC-repo to provision each instance. Head over to aiven-iac . Username and password \u00a7 For now we are manually distributing the username and password for each instance. There is only one user for Influxdb. Contact us in #pig-aiven to get your credentials. Default database name \u00a7 The default database name is defaultdb . Retention policies \u00a7 The default database is created with a default retention policy of 30 days. You might want to adjust this by e.g. creating a new default retention policy with 1 year retention: create retention policy \"365d\" on \"defaultdb\" duration 365d replication 1 shard duration 1w default Datasource in grafana.adeo.no \u00a7 Let us know in #pig-aiven if you want your Influxdb to be exposed at grafana.adeo.no. This means that everyone can access your data. Access from Nais-app \u00a7 If you need access from an application, you need to specify Inluxdb instance. See nais.yaml-reference . Loading CA-certificate \u00a7 Your application will also need an CA-certificate for your app to be able to connect to Aiven with SSL. The certificate will be loaded into your pod as an environment variable if you define a .spec.kafka.pool in your nais.yaml file. Access from laptop \u00a7 With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given. influx -username avnadmin -password foo -host influx-instancename-nav-dev.aivencloud.com -port 26482 -ssl PS: Remember to use Influxdb CLI pre v2. For example v1.8.3. Support \u00a7 We do not offer support on Influxdb as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack.","title":"InfluxDB"},{"location":"persistence/influxdb/#influxdb","text":"Aiven discontinues InfluxDB Aiven has informed us that they will discontinue support for InfluxDB. The timeline is yet to be confirmed, but could be as early as end-of-year 2022. Talk to us in NAIS before starting to use InfluxDB The NAIS platform offers Influxdb via Aiven . A typical use case is to store metrics from your app and visualize them in Grafana .","title":"Influxdb"},{"location":"persistence/influxdb/#get-your-own-instance","text":"As there are many teams that need Influxdb we use an IaC-repo to provision each instance. Head over to aiven-iac .","title":"Get your own instance"},{"location":"persistence/influxdb/#username-and-password","text":"For now we are manually distributing the username and password for each instance. There is only one user for Influxdb. Contact us in #pig-aiven to get your credentials.","title":"Username and password"},{"location":"persistence/influxdb/#default-database-name","text":"The default database name is defaultdb .","title":"Default database name"},{"location":"persistence/influxdb/#retention-policies","text":"The default database is created with a default retention policy of 30 days. You might want to adjust this by e.g. creating a new default retention policy with 1 year retention: create retention policy \"365d\" on \"defaultdb\" duration 365d replication 1 shard duration 1w default","title":"Retention policies"},{"location":"persistence/influxdb/#datasource-in-grafanaadeono","text":"Let us know in #pig-aiven if you want your Influxdb to be exposed at grafana.adeo.no. This means that everyone can access your data.","title":"Datasource in grafana.adeo.no"},{"location":"persistence/influxdb/#access-from-nais-app","text":"If you need access from an application, you need to specify Inluxdb instance. See nais.yaml-reference .","title":"Access from Nais-app"},{"location":"persistence/influxdb/#loading-ca-certificate","text":"Your application will also need an CA-certificate for your app to be able to connect to Aiven with SSL. The certificate will be loaded into your pod as an environment variable if you define a .spec.kafka.pool in your nais.yaml file.","title":"Loading CA-certificate"},{"location":"persistence/influxdb/#access-from-laptop","text":"With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given. influx -username avnadmin -password foo -host influx-instancename-nav-dev.aivencloud.com -port 26482 -ssl PS: Remember to use Influxdb CLI pre v2. For example v1.8.3.","title":"Access from laptop"},{"location":"persistence/influxdb/#support","text":"We do not offer support on Influxdb as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack.","title":"Support"},{"location":"persistence/mongodb-in-gcp/","text":"MongoDB in GKE \u00a7 Disclaimer Nais does not support MongoDB in GKE, however one can easily set up one manually. Your team is responsible for maintenance and upgrades. First create your MongoDB-instance with the following StatefulSet. The following example creates an instance with three replicas using a pre-existing storage-class. The storage-class is a cluster-wide resource. apiVersion : apps/v1 kind : StatefulSet metadata : name : my-mongodb namespace : myteam spec : serviceName : my-mongodb replicas : 3 selector : matchLabels : role : my-mongodb template : metadata : labels : role : my-mongodb environment : test replicaset : MainRepSet app : my-mongodb spec : terminationGracePeriodSeconds : 10 containers : - name : mongod-container image : mongo:5 command : - \"mongod\" - \"--bind_ip\" - \"0.0.0.0\" - \"--replSet\" - \"MainRepSet\" resources : requests : cpu : 0.2 memory : 1.25Gi ports : - containerPort : 27017 volumeMounts : - name : mongodb-persistent-storage-claim mountPath : /data/db volumeClaimTemplates : - metadata : name : mongodb-persistent-storage-claim annotations : volume.beta.kubernetes.io/storage-class : \"ssd-storage\" spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi Next, create a network policy to allow traffic from your application to MongoDB-instance. This is not handled by Naiserator, hence you will need an additional policy. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : labels : app : my-mongodb team : myteam name : my-mongodb namespace : myteam spec : egress : - to : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - namespaceSelector : {} podSelector : matchLabels : role : my-mongodb - namespaceSelector : {} podSelector : matchLabels : k8s-app : kube-dns - ipBlock : cidr : 0.0.0.0/0 except : - 10.6.0.0/15 - 172.16.0.0/12 - 192.168.0.0/16 ingress : - from : - namespaceSelector : matchLabels : name : nais podSelector : matchLabels : app : prometheus - from : - namespaceSelector : matchLabels : name : myteam podSelector : matchLabels : app : myapp - from : - namespaceSelector : {} podSelector : matchLabels : role : my-mongodb - from : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : tap - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : prometheus podSelector : matchLabels : role : my-mongodb policyTypes : - Ingress - Egress Then create a service for your MongoDB-instance. apiVersion : v1 kind : Service metadata : name : my-mongodb namespace : myteam labels : name : my-mongodb spec : ports : - port : 27017 targetPort : 27017 clusterIP : None selector : role : my-mongodb In your application-yaml, define an accessPolicy to allow traffic to the MongoDB-instance. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : myapp namespace : myteam labels : team : myteam spec : accessPolicy : outbound : rules : - application : my-mongodb env : - name : MONGODB_URL value : mongodb://my-mongodb-0.my-mongodb:27017,my-mongodb-1.my-mongodb:27017,my-mongodb-2.my-mongo:27017/deploy_log?replicaSet=MainRepSet Backup of MongoDB-instance \u00a7 Nais does not provide any backup solution for MongoDB-instances. However, one can easily set up cronjob for backup purposes. The following bash script will create a gzipped dump of my-mongodb-0, which then will be uploaded to the GCP storage bucket of your choosing. In production, the buckets will be archived on-premises as well. #!/usr/bin/env bash if ! kubectl exec -it -n aura my-mongodb-0 -c mongod-container -- mongodump --archive --gzip > ./dump.gz ; then echo \"failed to execute mongodump\" exit 1 fi backup_name = \"dump_ $( date + \"%Y-%m-%d_%H-%M\" ) .tgz\" if ! gsutil mv \"./dump.gz\" \"gs://<backup-bucket-name>/ $backup_name \" ; then echo \"failed to upload backup to bucket\" exit 1 fi rm dump.gz To set up your backup job you need a Naisjob bundling your bash script, a NetworkPolicy and RBAC resources as described below. apiVersion : nais.io/v1 kind : Naisjob metadata : labels : team : myteam name : my-backup namespace : myteam spec : activeDeadlineSeconds : 6000 backoffLimit : 5 failedJobsHistoryLimit : 2 gcp : buckets : - cascadingDelete : false name : my-backup-bucket retentionPeriodDays : 30 image : ghcr.io/navikt/my-backup:1.0 resources : limits : cpu : 200m memory : 128Mi requests : cpu : 100m memory : 64Mi restartPolicy : Never schedule : '0 */6 * * *' skipCaBundle : true successfulJobsHistoryLimit : 2 ttlSecondsAfterFinished : 60 --- apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : labels : app : my-backup team : myteam name : my-backup-apiserver namespace : myteam spec : egress : - to : - ipBlock : cidr : 172.16.0.2/32 podSelector : matchLabels : app : my-backup policyTypes : - Egress --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : my-backup namespace : myteam rules : - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : my-backup namespace : myteam roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : my-backup subjects : - namespace : myteam kind : ServiceAccount name : my-backup","title":"MongoDB in GCP"},{"location":"persistence/mongodb-in-gcp/#mongodb-in-gke","text":"Disclaimer Nais does not support MongoDB in GKE, however one can easily set up one manually. Your team is responsible for maintenance and upgrades. First create your MongoDB-instance with the following StatefulSet. The following example creates an instance with three replicas using a pre-existing storage-class. The storage-class is a cluster-wide resource. apiVersion : apps/v1 kind : StatefulSet metadata : name : my-mongodb namespace : myteam spec : serviceName : my-mongodb replicas : 3 selector : matchLabels : role : my-mongodb template : metadata : labels : role : my-mongodb environment : test replicaset : MainRepSet app : my-mongodb spec : terminationGracePeriodSeconds : 10 containers : - name : mongod-container image : mongo:5 command : - \"mongod\" - \"--bind_ip\" - \"0.0.0.0\" - \"--replSet\" - \"MainRepSet\" resources : requests : cpu : 0.2 memory : 1.25Gi ports : - containerPort : 27017 volumeMounts : - name : mongodb-persistent-storage-claim mountPath : /data/db volumeClaimTemplates : - metadata : name : mongodb-persistent-storage-claim annotations : volume.beta.kubernetes.io/storage-class : \"ssd-storage\" spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi Next, create a network policy to allow traffic from your application to MongoDB-instance. This is not handled by Naiserator, hence you will need an additional policy. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : labels : app : my-mongodb team : myteam name : my-mongodb namespace : myteam spec : egress : - to : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - namespaceSelector : {} podSelector : matchLabels : role : my-mongodb - namespaceSelector : {} podSelector : matchLabels : k8s-app : kube-dns - ipBlock : cidr : 0.0.0.0/0 except : - 10.6.0.0/15 - 172.16.0.0/12 - 192.168.0.0/16 ingress : - from : - namespaceSelector : matchLabels : name : nais podSelector : matchLabels : app : prometheus - from : - namespaceSelector : matchLabels : name : myteam podSelector : matchLabels : app : myapp - from : - namespaceSelector : {} podSelector : matchLabels : role : my-mongodb - from : - namespaceSelector : matchLabels : linkerd.io/is-control-plane : \"true\" - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : tap - from : - namespaceSelector : matchLabels : linkerd.io/extension : viz podSelector : matchLabels : component : prometheus podSelector : matchLabels : role : my-mongodb policyTypes : - Ingress - Egress Then create a service for your MongoDB-instance. apiVersion : v1 kind : Service metadata : name : my-mongodb namespace : myteam labels : name : my-mongodb spec : ports : - port : 27017 targetPort : 27017 clusterIP : None selector : role : my-mongodb In your application-yaml, define an accessPolicy to allow traffic to the MongoDB-instance. apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : myapp namespace : myteam labels : team : myteam spec : accessPolicy : outbound : rules : - application : my-mongodb env : - name : MONGODB_URL value : mongodb://my-mongodb-0.my-mongodb:27017,my-mongodb-1.my-mongodb:27017,my-mongodb-2.my-mongo:27017/deploy_log?replicaSet=MainRepSet","title":"MongoDB in GKE"},{"location":"persistence/mongodb-in-gcp/#backup-of-mongodb-instance","text":"Nais does not provide any backup solution for MongoDB-instances. However, one can easily set up cronjob for backup purposes. The following bash script will create a gzipped dump of my-mongodb-0, which then will be uploaded to the GCP storage bucket of your choosing. In production, the buckets will be archived on-premises as well. #!/usr/bin/env bash if ! kubectl exec -it -n aura my-mongodb-0 -c mongod-container -- mongodump --archive --gzip > ./dump.gz ; then echo \"failed to execute mongodump\" exit 1 fi backup_name = \"dump_ $( date + \"%Y-%m-%d_%H-%M\" ) .tgz\" if ! gsutil mv \"./dump.gz\" \"gs://<backup-bucket-name>/ $backup_name \" ; then echo \"failed to upload backup to bucket\" exit 1 fi rm dump.gz To set up your backup job you need a Naisjob bundling your bash script, a NetworkPolicy and RBAC resources as described below. apiVersion : nais.io/v1 kind : Naisjob metadata : labels : team : myteam name : my-backup namespace : myteam spec : activeDeadlineSeconds : 6000 backoffLimit : 5 failedJobsHistoryLimit : 2 gcp : buckets : - cascadingDelete : false name : my-backup-bucket retentionPeriodDays : 30 image : ghcr.io/navikt/my-backup:1.0 resources : limits : cpu : 200m memory : 128Mi requests : cpu : 100m memory : 64Mi restartPolicy : Never schedule : '0 */6 * * *' skipCaBundle : true successfulJobsHistoryLimit : 2 ttlSecondsAfterFinished : 60 --- apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : labels : app : my-backup team : myteam name : my-backup-apiserver namespace : myteam spec : egress : - to : - ipBlock : cidr : 172.16.0.2/32 podSelector : matchLabels : app : my-backup policyTypes : - Egress --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : my-backup namespace : myteam rules : - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : my-backup namespace : myteam roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : my-backup subjects : - namespace : myteam kind : ServiceAccount name : my-backup","title":"Backup of MongoDB-instance"},{"location":"persistence/mq/","text":"MQ \u00a7 MQ is supported on the nais platform on-premises and on GCP. We recommend using kafka where possible and sensible. Requirements \u00a7 Basta \u00a7 Info When ordering groups for existing service users the password will be reset. If you do not want the password to be reset, contact windows admin and ask them to add the existing user to the AD group and set the \"extensionAttribute9\" property on the user MQ groups and users are ordered using basta . For legacy applications registered in fasit use \"AD groups\" on the \"Create\" tab, for all other applications use \"Custom AD groups\". Access to basta (and fasit) can be obtained by requesting access from identity management . Access to development environments (including legacy u, t and q environments): 0000-GA-env-config-TestAdmin - Fasit T/Q Access to production environments (including legacy p environment): 0000-GA-env-config-ProdAdmin - Fasit P Order the group by using your applications name, the group name in AD will be 0000-GA-MQ- followed by the application name. If no service account exists with the name srv + application name, it will be created and added to the group. If the user already exists the user will be added to the group and the password will be updated and uploaded to vault in the serviceuser directory. From vault you can then mount this secret into your pod to authenticate with MQ. Environments \u00a7 Dev: QueueManager Hostname Port MQLS01 b27apvl219.preprod.local 1413 MQLS02 b27apvl220.preprod.local 1413 MQLS03 b27apvl221.preprod.local 1413 MQLS04 b27apvl222.preprod.local 1413 Production: QueueManager Hostname Port MPLS01 a01apvl247.adeo.no 1414 MPLS02 a01apvl269.adeo.no 1414 MPLS03 a01apvl270.adeo.no 1414 MPLS04 a01apvl271.adeo.no 1414 These servers are available from on-premises and GCP alike. Application \u00a7 The application needs to implement MQ authentication using the client libraries, as opposed to previously just sending username. If you are using MQ client lower than 9.2.1.0 set this parameter when connecting: USER_AUTHENTICATION_MQCSP=true Setting this in java: connectionFactory.setBooleanProperty(JmsConstants.USER_AUTHENTICATION_MQCSP, true);","title":"MQ"},{"location":"persistence/mq/#mq","text":"MQ is supported on the nais platform on-premises and on GCP. We recommend using kafka where possible and sensible.","title":"MQ"},{"location":"persistence/mq/#requirements","text":"","title":"Requirements"},{"location":"persistence/mq/#basta","text":"Info When ordering groups for existing service users the password will be reset. If you do not want the password to be reset, contact windows admin and ask them to add the existing user to the AD group and set the \"extensionAttribute9\" property on the user MQ groups and users are ordered using basta . For legacy applications registered in fasit use \"AD groups\" on the \"Create\" tab, for all other applications use \"Custom AD groups\". Access to basta (and fasit) can be obtained by requesting access from identity management . Access to development environments (including legacy u, t and q environments): 0000-GA-env-config-TestAdmin - Fasit T/Q Access to production environments (including legacy p environment): 0000-GA-env-config-ProdAdmin - Fasit P Order the group by using your applications name, the group name in AD will be 0000-GA-MQ- followed by the application name. If no service account exists with the name srv + application name, it will be created and added to the group. If the user already exists the user will be added to the group and the password will be updated and uploaded to vault in the serviceuser directory. From vault you can then mount this secret into your pod to authenticate with MQ.","title":"Basta"},{"location":"persistence/mq/#environments","text":"Dev: QueueManager Hostname Port MQLS01 b27apvl219.preprod.local 1413 MQLS02 b27apvl220.preprod.local 1413 MQLS03 b27apvl221.preprod.local 1413 MQLS04 b27apvl222.preprod.local 1413 Production: QueueManager Hostname Port MPLS01 a01apvl247.adeo.no 1414 MPLS02 a01apvl269.adeo.no 1414 MPLS03 a01apvl270.adeo.no 1414 MPLS04 a01apvl271.adeo.no 1414 These servers are available from on-premises and GCP alike.","title":"Environments"},{"location":"persistence/mq/#application","text":"The application needs to implement MQ authentication using the client libraries, as opposed to previously just sending username. If you are using MQ client lower than 9.2.1.0 set this parameter when connecting: USER_AUTHENTICATION_MQCSP=true Setting this in java: connectionFactory.setBooleanProperty(JmsConstants.USER_AUTHENTICATION_MQCSP, true);","title":"Application"},{"location":"persistence/objectstore/","text":"S3 object store \u00a7 objectstore on NAIS stores files in buckets. On-prem uses rook ceph storage nodes as the storage solution. This is persistent storage and available in all clusters. The preferred solution is to use GCP for both applications and buckets, but it is supported on-prem as well. The objectstore / S3 bucket access is secured with access keys and secret keys stored in kubernetes. Info This feature is only available in on-premises clusters. Info This feature does not have backups of the data in the buckets, please move to GCP if this is needed. How to \u00a7 Ceph object storage user \u00a7 On NAIS on-prem you're required to add a ceph object storage user. This is done through nais-yaml. After applying changes to the ceph-users.yaml and applying this to the cluster the rook operator will create the necessary secrets used to access the S3 bucket. To get access create a pull-request in navikt/nais-yaml . Example yaml for ceph user: ceph-user.yaml apiVersion : ceph.rook.io/v1 kind : CephObjectStoreUser metadata : name : example-application namespace : rook-ceph spec : store : objectstore displayName : \"example-application\" After a few minutes the rook operator in the cluster will create the secret. The keys necessary can then be fetched and stored in vault for mounting and usage runtime for the application. Contact @Sten.Ivar.R\u00f8kke or other nais team members for keys. The objectstore service endpoint is http://objectstore.rook-ceph.svc.nais.local . The objectstore can also be reached from other clusters using https://objectstore.nais.{domain name}. Metrics \u00a7 General ceph metrics are available from several dashboards in grafana: Grafana Ceph Cluster Grafana Ceph OSD Code examples \u00a7 Kotlin objectstore / S3 bucket access: package no.nav.example import com.amazonaws.auth.AWSStaticCredentialsProvider import com.amazonaws.auth.BasicAWSCredentials import com.amazonaws.client.builder.AwsClientBuilder import com.amazonaws.services.s3.AmazonS3 import com.amazonaws.services.s3.AmazonS3ClientBuilder import com.amazonaws.services.s3.model.CannedAccessControlList import com.amazonaws.services.s3.model.CreateBucketRequest import com.amazonaws.services.s3.model.PutObjectResult import com.amazonaws.services.s3.transfer.TransferManager import com.amazonaws.services.s3.transfer.TransferManagerBuilder import java.io.File import org.slf4j.LoggerFactory object S3Client { private val s3 : AmazonS3 private val log = LoggerFactory . getLogger ( javaClass ) init { val ev = EnvVarFactory . envVar val credentials = BasicAWSCredentials ( ev . s3AccessKey , ev . s3SecretKey ) log . info ( \"New Client: (host: \" + ev . s3Url + \" - \" + ev . s3Region + \", accesskey-length: \" + ev . s3AccessKey . length + \"S3 secret key Length: \" + ev . s3SecretKey . length ) s3 = AmazonS3ClientBuilder . standard () . withEndpointConfiguration ( AwsClientBuilder . EndpointConfiguration ( ev . s3Url , ev . s3Region )) . enablePathStyleAccess () . withCredentials ( AWSStaticCredentialsProvider ( credentials )) . build () createBucketIfMissing () } fun persistToS3 ( file : File ): PutObjectResult { return s3 . putObject ( Const . BUCKET , Const . FILE , file ) } fun loadFromS3 (): File { val tempFile = createTempFile () transferManager () . download ( Const . BUCKET , Const . FILE , tempFile ) . waitForCompletion () return tempFile } private fun createBucketIfMissing () { val bucketList = s3 . listBuckets (). filter { b -> b . name == Const . BUCKET } if ( bucketList . isEmpty ()) { log . info ( \"Creating new bucket as its missing: \" + Const . BUCKET ) s3 . createBucket ( CreateBucketRequest ( Const . BUCKET ). withCannedAcl ( CannedAccessControlList . Private )) } if ( ! s3 . doesObjectExist ( Const . BUCKET , Const . FILE )) { log . info ( \"Creating empty file for persisting what have been pushed: \" + Const . FILE ) s3 . putObject ( Const . BUCKET , Const . FILE , \"\" ) } } private fun transferManager (): TransferManager { return TransferManagerBuilder . standard (). withS3Client ( s3 ). build () } } Feel free to help us out by adding more examples!","title":"S3 objectstore"},{"location":"persistence/objectstore/#s3-object-store","text":"objectstore on NAIS stores files in buckets. On-prem uses rook ceph storage nodes as the storage solution. This is persistent storage and available in all clusters. The preferred solution is to use GCP for both applications and buckets, but it is supported on-prem as well. The objectstore / S3 bucket access is secured with access keys and secret keys stored in kubernetes. Info This feature is only available in on-premises clusters. Info This feature does not have backups of the data in the buckets, please move to GCP if this is needed.","title":"S3 object store"},{"location":"persistence/objectstore/#how-to","text":"","title":"How to"},{"location":"persistence/objectstore/#ceph-object-storage-user","text":"On NAIS on-prem you're required to add a ceph object storage user. This is done through nais-yaml. After applying changes to the ceph-users.yaml and applying this to the cluster the rook operator will create the necessary secrets used to access the S3 bucket. To get access create a pull-request in navikt/nais-yaml . Example yaml for ceph user: ceph-user.yaml apiVersion : ceph.rook.io/v1 kind : CephObjectStoreUser metadata : name : example-application namespace : rook-ceph spec : store : objectstore displayName : \"example-application\" After a few minutes the rook operator in the cluster will create the secret. The keys necessary can then be fetched and stored in vault for mounting and usage runtime for the application. Contact @Sten.Ivar.R\u00f8kke or other nais team members for keys. The objectstore service endpoint is http://objectstore.rook-ceph.svc.nais.local . The objectstore can also be reached from other clusters using https://objectstore.nais.{domain name}.","title":"Ceph object storage user"},{"location":"persistence/objectstore/#metrics","text":"General ceph metrics are available from several dashboards in grafana: Grafana Ceph Cluster Grafana Ceph OSD","title":"Metrics"},{"location":"persistence/objectstore/#code-examples","text":"Kotlin objectstore / S3 bucket access: package no.nav.example import com.amazonaws.auth.AWSStaticCredentialsProvider import com.amazonaws.auth.BasicAWSCredentials import com.amazonaws.client.builder.AwsClientBuilder import com.amazonaws.services.s3.AmazonS3 import com.amazonaws.services.s3.AmazonS3ClientBuilder import com.amazonaws.services.s3.model.CannedAccessControlList import com.amazonaws.services.s3.model.CreateBucketRequest import com.amazonaws.services.s3.model.PutObjectResult import com.amazonaws.services.s3.transfer.TransferManager import com.amazonaws.services.s3.transfer.TransferManagerBuilder import java.io.File import org.slf4j.LoggerFactory object S3Client { private val s3 : AmazonS3 private val log = LoggerFactory . getLogger ( javaClass ) init { val ev = EnvVarFactory . envVar val credentials = BasicAWSCredentials ( ev . s3AccessKey , ev . s3SecretKey ) log . info ( \"New Client: (host: \" + ev . s3Url + \" - \" + ev . s3Region + \", accesskey-length: \" + ev . s3AccessKey . length + \"S3 secret key Length: \" + ev . s3SecretKey . length ) s3 = AmazonS3ClientBuilder . standard () . withEndpointConfiguration ( AwsClientBuilder . EndpointConfiguration ( ev . s3Url , ev . s3Region )) . enablePathStyleAccess () . withCredentials ( AWSStaticCredentialsProvider ( credentials )) . build () createBucketIfMissing () } fun persistToS3 ( file : File ): PutObjectResult { return s3 . putObject ( Const . BUCKET , Const . FILE , file ) } fun loadFromS3 (): File { val tempFile = createTempFile () transferManager () . download ( Const . BUCKET , Const . FILE , tempFile ) . waitForCompletion () return tempFile } private fun createBucketIfMissing () { val bucketList = s3 . listBuckets (). filter { b -> b . name == Const . BUCKET } if ( bucketList . isEmpty ()) { log . info ( \"Creating new bucket as its missing: \" + Const . BUCKET ) s3 . createBucket ( CreateBucketRequest ( Const . BUCKET ). withCannedAcl ( CannedAccessControlList . Private )) } if ( ! s3 . doesObjectExist ( Const . BUCKET , Const . FILE )) { log . info ( \"Creating empty file for persisting what have been pushed: \" + Const . FILE ) s3 . putObject ( Const . BUCKET , Const . FILE , \"\" ) } } private fun transferManager (): TransferManager { return TransferManagerBuilder . standard (). withS3Client ( s3 ). build () } } Feel free to help us out by adding more examples!","title":"Code examples"},{"location":"persistence/open-search/","text":"OpenSearch \u00a7 The NAIS platform offers OpenSearch via Aiven . OpenSearch is a fork of ElasticSearch and Kibana 7.10.2. Get your own \u00a7 As there are few teams that need an OpenSearch instance we use a IaC-repo to provision each instance. Head over to aiven-iac to learn how to get your own instance. To make it easier for you, when creating the instance, we will also create four users with read, write, readwrite and admin access. Access from Nais-app \u00a7 If you need access from an application, use the following nais.yaml-reference . When an application requesting an OpenSearch instance is deployed, credentials will be provided as environment variables. The service URI for OpenSearch is also available. If you specify openSearch.access , the credentials will be for the user with those access rights. The available access levels are: 'admin', 'read', 'write', 'readwrite' If not specified, the credentials will be for a user with read access. Environment variable Description OPEN_SEARCH_USERNAME Username OPEN_SEARCH_PASSWORD Password OPEN_SEARCH_URI Service URI Access from laptop \u00a7 With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given. OpenSearch Dashboards \u00a7 The URL for OpenSearch Dashboards (similar to Kibana) is the same as the OpenSearch instance, but using port 443 (regular https). Support \u00a7 We do not offer support on OpenSearch as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack. Alerts \u00a7 We recommend that you set up your own alerts so that you can react to problems in your OpenSearch instance. Aiven uses Telegraf to collect and present metrics, so available metrics can be found in the Telegraf documentation . We have configured our Prometheus instances in GCP to scrape the OpenSearch clusters in Aiven, so these metrics should be available in Grafana. Particularly relevant input plugins are: elasticsearch (We belive the current open search plugin is very similar to this one) mem cpu diskio disk OpenSearch is relatively new, so there aren't a lot of good resources on alerts/metrics for it yet. However, since it is a fork of ElasticSearch, much of what is written about ElasticSearch applies to OpenSearch too. The metrics use the prefix opensearch instead of elasticsearch , otherwise it looks like the same metrics are available. O'Reilly has a useful article about metrics to watch in an Elastic cluster. Awesome prometheus alerts is another good source of alerts to look out for. Some metrics that might be useful to watch, based on above article (We need feedback on this list, as we have no practical experience to lean on): opensearch_cluster_health_status_code opensearch_cluster_health_active_shards opensearch_cluster_health_initializing_shards opensearch_cluster_health_relocating_shards opensearch_cluster_health_unassigned_shards opensearch_jvm_mem_heap_used_percent opensearch_jvm_gc_collectors_old_collection_time_in_millis opensearch_jvm_gc_collectors_young_collection_time_in_millis opensearch_jvm_mem_pools_old_used_in_byte vs. opensearch_jvm_mem_pools_old_max_in_bytes opensearch_jvm_mem_pools_survivor_used_in_byte vs. opensearch_jvm_mem_pools_survivor_max_in_bytes opensearch_jvm_mem_pools_young_used_in_byte vs. opensearch_jvm_mem_pools_young_max_in_bytes opensearch_indices_search_query_time_in_millis opensearch_indices_fielddata_evictions opensearch_indices_fielddata_memory_size_in_bytes opensearch_indices_indexing_index_time_in_millis opensearch_indices_indexing_index_total opensearch_indices_merges_total_time_in_millis opensearch_indices_store_size_in_bytes cpu_usage_user diskio_weighted_io_time","title":"OpenSearch"},{"location":"persistence/open-search/#opensearch","text":"The NAIS platform offers OpenSearch via Aiven . OpenSearch is a fork of ElasticSearch and Kibana 7.10.2.","title":"OpenSearch"},{"location":"persistence/open-search/#get-your-own","text":"As there are few teams that need an OpenSearch instance we use a IaC-repo to provision each instance. Head over to aiven-iac to learn how to get your own instance. To make it easier for you, when creating the instance, we will also create four users with read, write, readwrite and admin access.","title":"Get your own"},{"location":"persistence/open-search/#access-from-nais-app","text":"If you need access from an application, use the following nais.yaml-reference . When an application requesting an OpenSearch instance is deployed, credentials will be provided as environment variables. The service URI for OpenSearch is also available. If you specify openSearch.access , the credentials will be for the user with those access rights. The available access levels are: 'admin', 'read', 'write', 'readwrite' If not specified, the credentials will be for a user with read access. Environment variable Description OPEN_SEARCH_USERNAME Username OPEN_SEARCH_PASSWORD Password OPEN_SEARCH_URI Service URI","title":"Access from Nais-app"},{"location":"persistence/open-search/#access-from-laptop","text":"With Naisdevice you have access to the aiven-prod gateway. This is a JITA (just in time access) gateway, so you need to describe why, but the access is automatically given.","title":"Access from laptop"},{"location":"persistence/open-search/#opensearch-dashboards","text":"The URL for OpenSearch Dashboards (similar to Kibana) is the same as the OpenSearch instance, but using port 443 (regular https).","title":"OpenSearch Dashboards"},{"location":"persistence/open-search/#support","text":"We do not offer support on OpenSearch as software, but questions about Aiven and provisioning can be directed to #pig_aiven on Slack.","title":"Support"},{"location":"persistence/open-search/#alerts","text":"We recommend that you set up your own alerts so that you can react to problems in your OpenSearch instance. Aiven uses Telegraf to collect and present metrics, so available metrics can be found in the Telegraf documentation . We have configured our Prometheus instances in GCP to scrape the OpenSearch clusters in Aiven, so these metrics should be available in Grafana. Particularly relevant input plugins are: elasticsearch (We belive the current open search plugin is very similar to this one) mem cpu diskio disk OpenSearch is relatively new, so there aren't a lot of good resources on alerts/metrics for it yet. However, since it is a fork of ElasticSearch, much of what is written about ElasticSearch applies to OpenSearch too. The metrics use the prefix opensearch instead of elasticsearch , otherwise it looks like the same metrics are available. O'Reilly has a useful article about metrics to watch in an Elastic cluster. Awesome prometheus alerts is another good source of alerts to look out for. Some metrics that might be useful to watch, based on above article (We need feedback on this list, as we have no practical experience to lean on): opensearch_cluster_health_status_code opensearch_cluster_health_active_shards opensearch_cluster_health_initializing_shards opensearch_cluster_health_relocating_shards opensearch_cluster_health_unassigned_shards opensearch_jvm_mem_heap_used_percent opensearch_jvm_gc_collectors_old_collection_time_in_millis opensearch_jvm_gc_collectors_young_collection_time_in_millis opensearch_jvm_mem_pools_old_used_in_byte vs. opensearch_jvm_mem_pools_old_max_in_bytes opensearch_jvm_mem_pools_survivor_used_in_byte vs. opensearch_jvm_mem_pools_survivor_max_in_bytes opensearch_jvm_mem_pools_young_used_in_byte vs. opensearch_jvm_mem_pools_young_max_in_bytes opensearch_indices_search_query_time_in_millis opensearch_indices_fielddata_evictions opensearch_indices_fielddata_memory_size_in_bytes opensearch_indices_indexing_index_time_in_millis opensearch_indices_indexing_index_total opensearch_indices_merges_total_time_in_millis opensearch_indices_store_size_in_bytes cpu_usage_user diskio_weighted_io_time","title":"Alerts"},{"location":"persistence/postgres/","text":"Postgres \u00a7 You can provision and configure Postgres through nais.yaml . The database is provisioned into the teams own project in GCP. Here the team has full access to view logs, create and restore backups and other administrative database tasks. When you deploy your application with database config, NAIS will ensure the database exists in a Google Cloud SQL instance with the specified Postgres version, and configure the application with means to connect to it. Info This feature is only available in GCP clusters. If you need on-prem databases, head over to navikt/database-iac . Below is an example of the minimal configuration needed. See all configuration options in the nais.yaml reference . ... kind : Application metadata : name : myapp spec : gcp : sqlInstances : - type : POSTGRES_12 databases : - name : mydb Configuration \u00a7 To connect your application to the database, use information from the environment variables below. The prefix NAIS_DATABASE_MYAPP_MYDB is automatically generated from the instance name myapp (defaults to application name) and mydb (from database spec). You can customize these environment variable names by setting .spec.gcp.sqlInstances[].databases[].envVarPrefix . For instance, setting this to DB will give you DB_HOST , DB_USERNAME , etc. Note that changing or adding envVarPrefix requires you to manually delete the google-sql-<MYAPP> secret and SQLUser with the same name as the application, see below. key environment variable default hostname NAIS_DATABASE_MYAPP_MYDB_HOST 127.0.0.1 port NAIS_DATABASE_MYAPP_MYDB_PORT 5432 database name NAIS_DATABASE_MYAPP_MYDB_DATABASE .spec.gcp.sqlInstances[].databases[].name database user NAIS_DATABASE_MYAPP_MYDB_USERNAME .spec.gcp.sqlInstances[].name database password NAIS_DATABASE_MYAPP_MYDB_PASSWORD (randomly generated) database url with credentials NAIS_DATABASE_MYAPP_MYDB_URL postgres://username:password@127.0.0.1:5432/mydb Info The application is the only application that can access the database instance. Other applications can not connect. It is not, for instance, possible to have two applications (e.g. producer and consumer) connecting directly to the database. Info Note that if you change your application name, database name or envVarPrefix, and then change it later, you have to manually reset database credentials . Query Insights \u00a7 Query insights are now enabled by default in GCP. This feature provides query overview and analysis. The data is available in the Google cloud console. For further reading see Google Cloud SQL Query Insights Info Data is available for seven days, increasing this will incur extra cost. Maintenance window \u00a7 Google will automatically perform upgrades, fix bugs and apply security patches to prevent exploits. Your application should be able to handle occasional downtime as this maintenance is performed. Read more on maintenance windows here . NAIS does not configure the maintenance window, but this can be set up in the application spec: nais.yaml . If you wish to be notified about upcoming maintenance, you can opt-in for this on the Communications page in the GCP console. Sizing your database \u00a7 By default, the database server is db-f1-micro , has 1 vCPU, 614 MB RAM and 10GB of SSD storage with no automatic storage increase. If you need to change the defaults you can do this in nais.yaml . * Instance settings https://cloud.google.com/sql/docs/postgres/instance-settings * Shared CPU machine types ( db-f1-micro and db-g1-small ) are not covered by the Cloud SQL SLA . Automated backup \u00a7 The database is backed up nightly at 3 AM (GMT+1) by default, but can be overridden in nais.yaml by setting spec.gcp.sqlInstances[].autoBackupTime . By default, seven backups will be kept. More info here . The backups can be found in the Google Cloud SQL instance dashboard. Point-in-time recovery \u00a7 Point-in-time recovery can be enabled by configuring this in the sql instance for your application spec. This feature allows you to recover your database to a specific point in time. Info This feature is not enabled by default. When enabled the Postgres instance will be restarted. Warning Use this feature with automatic storage increase enabled. See application spec reference here For further reading see google Cloud SQL PIT recovery Disaster backup \u00a7 In case of catastrophic failure in GCP we are running a daily complete backup of the postgresql databases in GCP to an on-prem location. This backup currently runs at 5 am. This is in addition to the regular backups in GCP. Metrics \u00a7 Metrics for each Postgres can be found in Grafana . Cloud SQL credentials \u00a7 Cloud SQL uses ConfigConnector/CNRM to create and manage all relevant resources (sqldatabase, sqlinstance, sqluser, credentials) for postgreSQL. When creating an application via your nais.yaml the database in your google project, along with other necessary resources, are created. The creation of the database takes about ten minutes, and the credential settings will be updated after the database is ready for use. Warning If you delete and recreate your app new credentials will be created and a synchronization is needed. This process can take up to ten minutes. Using the workaround described below you can avoid this synchronization period. Workaround for password synchronization issues \u00a7 Retrieve the password from the secret google-sql- in your namespace (the password is base64 encoded). kubectl get secret google-sql-<MYAPP> -o jsonpath=\"{ .data['<YOUR PASSWORD VARIABLE>'] }\" | base64 -d Through gcloud-cli \u00a7 Give yourself the role of roles/cloudsql.admin (which includes the needed permission cloudsql.users.update ). gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member=user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role=roles/cloudsql.admin \\ --condition=\"expression=request.time < timestamp('$(date -v '+1H' -u +'%Y-%m-%dT%H:%M:%SZ')'),title=temp_access\" Then you can set the new password with the following command. gcloud sql users set-password <USERNAME> --instance <DB_INSTANCE> --prompt-for-password Through console.cloud.google.com \u00a7 Log in to the Google Cloud Console and set the password manually for the application user in the sql instance: SQL -> -> Users -> -> Change password Reset database credentials \u00a7 To reset the database credentials for your application (if application name, database name or envVarPrefix has been changed): $ kubectl delete secret google-sql-<MYAPP> $ kubectl delete sqluser <MYAPP> Cloud SQL Proxy \u00a7 The application will connect to the database using Cloud SQL Proxy , ensuring that the database communication happens in secure tunnel, authenticated with automatically rotated credentials. NAIS will add and configure the proxy client container as a sidecar in the pod, making it available on localhost for the application. The application will then connect to the proxy using standard database protocol just as if it was the actual database. For more detailed information, check out the Cloud SQL Proxy documentation Additional user(s) database(s) \u00a7 You can add users to your database by setting database configuration option: .spec.gcp.sqlInstances[].databases[].users[].name . Additional users needs to manually be given access to the database and table. Either directly or with Flyway or other database migration tools. Names added must match regex: ^[_a-zA-Z][_a-zA-Z0-9]+$ . Secrets is generated and mounted for each user. With .spec.gcp.sqlInstances[].databases[].envVarPrefix set to DB and additional username to _user2 you will get environment variables in format DB_USER2_MYDB_USERNAME etc. Details about environment variables is specified her: configuration Info If you've deployed your application with an additional users, and then change name or remove the user from configuration, you need to manually delete the google-sql-<MYAPP>-<USER> secret: $ kubectl delete secret google-sql-<MYAPP>-<USER> Personal database access \u00a7 Databases should always be accessed using a personal account, and the access should ideally be temporary. Prerequisites \u00a7 Step 1. Install local binaries This guide assumes that you have the following installed on your local machine: cloudsql-proxy binary psql binary Step 2. Allow your user to edit Cloud SQL resources for your project Ensure that you have authenticated gcloud by running gcloud auth login To be able to perform the gcloud commands mentioned below you need a role with user edit permissions, e.g. roles/cloudsql.admin To grant yourself this role for a given project, run the following command: gcloud projects add-iam-policy-binding <project-id> \\ --member user:<your-email> \\ --role roles/cloudsql.admin \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" where <project-id> can be found by running: gcloud projects list \\ --filter = <team> Step 3. One-time setup of privileges to SQL IAM users This is only required once per database instance and should be done before DDL scripts are run in the database in order to ensure the objects have the right permissions. Once the database instance is created, we need to grant the IAM users access to the public schema. This can either be done by using the default application database user during database creation/migration with scripts (e.g. Flyway), or as a one-time setup by using the default postgres user. Step 3a. Set password for postgres user In order to use the postgres user, you have to set a password first: gcloud sql users set-password postgres \\ --instance = <INSTANCE_NAME> \\ --prompt-for-password \\ --project <PROJECT_ID> Step 3b. Log in to the database with the postgres user Set up the cloudsql-proxy : CONNECTION_NAME = $( gcloud sql instances describe <INSTANCE_NAME> \\ --format = \"get(connectionName)\" \\ --project <PROJECT_ID> ) ; cloud_sql_proxy -instances = ${ CONNECTION_NAME } = tcp:5432 Log in to the database (you will be prompted for the password you set in the previous step): psql -U postgres -h localhost <DATABASE_NAME> -W If you are using Cloud SQL Auth proxy v1.21.0 or newer you can get the token in the cloud_sql_proxy command so you can run the psql-command without the -W parameter: cloud_sql_proxy -enable_iam_login -instances = ${ CONNECTION_NAME } = tcp:5432 Step 3c. Enable privileges for user(s) in database This can be enabled for all cloudsqliamusers (all IAM users are assigned the role cloudsqliamuser ) with the following command: alter default privileges in schema public grant all on tables to cloudsqliamuser ; Or for a specific user (the given IAM user must exist in the database): alter default privileges in schema public grant all on tables to 'user@nav.no' ; If your application created the tables before you were able to run these commands, then the owner of the tables is set to the application's user. Thus, your application must run the following command either through your chosen database migration tool (e.g. Flyway) or manually with the application user's credentials: grant all on all tables in schema public to cloudsqliamuser ; Granting temporary personal access \u00a7 Step 1. Create database IAM user This is required once per user and requires that you have create user permission in IAM in your project, e.g. Cloud SQL Admin. To grant yourself this role for a given project, run the following command: gcloud projects add-iam-policy-binding <project-id> \\ --member user:<your-email> \\ --role roles/cloudsql.admin \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" Then, to create the database IAM user: gcloud beta sql users create <FIRSTNAME>.<LASTNAME>@nav.no \\ --instance = <INSTANCE_NAME> \\ --type = cloud_iam_user \\ --project <PROJECT_ID> Step 2. Create a temporary IAM binding for 1 hour Generally, you should try to keep your personal database access time-limited. The following command grants your user permission to log into the database for 1 hour. If your system has GNU utilities installed: gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member = user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role = roles/cloudsql.instanceUser \\ --condition = \"expression=request.time < timestamp(' $( date --iso-8601 = seconds -d '+1 hours' ) '),title=temp_access\" Otherwise (e.g. MacOS users): gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member = user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role = roles/cloudsql.instanceUser \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" Step 3. Log in with personal user Ensure that the cloudsql-proxy is up and running, if not then: CONNECTION_NAME = $( gcloud sql instances describe <INSTANCE_NAME> \\ --format = \"get(connectionName)\" \\ --project <PROJECT_ID> ) ; cloud_sql_proxy -instances = ${ CONNECTION_NAME } = tcp:5432 Then, connect to the database: export PGPASSWORD = $( gcloud auth print-access-token ) psql -U <FIRSTNAME>.<LASTNAME>@nav.no -h localhost <DATABASE_NAME> Upgrading major version \u00a7 In-place database upgrades through nais.yaml is currently not supported. If you attempt to change the version this way, you will get an error message. In order to upgrade the database version, you will need to follow the Cloud SQL docs on upgrading PostgreSQL for an instance . Deleting the database \u00a7 The database is not automatically removed when deleting your NAIS application. Remove unused databases to avoid incurring unnecessary costs. This is done by setting cascadingDelete in your nais.yaml -specification. Danger When you delete an Cloud SQL instance, you cannot reuse the name of the deleted instance until one week from the deletion date. Debugging \u00a7 Check the events on the Config Connector resources $ kubectl describe sqlinstance <myapp> $ kubectl describe sqldatabase <mydb> $ kubectl describe sqluser <myapp> Check the logs of the Cloud SQL Proxy $ kubectl logs <pod> -c cloudsql-proxy Example with all configuration options \u00a7 See full example .","title":"Google Cloud SQL (postgres)"},{"location":"persistence/postgres/#postgres","text":"You can provision and configure Postgres through nais.yaml . The database is provisioned into the teams own project in GCP. Here the team has full access to view logs, create and restore backups and other administrative database tasks. When you deploy your application with database config, NAIS will ensure the database exists in a Google Cloud SQL instance with the specified Postgres version, and configure the application with means to connect to it. Info This feature is only available in GCP clusters. If you need on-prem databases, head over to navikt/database-iac . Below is an example of the minimal configuration needed. See all configuration options in the nais.yaml reference . ... kind : Application metadata : name : myapp spec : gcp : sqlInstances : - type : POSTGRES_12 databases : - name : mydb","title":"Postgres"},{"location":"persistence/postgres/#configuration","text":"To connect your application to the database, use information from the environment variables below. The prefix NAIS_DATABASE_MYAPP_MYDB is automatically generated from the instance name myapp (defaults to application name) and mydb (from database spec). You can customize these environment variable names by setting .spec.gcp.sqlInstances[].databases[].envVarPrefix . For instance, setting this to DB will give you DB_HOST , DB_USERNAME , etc. Note that changing or adding envVarPrefix requires you to manually delete the google-sql-<MYAPP> secret and SQLUser with the same name as the application, see below. key environment variable default hostname NAIS_DATABASE_MYAPP_MYDB_HOST 127.0.0.1 port NAIS_DATABASE_MYAPP_MYDB_PORT 5432 database name NAIS_DATABASE_MYAPP_MYDB_DATABASE .spec.gcp.sqlInstances[].databases[].name database user NAIS_DATABASE_MYAPP_MYDB_USERNAME .spec.gcp.sqlInstances[].name database password NAIS_DATABASE_MYAPP_MYDB_PASSWORD (randomly generated) database url with credentials NAIS_DATABASE_MYAPP_MYDB_URL postgres://username:password@127.0.0.1:5432/mydb Info The application is the only application that can access the database instance. Other applications can not connect. It is not, for instance, possible to have two applications (e.g. producer and consumer) connecting directly to the database. Info Note that if you change your application name, database name or envVarPrefix, and then change it later, you have to manually reset database credentials .","title":"Configuration"},{"location":"persistence/postgres/#query-insights","text":"Query insights are now enabled by default in GCP. This feature provides query overview and analysis. The data is available in the Google cloud console. For further reading see Google Cloud SQL Query Insights Info Data is available for seven days, increasing this will incur extra cost.","title":"Query Insights"},{"location":"persistence/postgres/#maintenance-window","text":"Google will automatically perform upgrades, fix bugs and apply security patches to prevent exploits. Your application should be able to handle occasional downtime as this maintenance is performed. Read more on maintenance windows here . NAIS does not configure the maintenance window, but this can be set up in the application spec: nais.yaml . If you wish to be notified about upcoming maintenance, you can opt-in for this on the Communications page in the GCP console.","title":"Maintenance window"},{"location":"persistence/postgres/#sizing-your-database","text":"By default, the database server is db-f1-micro , has 1 vCPU, 614 MB RAM and 10GB of SSD storage with no automatic storage increase. If you need to change the defaults you can do this in nais.yaml . * Instance settings https://cloud.google.com/sql/docs/postgres/instance-settings * Shared CPU machine types ( db-f1-micro and db-g1-small ) are not covered by the Cloud SQL SLA .","title":"Sizing your database"},{"location":"persistence/postgres/#automated-backup","text":"The database is backed up nightly at 3 AM (GMT+1) by default, but can be overridden in nais.yaml by setting spec.gcp.sqlInstances[].autoBackupTime . By default, seven backups will be kept. More info here . The backups can be found in the Google Cloud SQL instance dashboard.","title":"Automated backup"},{"location":"persistence/postgres/#point-in-time-recovery","text":"Point-in-time recovery can be enabled by configuring this in the sql instance for your application spec. This feature allows you to recover your database to a specific point in time. Info This feature is not enabled by default. When enabled the Postgres instance will be restarted. Warning Use this feature with automatic storage increase enabled. See application spec reference here For further reading see google Cloud SQL PIT recovery","title":"Point-in-time recovery"},{"location":"persistence/postgres/#disaster-backup","text":"In case of catastrophic failure in GCP we are running a daily complete backup of the postgresql databases in GCP to an on-prem location. This backup currently runs at 5 am. This is in addition to the regular backups in GCP.","title":"Disaster backup"},{"location":"persistence/postgres/#metrics","text":"Metrics for each Postgres can be found in Grafana .","title":"Metrics"},{"location":"persistence/postgres/#cloud-sql-credentials","text":"Cloud SQL uses ConfigConnector/CNRM to create and manage all relevant resources (sqldatabase, sqlinstance, sqluser, credentials) for postgreSQL. When creating an application via your nais.yaml the database in your google project, along with other necessary resources, are created. The creation of the database takes about ten minutes, and the credential settings will be updated after the database is ready for use. Warning If you delete and recreate your app new credentials will be created and a synchronization is needed. This process can take up to ten minutes. Using the workaround described below you can avoid this synchronization period.","title":"Cloud SQL credentials"},{"location":"persistence/postgres/#workaround-for-password-synchronization-issues","text":"Retrieve the password from the secret google-sql- in your namespace (the password is base64 encoded). kubectl get secret google-sql-<MYAPP> -o jsonpath=\"{ .data['<YOUR PASSWORD VARIABLE>'] }\" | base64 -d","title":"Workaround for password synchronization issues"},{"location":"persistence/postgres/#through-gcloud-cli","text":"Give yourself the role of roles/cloudsql.admin (which includes the needed permission cloudsql.users.update ). gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member=user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role=roles/cloudsql.admin \\ --condition=\"expression=request.time < timestamp('$(date -v '+1H' -u +'%Y-%m-%dT%H:%M:%SZ')'),title=temp_access\" Then you can set the new password with the following command. gcloud sql users set-password <USERNAME> --instance <DB_INSTANCE> --prompt-for-password","title":"Through gcloud-cli"},{"location":"persistence/postgres/#through-consolecloudgooglecom","text":"Log in to the Google Cloud Console and set the password manually for the application user in the sql instance: SQL -> -> Users -> -> Change password","title":"Through console.cloud.google.com"},{"location":"persistence/postgres/#reset-database-credentials","text":"To reset the database credentials for your application (if application name, database name or envVarPrefix has been changed): $ kubectl delete secret google-sql-<MYAPP> $ kubectl delete sqluser <MYAPP>","title":"Reset database credentials"},{"location":"persistence/postgres/#cloud-sql-proxy","text":"The application will connect to the database using Cloud SQL Proxy , ensuring that the database communication happens in secure tunnel, authenticated with automatically rotated credentials. NAIS will add and configure the proxy client container as a sidecar in the pod, making it available on localhost for the application. The application will then connect to the proxy using standard database protocol just as if it was the actual database. For more detailed information, check out the Cloud SQL Proxy documentation","title":"Cloud SQL Proxy"},{"location":"persistence/postgres/#additional-users-databases","text":"You can add users to your database by setting database configuration option: .spec.gcp.sqlInstances[].databases[].users[].name . Additional users needs to manually be given access to the database and table. Either directly or with Flyway or other database migration tools. Names added must match regex: ^[_a-zA-Z][_a-zA-Z0-9]+$ . Secrets is generated and mounted for each user. With .spec.gcp.sqlInstances[].databases[].envVarPrefix set to DB and additional username to _user2 you will get environment variables in format DB_USER2_MYDB_USERNAME etc. Details about environment variables is specified her: configuration Info If you've deployed your application with an additional users, and then change name or remove the user from configuration, you need to manually delete the google-sql-<MYAPP>-<USER> secret: $ kubectl delete secret google-sql-<MYAPP>-<USER>","title":"Additional user(s) database(s)"},{"location":"persistence/postgres/#personal-database-access","text":"Databases should always be accessed using a personal account, and the access should ideally be temporary.","title":"Personal database access"},{"location":"persistence/postgres/#prerequisites","text":"Step 1. Install local binaries This guide assumes that you have the following installed on your local machine: cloudsql-proxy binary psql binary Step 2. Allow your user to edit Cloud SQL resources for your project Ensure that you have authenticated gcloud by running gcloud auth login To be able to perform the gcloud commands mentioned below you need a role with user edit permissions, e.g. roles/cloudsql.admin To grant yourself this role for a given project, run the following command: gcloud projects add-iam-policy-binding <project-id> \\ --member user:<your-email> \\ --role roles/cloudsql.admin \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" where <project-id> can be found by running: gcloud projects list \\ --filter = <team> Step 3. One-time setup of privileges to SQL IAM users This is only required once per database instance and should be done before DDL scripts are run in the database in order to ensure the objects have the right permissions. Once the database instance is created, we need to grant the IAM users access to the public schema. This can either be done by using the default application database user during database creation/migration with scripts (e.g. Flyway), or as a one-time setup by using the default postgres user. Step 3a. Set password for postgres user In order to use the postgres user, you have to set a password first: gcloud sql users set-password postgres \\ --instance = <INSTANCE_NAME> \\ --prompt-for-password \\ --project <PROJECT_ID> Step 3b. Log in to the database with the postgres user Set up the cloudsql-proxy : CONNECTION_NAME = $( gcloud sql instances describe <INSTANCE_NAME> \\ --format = \"get(connectionName)\" \\ --project <PROJECT_ID> ) ; cloud_sql_proxy -instances = ${ CONNECTION_NAME } = tcp:5432 Log in to the database (you will be prompted for the password you set in the previous step): psql -U postgres -h localhost <DATABASE_NAME> -W If you are using Cloud SQL Auth proxy v1.21.0 or newer you can get the token in the cloud_sql_proxy command so you can run the psql-command without the -W parameter: cloud_sql_proxy -enable_iam_login -instances = ${ CONNECTION_NAME } = tcp:5432 Step 3c. Enable privileges for user(s) in database This can be enabled for all cloudsqliamusers (all IAM users are assigned the role cloudsqliamuser ) with the following command: alter default privileges in schema public grant all on tables to cloudsqliamuser ; Or for a specific user (the given IAM user must exist in the database): alter default privileges in schema public grant all on tables to 'user@nav.no' ; If your application created the tables before you were able to run these commands, then the owner of the tables is set to the application's user. Thus, your application must run the following command either through your chosen database migration tool (e.g. Flyway) or manually with the application user's credentials: grant all on all tables in schema public to cloudsqliamuser ;","title":"Prerequisites"},{"location":"persistence/postgres/#granting-temporary-personal-access","text":"Step 1. Create database IAM user This is required once per user and requires that you have create user permission in IAM in your project, e.g. Cloud SQL Admin. To grant yourself this role for a given project, run the following command: gcloud projects add-iam-policy-binding <project-id> \\ --member user:<your-email> \\ --role roles/cloudsql.admin \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" Then, to create the database IAM user: gcloud beta sql users create <FIRSTNAME>.<LASTNAME>@nav.no \\ --instance = <INSTANCE_NAME> \\ --type = cloud_iam_user \\ --project <PROJECT_ID> Step 2. Create a temporary IAM binding for 1 hour Generally, you should try to keep your personal database access time-limited. The following command grants your user permission to log into the database for 1 hour. If your system has GNU utilities installed: gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member = user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role = roles/cloudsql.instanceUser \\ --condition = \"expression=request.time < timestamp(' $( date --iso-8601 = seconds -d '+1 hours' ) '),title=temp_access\" Otherwise (e.g. MacOS users): gcloud projects add-iam-policy-binding <PROJECT_ID> \\ --member = user:<FIRSTNAME>.<LASTNAME>@nav.no \\ --role = roles/cloudsql.instanceUser \\ --condition = \"expression=request.time < timestamp(' $( date -v '+1H' -u + '%Y-%m-%dT%H:%M:%SZ' ) '),title=temp_access\" Step 3. Log in with personal user Ensure that the cloudsql-proxy is up and running, if not then: CONNECTION_NAME = $( gcloud sql instances describe <INSTANCE_NAME> \\ --format = \"get(connectionName)\" \\ --project <PROJECT_ID> ) ; cloud_sql_proxy -instances = ${ CONNECTION_NAME } = tcp:5432 Then, connect to the database: export PGPASSWORD = $( gcloud auth print-access-token ) psql -U <FIRSTNAME>.<LASTNAME>@nav.no -h localhost <DATABASE_NAME>","title":"Granting temporary personal access"},{"location":"persistence/postgres/#upgrading-major-version","text":"In-place database upgrades through nais.yaml is currently not supported. If you attempt to change the version this way, you will get an error message. In order to upgrade the database version, you will need to follow the Cloud SQL docs on upgrading PostgreSQL for an instance .","title":"Upgrading major version"},{"location":"persistence/postgres/#deleting-the-database","text":"The database is not automatically removed when deleting your NAIS application. Remove unused databases to avoid incurring unnecessary costs. This is done by setting cascadingDelete in your nais.yaml -specification. Danger When you delete an Cloud SQL instance, you cannot reuse the name of the deleted instance until one week from the deletion date.","title":"Deleting the database"},{"location":"persistence/postgres/#debugging","text":"Check the events on the Config Connector resources $ kubectl describe sqlinstance <myapp> $ kubectl describe sqldatabase <mydb> $ kubectl describe sqluser <myapp> Check the logs of the Cloud SQL Proxy $ kubectl logs <pod> -c cloudsql-proxy","title":"Debugging"},{"location":"persistence/postgres/#example-with-all-configuration-options","text":"See full example .","title":"Example with all configuration options"},{"location":"persistence/redis/","text":"Redis \u00a7 Redis on NAIS is run without disk/storage. This means that if a Redis instance is restarted due to something like maintenance, the data will be lost. In other words, do not store data here that you cannot afford to lose. It's also possible to password protect the Redis instance with some configuration. How to \u00a7 Redis can be run as a normal NAIS application. This means that you can only run single instances that are not scalable; increasing replicas will only start new databases that are not synced. Contact @Kyrre.Havik.Eriksen if you need High Availability-Redis. Example deployments \u00a7 An example Redis setup looks like this: Vanilla Redis --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : nais.io/read-only-file-system : \"false\" nais.io/run-as-user : \"999\" name : ${appname} namespace : ${teamname} spec : image : redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${inbound-app} Vanilla Redis with metrics --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : nais.io/read-only-file-system : \"false\" nais.io/run-as-user : \"999\" name : ${appname} namespace : ${teamname} spec : image : redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${appname}-redisexporter --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} name : ${appname}-redisexporter namespace : ${teamname} spec : image : oliver006/redis_exporter:<latest-docker> port : 9121 prometheus : enabled : true replicas : min : 1 max : 1 resources : limits : cpu : 100m memory : 100Mi requests : cpu : 100m memory : 100Mi liveness : path : /health accessPolicy : # for GCP outbound : rules : - application : ${appname} env : - name : REDIS_ADDR value : ${appname}:6379 - name : REDIS_EXPORTER_LOG_FORMAT value : json Secured Redis with metrics --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : \"nais.io/run-as-group\" : \"0\" \"nais.io/read-only-file-system\" : \"false\" name : ${appname} namespace : ${teamname} spec : image : bitnami/redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${appname}-redisexporter envFrom : - secret : ${secret-name} --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} name : ${appname}-redisexporter namespace : ${teamname} spec : image : oliver006/redis_exporter:<latest-docker> port : 9121 prometheus : enabled : true replicas : min : 1 max : 1 resources : limits : cpu : 100m memory : 100Mi requests : cpu : 100m memory : 100Mi liveness : path : /health accessPolicy : # for GCP outbound : rules : - application : ${appname} env : - name : REDIS_ADDR value : ${appname}:6379 - name : REDIS_EXPORTER_LOG_FORMAT value : json envFrom : - secret : ${secret-name} Then, running kubectl apply -f <redis-config>.yaml will start up a Redis instance. Or deploy it with nais/deploy . Success Check out hub.docker.com for latest Redis version For any applications that wishes to communicate with the Redis instance, it is recommended to add the following environment variable to the applications' nais.yaml files: env : - name : REDIS_HOST value : ${appname}.${namespace}.svc.nais.local GCP \u00a7 If on GCP , the host name is slightly different: env : - name : REDIS_HOST value : ${appname}.${namespace}.svc.cluster.local Remember to set up access policies for your Redis instance as well: accessPolicy : inbound : rules : - application : ${inbound-appname} And the equivalent outbound policy for any apps that should be able to connect to the redis instance: accessPolicy : outbound : rules : - application : ${redis-appname} Redis metrics \u00a7 If you want metrics from a Redis instance running on NAIS, a separate exporter must also be run. An example nais.yaml for the simplest version of such an exporter is found above. NAIS has also made a dashboard that everyone can use. The only caveat is that the exporter application needs to end its name with redisexporter in the configuration. The dashboard is called Redis exporters . The dashboard sorts by addr , enabling a single exporter to scrape several Redis instances. Info See the redis-with-metrics.yaml example in examples above for setting up a configuration with the metrics exporter. Success Check out hub.docker.com for latest Redis metrics exporter version Secure Redis \u00a7 If you need to password protect your Redis instance, the easiest way to do this is to use Kubernetes secrets and mount them to your container, however you will also have to use a different Redis baseimage from Bitnami . Success Check out hub.docker.com for latest Bitnami Redis version Start by creating a Kubernetes secret: kubectl create secret generic ${secret-name} \\ --from-literal=${key}=${value} For example: kubectl create secret generic redis-password \\ --from-literal=REDIS_PASSWORD=$(cat /dev/urandom | env LC_ALL=C tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1) Now that you have a secret in Kubernetes (use kubectl describe secret redis-password to look at it), all you have to do left is to mount it. Then you should also mount it to any applications that connects to the Redis instance. This is done by adding the following to your nais.yaml . spec : envFrom : - secret : redis-password Info See the redis-secure.yaml example in the examples above for setting up Redis with a password from the secret you just created. Code examples \u00a7 We are not application developers, so please help us out by expanding with examples! Redis cache in Spring Boot \u00a7 Add the following to the Spring Boot application's application.yaml to enable Spring to use Redis as cache. session : store-type : redis redis : host : ${REDIS_HOST} port : 6379","title":"Redis"},{"location":"persistence/redis/#redis","text":"Redis on NAIS is run without disk/storage. This means that if a Redis instance is restarted due to something like maintenance, the data will be lost. In other words, do not store data here that you cannot afford to lose. It's also possible to password protect the Redis instance with some configuration.","title":"Redis"},{"location":"persistence/redis/#how-to","text":"Redis can be run as a normal NAIS application. This means that you can only run single instances that are not scalable; increasing replicas will only start new databases that are not synced. Contact @Kyrre.Havik.Eriksen if you need High Availability-Redis.","title":"How to"},{"location":"persistence/redis/#example-deployments","text":"An example Redis setup looks like this: Vanilla Redis --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : nais.io/read-only-file-system : \"false\" nais.io/run-as-user : \"999\" name : ${appname} namespace : ${teamname} spec : image : redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${inbound-app} Vanilla Redis with metrics --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : nais.io/read-only-file-system : \"false\" nais.io/run-as-user : \"999\" name : ${appname} namespace : ${teamname} spec : image : redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${appname}-redisexporter --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} name : ${appname}-redisexporter namespace : ${teamname} spec : image : oliver006/redis_exporter:<latest-docker> port : 9121 prometheus : enabled : true replicas : min : 1 max : 1 resources : limits : cpu : 100m memory : 100Mi requests : cpu : 100m memory : 100Mi liveness : path : /health accessPolicy : # for GCP outbound : rules : - application : ${appname} env : - name : REDIS_ADDR value : ${appname}:6379 - name : REDIS_EXPORTER_LOG_FORMAT value : json Secured Redis with metrics --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} annotations : \"nais.io/run-as-group\" : \"0\" \"nais.io/read-only-file-system\" : \"false\" name : ${appname} namespace : ${teamname} spec : image : bitnami/redis:<latest-docker-tag> # or a custom Redis-image port : 6379 replicas : # A single Redis-app doesn't scale min : 1 max : 1 # More replicas doesn't sync resources : # you need to monitor the resource usage yourself limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi service : port : 6379 protocol : redis accessPolicy : # for GCP inbound : rules : - application : ${appname}-redisexporter envFrom : - secret : ${secret-name} --- apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : labels : team : ${teamname} name : ${appname}-redisexporter namespace : ${teamname} spec : image : oliver006/redis_exporter:<latest-docker> port : 9121 prometheus : enabled : true replicas : min : 1 max : 1 resources : limits : cpu : 100m memory : 100Mi requests : cpu : 100m memory : 100Mi liveness : path : /health accessPolicy : # for GCP outbound : rules : - application : ${appname} env : - name : REDIS_ADDR value : ${appname}:6379 - name : REDIS_EXPORTER_LOG_FORMAT value : json envFrom : - secret : ${secret-name} Then, running kubectl apply -f <redis-config>.yaml will start up a Redis instance. Or deploy it with nais/deploy . Success Check out hub.docker.com for latest Redis version For any applications that wishes to communicate with the Redis instance, it is recommended to add the following environment variable to the applications' nais.yaml files: env : - name : REDIS_HOST value : ${appname}.${namespace}.svc.nais.local","title":"Example deployments"},{"location":"persistence/redis/#gcp","text":"If on GCP , the host name is slightly different: env : - name : REDIS_HOST value : ${appname}.${namespace}.svc.cluster.local Remember to set up access policies for your Redis instance as well: accessPolicy : inbound : rules : - application : ${inbound-appname} And the equivalent outbound policy for any apps that should be able to connect to the redis instance: accessPolicy : outbound : rules : - application : ${redis-appname}","title":"GCP"},{"location":"persistence/redis/#redis-metrics","text":"If you want metrics from a Redis instance running on NAIS, a separate exporter must also be run. An example nais.yaml for the simplest version of such an exporter is found above. NAIS has also made a dashboard that everyone can use. The only caveat is that the exporter application needs to end its name with redisexporter in the configuration. The dashboard is called Redis exporters . The dashboard sorts by addr , enabling a single exporter to scrape several Redis instances. Info See the redis-with-metrics.yaml example in examples above for setting up a configuration with the metrics exporter. Success Check out hub.docker.com for latest Redis metrics exporter version","title":"Redis metrics"},{"location":"persistence/redis/#secure-redis","text":"If you need to password protect your Redis instance, the easiest way to do this is to use Kubernetes secrets and mount them to your container, however you will also have to use a different Redis baseimage from Bitnami . Success Check out hub.docker.com for latest Bitnami Redis version Start by creating a Kubernetes secret: kubectl create secret generic ${secret-name} \\ --from-literal=${key}=${value} For example: kubectl create secret generic redis-password \\ --from-literal=REDIS_PASSWORD=$(cat /dev/urandom | env LC_ALL=C tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1) Now that you have a secret in Kubernetes (use kubectl describe secret redis-password to look at it), all you have to do left is to mount it. Then you should also mount it to any applications that connects to the Redis instance. This is done by adding the following to your nais.yaml . spec : envFrom : - secret : redis-password Info See the redis-secure.yaml example in the examples above for setting up Redis with a password from the secret you just created.","title":"Secure Redis"},{"location":"persistence/redis/#code-examples","text":"We are not application developers, so please help us out by expanding with examples!","title":"Code examples"},{"location":"persistence/redis/#redis-cache-in-spring-boot","text":"Add the following to the Spring Boot application's application.yaml to enable Spring to use Redis as cache. session : store-type : redis redis : host : ${REDIS_HOST} port : 6379","title":"Redis cache in Spring Boot"},{"location":"persistence/responsibilities/","text":"Responsibilities \u00a7 Responsibilities \u00a7 Operating storage infrastructure \u00a7 For data stored in NAV's on-premises data centers, ITIP is operating the storage infrastructure For data stored in GCP, Google is operating the storage infrastructure. If your team uses data storage in GCP, [Behandlingskatalogen]<> should be updated to reflect that Google is a data processor For data stored in Azure, Microsoft is operating the storage infrastructure. If your team uses data storage in Azure, [Behandlingskatalogen]<> should be updated to reflect that Microsoft is a data processor Operating and maintaing tooling for provisioning and interfacing with storage \u00a7 For on-premises storage, either ITIP or the NAIS team is responsible for storage tooling and interfaces depending on the tool/interface For cloud storage, the NAIS team maintains some tooling (like provisioning for GCP buckets), while other tools/interfaces are operated by the cloud vendor Data contents, operations and compliance with data policies \u00a7 At the end of the day, the team is responsible for its own data. This includes compliance with data policies (e.g. GDPR or archiving), ensuring disaster recovery (aided by tooling and interfaces supplied by the platform) and daily operations.","title":"Responsibilities"},{"location":"persistence/responsibilities/#responsibilities","text":"","title":"Responsibilities"},{"location":"persistence/responsibilities/#responsibilities_1","text":"","title":"Responsibilities"},{"location":"persistence/responsibilities/#operating-storage-infrastructure","text":"For data stored in NAV's on-premises data centers, ITIP is operating the storage infrastructure For data stored in GCP, Google is operating the storage infrastructure. If your team uses data storage in GCP, [Behandlingskatalogen]<> should be updated to reflect that Google is a data processor For data stored in Azure, Microsoft is operating the storage infrastructure. If your team uses data storage in Azure, [Behandlingskatalogen]<> should be updated to reflect that Microsoft is a data processor","title":"Operating storage infrastructure"},{"location":"persistence/responsibilities/#operating-and-maintaing-tooling-for-provisioning-and-interfacing-with-storage","text":"For on-premises storage, either ITIP or the NAIS team is responsible for storage tooling and interfaces depending on the tool/interface For cloud storage, the NAIS team maintains some tooling (like provisioning for GCP buckets), while other tools/interfaces are operated by the cloud vendor","title":"Operating and maintaing tooling for provisioning and interfacing with storage"},{"location":"persistence/responsibilities/#data-contents-operations-and-compliance-with-data-policies","text":"At the end of the day, the team is responsible for its own data. This includes compliance with data policies (e.g. GDPR or archiving), ensuring disaster recovery (aided by tooling and interfaces supplied by the platform) and daily operations.","title":"Data contents, operations and compliance with data policies"},{"location":"persistence/volume-storage/","text":"On-premises disk \u00a7 The volume storage in NAIS is supported using the storage class rook-ceph. Rook handles the ceph cluster which in turn provides persistent voluments and persistent volume claims. This is most commonly used for elasticsearch installations and other solutions that require persistent volumes for databases. This is persistent storage and available in all clusters. The preferred solution is to use GCP for both applications and persistent volumes, but it is supported on-prem as well. The installation is done through [navikt/nais-tpa] with helm . This also supports dynamic resizing of the persistent volumes. Info This feature is only available in on-premises clusters. How to \u00a7 Installation of helm chart \u00a7 On NAIS on-prem we use nais-tpa to install helm charts with the defined persistent volume claims. When the chart is installed rook will initiate the persistent volumes required by the chart. Contact the nais team for questions using slack channel #nais-tpa . Example yaml for elasticsearch install: example-opendistro-elasticsearch.yaml name : example release : chart : nais/opendistro-elasticsearch:0.3 configuration : odfe : image : repository : amazon/opendistro-for-elasticsearch tag : 1.0.0 pullPolicy : IfNotPresent stateful : enabled : true class : \"rook-ceph-block\" serviceType : ClusterIP env : TEAM : \"team\" cluster.name : \"example\" security : enabled : true master : replicas : 3 resources : limits : cpu : 2 memory : 2048Mi requests : cpu : 0.2 memory : 1152Mi stateful : size : 4Gi env : ES_JAVA_OPTS : \"-Xms1024m -Xmx1024m\" ingest : replicas : 2 resources : limits : cpu : 2 memory : 2048Mi requests : cpu : 0.2 memory : 1152Mi env : ES_JAVA_OPTS : \"-Xms1024m -Xmx1024m\" data : replicas : 3 antiAffinity : \"soft\" resources : limits : cpu : 2 memory : 4096Mi requests : cpu : 0.2 memory : 2176Mi stateful : size : 24Gi env : ES_JAVA_OPTS : \"-Xms2048m -Xmx2048m\" kibana : replicas : 1 resources : limits : cpu : 1000m memory : 2048Mi requests : cpu : 100m env : ES_JAVA_OPTS : \"-Xmx1024m\" ingress : domain : nais.example.com If you are installing elasticsearch and the two client pods does not start as expected, log in to the master-1 pod using kubectl exec and run /sgadmin.sh Resizing volumes requires changing the persistent volume claim in kubernetes and then changing the helm chart definitions. Contact the nais team if you need the change the storage parameters. Metrics \u00a7 General ceph metrics are available from several dashboards in grafana: Grafana Ceph Cluster Grafana Ceph OSD Open Distro Elasticsearch metric referencel Code examples \u00a7 Feel free to help us out by adding examples!","title":"On-premises disk"},{"location":"persistence/volume-storage/#on-premises-disk","text":"The volume storage in NAIS is supported using the storage class rook-ceph. Rook handles the ceph cluster which in turn provides persistent voluments and persistent volume claims. This is most commonly used for elasticsearch installations and other solutions that require persistent volumes for databases. This is persistent storage and available in all clusters. The preferred solution is to use GCP for both applications and persistent volumes, but it is supported on-prem as well. The installation is done through [navikt/nais-tpa] with helm . This also supports dynamic resizing of the persistent volumes. Info This feature is only available in on-premises clusters.","title":"On-premises disk"},{"location":"persistence/volume-storage/#how-to","text":"","title":"How to"},{"location":"persistence/volume-storage/#installation-of-helm-chart","text":"On NAIS on-prem we use nais-tpa to install helm charts with the defined persistent volume claims. When the chart is installed rook will initiate the persistent volumes required by the chart. Contact the nais team for questions using slack channel #nais-tpa . Example yaml for elasticsearch install: example-opendistro-elasticsearch.yaml name : example release : chart : nais/opendistro-elasticsearch:0.3 configuration : odfe : image : repository : amazon/opendistro-for-elasticsearch tag : 1.0.0 pullPolicy : IfNotPresent stateful : enabled : true class : \"rook-ceph-block\" serviceType : ClusterIP env : TEAM : \"team\" cluster.name : \"example\" security : enabled : true master : replicas : 3 resources : limits : cpu : 2 memory : 2048Mi requests : cpu : 0.2 memory : 1152Mi stateful : size : 4Gi env : ES_JAVA_OPTS : \"-Xms1024m -Xmx1024m\" ingest : replicas : 2 resources : limits : cpu : 2 memory : 2048Mi requests : cpu : 0.2 memory : 1152Mi env : ES_JAVA_OPTS : \"-Xms1024m -Xmx1024m\" data : replicas : 3 antiAffinity : \"soft\" resources : limits : cpu : 2 memory : 4096Mi requests : cpu : 0.2 memory : 2176Mi stateful : size : 24Gi env : ES_JAVA_OPTS : \"-Xms2048m -Xmx2048m\" kibana : replicas : 1 resources : limits : cpu : 1000m memory : 2048Mi requests : cpu : 100m env : ES_JAVA_OPTS : \"-Xmx1024m\" ingress : domain : nais.example.com If you are installing elasticsearch and the two client pods does not start as expected, log in to the master-1 pod using kubectl exec and run /sgadmin.sh Resizing volumes requires changing the persistent volume claim in kubernetes and then changing the helm chart definitions. Contact the nais team if you need the change the storage parameters.","title":"Installation of helm chart"},{"location":"persistence/volume-storage/#metrics","text":"General ceph metrics are available from several dashboards in grafana: Grafana Ceph Cluster Grafana Ceph OSD Open Distro Elasticsearch metric referencel","title":"Metrics"},{"location":"persistence/volume-storage/#code-examples","text":"Feel free to help us out by adding examples!","title":"Code examples"},{"location":"persistence/kafka/","text":"Kafka \u00a7 Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation . Abstract \u00a7 NAV uses Aiven hosted Kafka. Access to Kafka is granted by defining a Topic resource in one of our Kubernetes clusters. Upon defining a Topic, Kafkarator will create the Topic in one of the Kafka pools . A pool is a highly available, replicated Kafka cluster running at Aiven. After the topic is created, Kafkarator will add relevant users to the topic's access control list (ACL). When an application that uses Kafka is deployed, Aivenator will orchestrate generation of users and credentials. These credentials are made available to applications through a Secret in the relevant team namespace. This secret is automatically mounted by Naiserator into application pods as environment variables and files. For a list of variables, see accessing topics from an application . Status and roadmap \u00a7 Release status: Generally Available Availability: NAIS GCP, NAIS on-premises, legacy infrastructure on-premises Follow development on the PIG-Aiven Trello board .","title":"Overview"},{"location":"persistence/kafka/#kafka","text":"Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation .","title":"Kafka"},{"location":"persistence/kafka/#abstract","text":"NAV uses Aiven hosted Kafka. Access to Kafka is granted by defining a Topic resource in one of our Kubernetes clusters. Upon defining a Topic, Kafkarator will create the Topic in one of the Kafka pools . A pool is a highly available, replicated Kafka cluster running at Aiven. After the topic is created, Kafkarator will add relevant users to the topic's access control list (ACL). When an application that uses Kafka is deployed, Aivenator will orchestrate generation of users and credentials. These credentials are made available to applications through a Secret in the relevant team namespace. This secret is automatically mounted by Naiserator into application pods as environment variables and files. For a list of variables, see accessing topics from an application .","title":"Abstract"},{"location":"persistence/kafka/#status-and-roadmap","text":"Release status: Generally Available Availability: NAIS GCP, NAIS on-premises, legacy infrastructure on-premises Follow development on the PIG-Aiven Trello board .","title":"Status and roadmap"},{"location":"persistence/kafka/application/","text":"Application \u00a7 Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation . Application config \u00a7 These variables are made available inside the pod. Variable name Description KAFKA_BROKERS Comma-separated list of HOST:PORT pairs to Kafka brokers KAFKA_SCHEMA_REGISTRY URL to schema registry KAFKA_SCHEMA_REGISTRY_USER Username to use with schema registry KAFKA_SCHEMA_REGISTRY_PASSWORD Password to use with schema registry KAFKA_CERTIFICATE Client certificate for connecting to the Kafka brokers, as string data KAFKA_CERTIFICATE_PATH Client certificate for connecting to the Kafka brokers, as file KAFKA_PRIVATE_KEY Client certificate key for connecting to the Kafka brokers, as string data KAFKA_PRIVATE_KEY_PATH Client certificate key for connecting to the Kafka brokers, as file KAFKA_CA Certificate authority used to validate the Kafka brokers, as string data KAFKA_CA_PATH Certificate authority used to validate the Kafka brokers, as file KAFKA_CREDSTORE_PASSWORD Password needed to use the keystore and truststore KAFKA_KEYSTORE_PATH PKCS#12 keystore for use with Java clients, as file KAFKA_TRUSTSTORE_PATH JKS truststore for use with Java clients, as file AIVEN_SECRET_UPDATED A timestamp of when the secret was created What happens on deploy? \u00a7 When you deploy an application that requests access to Kafka, Naiserator will create an AivenApplication resource in the cluster. The AivenApplication has a name that matches the deployed application, and the name of the secret to generate. Naiserator will request that a secret with this name used in the deployment. When an AivenApplication resource is created or updated, Aivenator will create a new service user and generate credentials. These credentials are then inserted into the requested secret and used in the deployment. If there is a problem generating the secret, this might fail your deployment. In this case, Aivenator will update the status part of the resource, with further information about the problem. Testing your application \u00a7 If you need to test that you have configured your consumer application correctly, you can use one of the kafkarator-canary topics. All applications (from all teams) have read access to the canary topics. All the canary topics receives a message on a fixed interval, containing a RFC3339 formatted timestamp. The only difference between each, is where the producer is located. The available canary topics are: Pool Topic name nav-dev aura.kafkarator-canary-dev-gcp nav-dev aura.kafkarator-canary-dev-sbs nav-dev aura.kafkarator-canary-dev-fss nav-prod aura.kafkarator-canary-prod-gcp nav-prod aura.kafkarator-canary-prod-sbs nav-prod aura.kafkarator-canary-prod-fss Using Kafka Streams with internal topics \u00a7 Info This feature is only available in GCP clusters. In some configurations of kafka streams your application needs to create internal topics. To allow your app to make internal topics, you need to set .spec.kafka.streams to true in your application spec (nais.yaml) When you do this you must configure Kafka Streams by setting the property application.id to the value of the env var KAFKA_STREAMS_APPLICATION_ID , which will be injected into your pod automatically. Accessing topics from an application on legacy infrastructure \u00a7 If you have an application on legacy infrastructure (outside NAIS clusters), you can still access topics with a few more manual steps. The first step is to add your application to the topic ACLs, the same way as for applications in NAIS clusters (see the previous section ). Use your team name, and a suitable name for the application, following NAIS naming conventions. To create a credentials for your application, you need to manually create the AivenApplication resource that would normally be created by Naiserator. aivenapp.yaml --- apiVersion : aiven.nais.io/v1 kind : AivenApplication metadata : name : legacyapplication namespace : myteam spec : kafka : pool : nav-dev secretName : unique-name protected : true Since Aivenator automatically deletes secrets that are not in use by any pod, you need to set the protected flag to true . This ensures that the secret will not be deleted by any automated process. After the AivenApplication resources has been created, Aivenator will create the secret, using the name specified. Using kubectl , extract the secret and make the values available to your legacy application. When you no longer have a need for the credentials created in this way, delete the AivenApplication resource, and make sure the secret is also deleted. If you migrate the application to NAIS, the first deploy to NAIS will overwrite the AivenApplication resource. When this happens, it is no longer protected . In this case, it is recommended that you manually delete the protected secret when it is no longer needed. Application design guidelines \u00a7 Authentication and authorization \u00a7 The NAIS platform will generate new credentials when your applications is deployed. Kafka requires TLS client certificates for authentication. Make sure your Kafka and/or TLS library can do client certificate authentication, and that you can specify a custom CA certificate for server validation. Readiness and liveness \u00a7 Making proper use of liveness and readiness probes can help with many situations. If producing or consuming Kafka messages are a vital part of your application, you should consider failing one or both probes if you have trouble with Kafka connectivity. Depending on your application, failing liveness might be the proper course of action. This will make sure your application is restarted when it is experiencing problems, which might help. In other cases, failing just the readiness probe will allow your application to continue running, attempting to move forward without being killed. Failing readiness will be most helpful during deployment, where the old instances will keep running until the new are ready. If the new instances are not able to connect to Kafka, keeping the old ones until the problem is resolved will allow your application to continue working.","title":"Application"},{"location":"persistence/kafka/application/#application","text":"Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation .","title":"Application"},{"location":"persistence/kafka/application/#application-config","text":"These variables are made available inside the pod. Variable name Description KAFKA_BROKERS Comma-separated list of HOST:PORT pairs to Kafka brokers KAFKA_SCHEMA_REGISTRY URL to schema registry KAFKA_SCHEMA_REGISTRY_USER Username to use with schema registry KAFKA_SCHEMA_REGISTRY_PASSWORD Password to use with schema registry KAFKA_CERTIFICATE Client certificate for connecting to the Kafka brokers, as string data KAFKA_CERTIFICATE_PATH Client certificate for connecting to the Kafka brokers, as file KAFKA_PRIVATE_KEY Client certificate key for connecting to the Kafka brokers, as string data KAFKA_PRIVATE_KEY_PATH Client certificate key for connecting to the Kafka brokers, as file KAFKA_CA Certificate authority used to validate the Kafka brokers, as string data KAFKA_CA_PATH Certificate authority used to validate the Kafka brokers, as file KAFKA_CREDSTORE_PASSWORD Password needed to use the keystore and truststore KAFKA_KEYSTORE_PATH PKCS#12 keystore for use with Java clients, as file KAFKA_TRUSTSTORE_PATH JKS truststore for use with Java clients, as file AIVEN_SECRET_UPDATED A timestamp of when the secret was created","title":"Application config"},{"location":"persistence/kafka/application/#what-happens-on-deploy","text":"When you deploy an application that requests access to Kafka, Naiserator will create an AivenApplication resource in the cluster. The AivenApplication has a name that matches the deployed application, and the name of the secret to generate. Naiserator will request that a secret with this name used in the deployment. When an AivenApplication resource is created or updated, Aivenator will create a new service user and generate credentials. These credentials are then inserted into the requested secret and used in the deployment. If there is a problem generating the secret, this might fail your deployment. In this case, Aivenator will update the status part of the resource, with further information about the problem.","title":"What happens on deploy?"},{"location":"persistence/kafka/application/#testing-your-application","text":"If you need to test that you have configured your consumer application correctly, you can use one of the kafkarator-canary topics. All applications (from all teams) have read access to the canary topics. All the canary topics receives a message on a fixed interval, containing a RFC3339 formatted timestamp. The only difference between each, is where the producer is located. The available canary topics are: Pool Topic name nav-dev aura.kafkarator-canary-dev-gcp nav-dev aura.kafkarator-canary-dev-sbs nav-dev aura.kafkarator-canary-dev-fss nav-prod aura.kafkarator-canary-prod-gcp nav-prod aura.kafkarator-canary-prod-sbs nav-prod aura.kafkarator-canary-prod-fss","title":"Testing your application"},{"location":"persistence/kafka/application/#using-kafka-streams-with-internal-topics","text":"Info This feature is only available in GCP clusters. In some configurations of kafka streams your application needs to create internal topics. To allow your app to make internal topics, you need to set .spec.kafka.streams to true in your application spec (nais.yaml) When you do this you must configure Kafka Streams by setting the property application.id to the value of the env var KAFKA_STREAMS_APPLICATION_ID , which will be injected into your pod automatically.","title":"Using Kafka Streams with internal topics"},{"location":"persistence/kafka/application/#accessing-topics-from-an-application-on-legacy-infrastructure","text":"If you have an application on legacy infrastructure (outside NAIS clusters), you can still access topics with a few more manual steps. The first step is to add your application to the topic ACLs, the same way as for applications in NAIS clusters (see the previous section ). Use your team name, and a suitable name for the application, following NAIS naming conventions. To create a credentials for your application, you need to manually create the AivenApplication resource that would normally be created by Naiserator. aivenapp.yaml --- apiVersion : aiven.nais.io/v1 kind : AivenApplication metadata : name : legacyapplication namespace : myteam spec : kafka : pool : nav-dev secretName : unique-name protected : true Since Aivenator automatically deletes secrets that are not in use by any pod, you need to set the protected flag to true . This ensures that the secret will not be deleted by any automated process. After the AivenApplication resources has been created, Aivenator will create the secret, using the name specified. Using kubectl , extract the secret and make the values available to your legacy application. When you no longer have a need for the credentials created in this way, delete the AivenApplication resource, and make sure the secret is also deleted. If you migrate the application to NAIS, the first deploy to NAIS will overwrite the AivenApplication resource. When this happens, it is no longer protected . In this case, it is recommended that you manually delete the protected secret when it is no longer needed.","title":"Accessing topics from an application on legacy infrastructure"},{"location":"persistence/kafka/application/#application-design-guidelines","text":"","title":"Application design guidelines"},{"location":"persistence/kafka/application/#authentication-and-authorization","text":"The NAIS platform will generate new credentials when your applications is deployed. Kafka requires TLS client certificates for authentication. Make sure your Kafka and/or TLS library can do client certificate authentication, and that you can specify a custom CA certificate for server validation.","title":"Authentication and authorization"},{"location":"persistence/kafka/application/#readiness-and-liveness","text":"Making proper use of liveness and readiness probes can help with many situations. If producing or consuming Kafka messages are a vital part of your application, you should consider failing one or both probes if you have trouble with Kafka connectivity. Depending on your application, failing liveness might be the proper course of action. This will make sure your application is restarted when it is experiencing problems, which might help. In other cases, failing just the readiness probe will allow your application to continue running, attempting to move forward without being killed. Failing readiness will be most helpful during deployment, where the old instances will keep running until the new are ready. If the new instances are not able to connect to Kafka, keeping the old ones until the problem is resolved will allow your application to continue working.","title":"Readiness and liveness"},{"location":"persistence/kafka/avro_and_rest/","text":"Avro and schema \u00a7 Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation . Delete schema \u00a7 You can delete schemaes (or versions) using the REST-API for the schema registry. The easiest way to communicate with the API is to use curl from one of your Kafka-pods, so that you have easy access to both the schema registry URL and the username/password. In order to delete version 10 of the schema registered under subject \"test-key\" (if it exists): $ curl -X DELETE -u brukernavn:passord http://$KAFKA_SCHEMA_REGISTRY/subjects/test-key/versions/10 10 To delete all versions of the schema registered under subject \"test-key\": $ curl -X DELETE -u brukernavn:passord http://$KAFKA_SCHEMA_REGISTRY/subjects/test-key [1] REST API \u00a7 For applications that can't use Kafka directly, a REST API is possible. Because of security implications, we have not enabled the REST API on the cluster, but interested parties may run their own instance. We have packaged Aivens Karapace project in a NAIS-friendly package. Teams can install Karapace in their own namespace with relevant access to provide a REST API for Kafka topics. Check the Karapace Readme for details.","title":"Avro and REST"},{"location":"persistence/kafka/avro_and_rest/#avro-and-schema","text":"Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation .","title":"Avro and schema"},{"location":"persistence/kafka/avro_and_rest/#delete-schema","text":"You can delete schemaes (or versions) using the REST-API for the schema registry. The easiest way to communicate with the API is to use curl from one of your Kafka-pods, so that you have easy access to both the schema registry URL and the username/password. In order to delete version 10 of the schema registered under subject \"test-key\" (if it exists): $ curl -X DELETE -u brukernavn:passord http://$KAFKA_SCHEMA_REGISTRY/subjects/test-key/versions/10 10 To delete all versions of the schema registered under subject \"test-key\": $ curl -X DELETE -u brukernavn:passord http://$KAFKA_SCHEMA_REGISTRY/subjects/test-key [1]","title":"Delete schema"},{"location":"persistence/kafka/avro_and_rest/#rest-api","text":"For applications that can't use Kafka directly, a REST API is possible. Because of security implications, we have not enabled the REST API on the cluster, but interested parties may run their own instance. We have packaged Aivens Karapace project in a NAIS-friendly package. Teams can install Karapace in their own namespace with relevant access to provide a REST API for Kafka topics. Check the Karapace Readme for details.","title":"REST API"},{"location":"persistence/kafka/faq/","text":"FAQ/Troubleshooting \u00a7 Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation . Why do I have to specify a pool name if there is only nav-dev and nav-prod ? \u00a7 Custom pools might be added in the future, so this is done to avoid changing that part of the API. I can't produce/consume on my topic, with an error message like \"topic not found\". What's wrong? \u00a7 You need to use the fully qualified name ; check the .status.fullyQualifiedName field in your Topic resource. I can't produce/consume on my topic, with an error message like \"not authorized\". What's wrong? \u00a7 Make sure you added the application to .spec.acl in your topic.yaml . I get the error MountVolume.SetUp failed for volume \"kafka-credentials\" : secret ... not found \u00a7 Check the status of the AivenApplication resource created by Naiserator to look for errors.","title":"FAQ"},{"location":"persistence/kafka/faq/#faqtroubleshooting","text":"Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation .","title":"FAQ/Troubleshooting"},{"location":"persistence/kafka/faq/#why-do-i-have-to-specify-a-pool-name-if-there-is-only-nav-dev-and-nav-prod","text":"Custom pools might be added in the future, so this is done to avoid changing that part of the API.","title":"Why do I have to specify a pool name if there is only nav-dev and nav-prod?"},{"location":"persistence/kafka/faq/#i-cant-produceconsume-on-my-topic-with-an-error-message-like-topic-not-found-whats-wrong","text":"You need to use the fully qualified name ; check the .status.fullyQualifiedName field in your Topic resource.","title":"I can't produce/consume on my topic, with an error message like \"topic not found\". What's wrong?"},{"location":"persistence/kafka/faq/#i-cant-produceconsume-on-my-topic-with-an-error-message-like-not-authorized-whats-wrong","text":"Make sure you added the application to .spec.acl in your topic.yaml .","title":"I can't produce/consume on my topic, with an error message like \"not authorized\". What's wrong?"},{"location":"persistence/kafka/faq/#i-get-the-error-mountvolumesetup-failed-for-volume-kafka-credentials-secret-not-found","text":"Check the status of the AivenApplication resource created by Naiserator to look for errors.","title":"I get the error MountVolume.SetUp failed for volume \"kafka-credentials\" : secret ... not found"},{"location":"persistence/kafka/manage_topics/","text":"Managing topics and access \u00a7 Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation . Creating topics and defining access \u00a7 Creating or modifying a Topic Kubernetes resource will trigger topic creation and ACL management with Aiven (hosted Kafka provider). The topic name will be prefixed with your team namespace, thus in the example below, the fully qualified topic name will be myteam.mytopic . This name will be set in the .status.fullyQualifiedName field on your Topic resource once the Topic is synchronized to Aiven. To add access to this topic for your application, see the next section: Accessing topics from an application . Topic resources can only be specified in GCP clusters. However, applications might access topics from any cluster, including on-premises. For details, read the next section. Currently, use the nav-dev pool for development, and nav-prod for production. If you need cross-environment communications, use the nav-infrastructure pool, but please consult the NAIS team before you do. Pool Min. replication Max. replication Topic declared in Available from nav-dev 2 3 dev-gcp dev-gcp , dev-fss , dev-sbs nav-prod 2 3 prod-gcp prod-gcp , prod-fss , prod-sbs nav-infrastructure 2 3 prod-gcp dev-gcp , dev-fss , dev-sbs , prod-gcp , prod-fss , prod-sbs topic.yaml --- apiVersion : kafka.nais.io/v1 kind : Topic metadata : name : mytopic namespace : myteam labels : team : myteam spec : pool : nav-dev config : # optional; all fields are optional too; defaults shown cleanupPolicy : delete # delete, compact, compact,delete minimumInSyncReplicas : 1 partitions : 1 replication : 3 # see min/max requirements retentionBytes : -1 # -1 means unlimited retentionHours : 72 # -1 means unlimited acl : - team : myteam application : ownerapp access : readwrite # read, write, readwrite - team : bigteam application : consumerapp1 access : read - team : bigteam application : consumerapp2 access : read - team : bigteam application : producerapp1 access : write - team : producerteam application : producerapp access : write Data catalog metadata \u00a7 If your topic exposes data meant for consumption by a wider audience, you should define some metadata describing the topic and its contents. This data will be automatically scraped and added to the internal data catalog . If the catalog key is set to public , the topic metadata is also published to the external data catalog and the National Data Catalog . Syntax: topic.yaml apiVersion: kafka.nais.io/v1 kind: Topic metadata: annotations: dcat.data.nav.no/<key>: \"<value>\" Use the following annotations and prefix them with dcat.data.nav.no/ . Default values will be used where not supplied. Key Importance Comment Example Value title mandatory String Inntektskjema mottatt fra Altinn topic name description mandatory String Inntektsmeldingen arbeidsgiveren sender fra eget l\u00f8nns- og personalsystem eller fra altinn.no theme recommended A main category of the resource. A resource can have multiple themes entered as a comma-separated list of strings. inntekt keyword recommended A string or a list of strings inntekt,arbeidsgiver,altinn One or more of the following keys can also be supplied if the default values below are not sufficient: Key Importance Comment Example Value Default value temporal optional An interval of time covered by the topic, start and end date. Formatted as two ISO 8601 dates (or datetimes) separated by a slash. 2020/2020 or 2020-06/2020-06 current year / current year language optional Two or three letter code. NO NO creator optional The entity responsible for producing the topic. An agent (eg. person, group, software or physical artifact). NAV team name publisher optional The entity responsible for making the topic available. An agent (eg. person, group, software or physical artifact). NAV NAV accessRights optional Information about who can access the topic or an indication of its security status. internal internal license optional Either a license URI or a title. MIT rights optional A statement that concerns all rights not addressed with license or accessRights , such as copyright statements. Copyright 2020, NAV Copyright year , NAV catalog optional The catalog(s) where the metadata will be published. The value can be either internal (only visibible within the organization) or public . public internal Permanently deleting topic and data \u00a7 Warning Permanent deletes are irreversible. Enable this feature only as a step to completely remove your data. When a Topic resource is deleted from a Kubernetes cluster, the Kafka topic is still retained, and the data kept intact. If you need to remove data and start from scratch, you must add the following annotation to your Topic resource: topic.yaml --- apiVersion : kafka.nais.io/v1 kind : Topic metadata : annotations : kafka.nais.io/removeDataWhenResourceIsDeleted : \"true\" When this annotation is in place, deleting the topic resource from Kubernetes will also delete the Kafka topic and all of its data. Accessing topics from an application \u00a7 Adding .kafka.pool to your Application spec will inject Kafka credentials into your pod. Your application needs to follow some design guidelines; see the next section on application design guidelines . Make sure that the topic name matches the fullyQualifiedName found in the Topic resource, e.g. myteam.mytopic . nais.yaml --- apiVersion : nais.io/v1alpha1 kind : Application metadata : name : consumerapp namespace : myteam labels : team : myteam spec : kafka : pool : nav-dev # enum of nav-dev, nav-prod","title":"Manage topics"},{"location":"persistence/kafka/manage_topics/#managing-topics-and-access","text":"Warning This feature applies only to Aiven hosted Kafka . On-premises Kafka is deprecated, and creating new topics on-premises was disabled summer 2021. For on-premises Kafka, see on-premises Kafka documentation .","title":"Managing topics and access"},{"location":"persistence/kafka/manage_topics/#creating-topics-and-defining-access","text":"Creating or modifying a Topic Kubernetes resource will trigger topic creation and ACL management with Aiven (hosted Kafka provider). The topic name will be prefixed with your team namespace, thus in the example below, the fully qualified topic name will be myteam.mytopic . This name will be set in the .status.fullyQualifiedName field on your Topic resource once the Topic is synchronized to Aiven. To add access to this topic for your application, see the next section: Accessing topics from an application . Topic resources can only be specified in GCP clusters. However, applications might access topics from any cluster, including on-premises. For details, read the next section. Currently, use the nav-dev pool for development, and nav-prod for production. If you need cross-environment communications, use the nav-infrastructure pool, but please consult the NAIS team before you do. Pool Min. replication Max. replication Topic declared in Available from nav-dev 2 3 dev-gcp dev-gcp , dev-fss , dev-sbs nav-prod 2 3 prod-gcp prod-gcp , prod-fss , prod-sbs nav-infrastructure 2 3 prod-gcp dev-gcp , dev-fss , dev-sbs , prod-gcp , prod-fss , prod-sbs topic.yaml --- apiVersion : kafka.nais.io/v1 kind : Topic metadata : name : mytopic namespace : myteam labels : team : myteam spec : pool : nav-dev config : # optional; all fields are optional too; defaults shown cleanupPolicy : delete # delete, compact, compact,delete minimumInSyncReplicas : 1 partitions : 1 replication : 3 # see min/max requirements retentionBytes : -1 # -1 means unlimited retentionHours : 72 # -1 means unlimited acl : - team : myteam application : ownerapp access : readwrite # read, write, readwrite - team : bigteam application : consumerapp1 access : read - team : bigteam application : consumerapp2 access : read - team : bigteam application : producerapp1 access : write - team : producerteam application : producerapp access : write","title":"Creating topics and defining access"},{"location":"persistence/kafka/manage_topics/#data-catalog-metadata","text":"If your topic exposes data meant for consumption by a wider audience, you should define some metadata describing the topic and its contents. This data will be automatically scraped and added to the internal data catalog . If the catalog key is set to public , the topic metadata is also published to the external data catalog and the National Data Catalog . Syntax: topic.yaml apiVersion: kafka.nais.io/v1 kind: Topic metadata: annotations: dcat.data.nav.no/<key>: \"<value>\" Use the following annotations and prefix them with dcat.data.nav.no/ . Default values will be used where not supplied. Key Importance Comment Example Value title mandatory String Inntektskjema mottatt fra Altinn topic name description mandatory String Inntektsmeldingen arbeidsgiveren sender fra eget l\u00f8nns- og personalsystem eller fra altinn.no theme recommended A main category of the resource. A resource can have multiple themes entered as a comma-separated list of strings. inntekt keyword recommended A string or a list of strings inntekt,arbeidsgiver,altinn One or more of the following keys can also be supplied if the default values below are not sufficient: Key Importance Comment Example Value Default value temporal optional An interval of time covered by the topic, start and end date. Formatted as two ISO 8601 dates (or datetimes) separated by a slash. 2020/2020 or 2020-06/2020-06 current year / current year language optional Two or three letter code. NO NO creator optional The entity responsible for producing the topic. An agent (eg. person, group, software or physical artifact). NAV team name publisher optional The entity responsible for making the topic available. An agent (eg. person, group, software or physical artifact). NAV NAV accessRights optional Information about who can access the topic or an indication of its security status. internal internal license optional Either a license URI or a title. MIT rights optional A statement that concerns all rights not addressed with license or accessRights , such as copyright statements. Copyright 2020, NAV Copyright year , NAV catalog optional The catalog(s) where the metadata will be published. The value can be either internal (only visibible within the organization) or public . public internal","title":"Data catalog metadata"},{"location":"persistence/kafka/manage_topics/#permanently-deleting-topic-and-data","text":"Warning Permanent deletes are irreversible. Enable this feature only as a step to completely remove your data. When a Topic resource is deleted from a Kubernetes cluster, the Kafka topic is still retained, and the data kept intact. If you need to remove data and start from scratch, you must add the following annotation to your Topic resource: topic.yaml --- apiVersion : kafka.nais.io/v1 kind : Topic metadata : annotations : kafka.nais.io/removeDataWhenResourceIsDeleted : \"true\" When this annotation is in place, deleting the topic resource from Kubernetes will also delete the Kafka topic and all of its data.","title":"Permanently deleting topic and data"},{"location":"persistence/kafka/manage_topics/#accessing-topics-from-an-application","text":"Adding .kafka.pool to your Application spec will inject Kafka credentials into your pod. Your application needs to follow some design guidelines; see the next section on application design guidelines . Make sure that the topic name matches the fullyQualifiedName found in the Topic resource, e.g. myteam.mytopic . nais.yaml --- apiVersion : nais.io/v1alpha1 kind : Application metadata : name : consumerapp namespace : myteam labels : team : myteam spec : kafka : pool : nav-dev # enum of nav-dev, nav-prod","title":"Accessing topics from an application"},{"location":"persistence/kafka/metrics/","text":"Grafana metrics \u00a7 This is a user-generated list of metrics that can be used with Grafana to monitor your Kafka topics. Since you can use Kafka both from on-prem and GCP, the metrics are either read from the datasource where your app is running, or from the datasource where the topics belong (topics can only be created in GCP). GCP apps will use the same datasource for all listed metrics. General tips \u00a7 kafka_producer_topic_record_send_total is a good metric for the total amount of produced messages on a topic. kafka_consumer_fetch_manager_records_consumed_total is a good metric for consumed messages of a topic. kafka_consumergroup_group_topic_sum_lag can be used to follow the offset of a consumer group. kafka_server_BrokerTopicMetrics_MessagesInPerSec_Count can be used to follow the number of messages produced for a topic. kafka_server_BrokerTopicMetrics_BytesInPerSec_Count can be used to follow the amount of bytes produced for a topic. Metric examples \u00a7 For watching how many messages are produced hourly on a specific topic: sum(increase((kafka_producer_topic_record_send_total{topic=\"TOPIC\"}[1h]))) For watching how many messages are consumed hourly on a specific topic: sum(increase((kafka_consumer_fetch_manager_records_consumed_total{topic=\"TOPIC\"}[1h]))) For following the topic offset for a consumer group: sum(kafka_consumergroup_group_topic_sum_lag{topic=\"TOPIC\",group=\"GROUP\"}) Amount of messages on a topic per seconds: sum by(topic) (rate(kafka_server_BrokerTopicMetrics_MessagesInPerSec_Count{topic=\"TOPIC\"}[2m])) Bytes produces on a topic per second: sum by(topic) (rate(kafka_server_BrokerTopicMetrics_BytesInPerSec_Count{topic=\"TOPIC\"}[2m])) Read more \u00a7 https://help.aiven.io/en/articles/3298562-kafka-metrics-through-prometheus-integration","title":"Grafana metrics"},{"location":"persistence/kafka/metrics/#grafana-metrics","text":"This is a user-generated list of metrics that can be used with Grafana to monitor your Kafka topics. Since you can use Kafka both from on-prem and GCP, the metrics are either read from the datasource where your app is running, or from the datasource where the topics belong (topics can only be created in GCP). GCP apps will use the same datasource for all listed metrics.","title":"Grafana metrics"},{"location":"persistence/kafka/metrics/#general-tips","text":"kafka_producer_topic_record_send_total is a good metric for the total amount of produced messages on a topic. kafka_consumer_fetch_manager_records_consumed_total is a good metric for consumed messages of a topic. kafka_consumergroup_group_topic_sum_lag can be used to follow the offset of a consumer group. kafka_server_BrokerTopicMetrics_MessagesInPerSec_Count can be used to follow the number of messages produced for a topic. kafka_server_BrokerTopicMetrics_BytesInPerSec_Count can be used to follow the amount of bytes produced for a topic.","title":"General tips"},{"location":"persistence/kafka/metrics/#metric-examples","text":"For watching how many messages are produced hourly on a specific topic: sum(increase((kafka_producer_topic_record_send_total{topic=\"TOPIC\"}[1h]))) For watching how many messages are consumed hourly on a specific topic: sum(increase((kafka_consumer_fetch_manager_records_consumed_total{topic=\"TOPIC\"}[1h]))) For following the topic offset for a consumer group: sum(kafka_consumergroup_group_topic_sum_lag{topic=\"TOPIC\",group=\"GROUP\"}) Amount of messages on a topic per seconds: sum by(topic) (rate(kafka_server_BrokerTopicMetrics_MessagesInPerSec_Count{topic=\"TOPIC\"}[2m])) Bytes produces on a topic per second: sum by(topic) (rate(kafka_server_BrokerTopicMetrics_BytesInPerSec_Count{topic=\"TOPIC\"}[2m]))","title":"Metric examples"},{"location":"persistence/kafka/metrics/#read-more","text":"https://help.aiven.io/en/articles/3298562-kafka-metrics-through-prometheus-integration","title":"Read more"},{"location":"persistence/kafka/migrate_from_onprem/","text":"Migrating from on-prem to Aiven \u00a7 There are multiple viable strategies for migrating to Aiven. Which one is best for your team depends on many factors. Here we will describe a few approaches, other approaches are also possible. You need to find the one that suits your situation best. Make sure you understand how the different approaches affects your message processing. Some approaches might result in messages being processed more than once, and some approaches might allow for messages to be processed out of order. If you are using the Schema Registry, you need to make sure that your schemas are uploaded to the new schema registry in Aiven when migrating. How you do that depends on how you do your migration. The first step is always: Create the topic using the new Topic resource , and set proper ACLs The final step should always be: Remove the topic from the on-prem clusters, making sure to delete all data The simplest case \u00a7 This approach is suitable if: You don't need to keep historical data You control all producers and consumers of the topic A short pause in processing of messages is acceptable The process is quite simple: Change producers to write messages to Aiven Wait for consumers to process the last messages on-prem Change consumers to read messages from Aiven Double producer \u00a7 This approach is suitable if: You don't need to keep historical data You control the producer Your messages are idempotent Follow these steps: Change producers to write messages both to on-prem and to Aiven Change consumers to read messages from Aiven, allowing for the fact that some messages will be processed twice (once when read from on-prem, once when read from Aiven) When all consumers are reading from Aiven, remove old code from producer Double consumers \u00a7 This approach is suitable if: You don't need to keep historical data The consumers are easier to change than the producer, or there are multiple producers Follow these steps: Change all consumers to read messages from both on-prem and Aiven Change producer(s) to write messages to Aiven Wait for consumers to process last messages on-prem Remove old code from consumers Simple mirroring \u00a7 This approach is suitable if: You have historical data you wish to keep You can afford a pause in processing of messages You don't use Schema Registry Follow these steps: Create a mirroring application that reads messages from on-prem and writes them to Aiven (The NAIS team has created AiviA for this) Stop producers Wait for mirroring to catch up Change consumers to read messages from Aiven Change producer to write messages to Aiven Delete mirroring application Advanced mirroring \u00a7 This approach is more complicated and requires considerable effort to get set up and working correctly. If possible we want to avoid having to do this. This approach is suitable if: You have historical data you wish to keep You can't afford a pause in processing of messages Your messages are not idempotent You require migrating Schemas in sync with migrating messages Follow these steps: Contact us to discuss if you really need this Wait while we set up and configure MirrorMaker to handle the mirroring of your topic, including schemas and consumer offsets Change consumers to read messages from Aiven Change producer to write messages to Aiven Remove MirrorMaker configuration (or let us know that we can remove it)","title":"Migrate from onprem"},{"location":"persistence/kafka/migrate_from_onprem/#migrating-from-on-prem-to-aiven","text":"There are multiple viable strategies for migrating to Aiven. Which one is best for your team depends on many factors. Here we will describe a few approaches, other approaches are also possible. You need to find the one that suits your situation best. Make sure you understand how the different approaches affects your message processing. Some approaches might result in messages being processed more than once, and some approaches might allow for messages to be processed out of order. If you are using the Schema Registry, you need to make sure that your schemas are uploaded to the new schema registry in Aiven when migrating. How you do that depends on how you do your migration. The first step is always: Create the topic using the new Topic resource , and set proper ACLs The final step should always be: Remove the topic from the on-prem clusters, making sure to delete all data","title":"Migrating from on-prem to Aiven"},{"location":"persistence/kafka/migrate_from_onprem/#the-simplest-case","text":"This approach is suitable if: You don't need to keep historical data You control all producers and consumers of the topic A short pause in processing of messages is acceptable The process is quite simple: Change producers to write messages to Aiven Wait for consumers to process the last messages on-prem Change consumers to read messages from Aiven","title":"The simplest case"},{"location":"persistence/kafka/migrate_from_onprem/#double-producer","text":"This approach is suitable if: You don't need to keep historical data You control the producer Your messages are idempotent Follow these steps: Change producers to write messages both to on-prem and to Aiven Change consumers to read messages from Aiven, allowing for the fact that some messages will be processed twice (once when read from on-prem, once when read from Aiven) When all consumers are reading from Aiven, remove old code from producer","title":"Double producer"},{"location":"persistence/kafka/migrate_from_onprem/#double-consumers","text":"This approach is suitable if: You don't need to keep historical data The consumers are easier to change than the producer, or there are multiple producers Follow these steps: Change all consumers to read messages from both on-prem and Aiven Change producer(s) to write messages to Aiven Wait for consumers to process last messages on-prem Remove old code from consumers","title":"Double consumers"},{"location":"persistence/kafka/migrate_from_onprem/#simple-mirroring","text":"This approach is suitable if: You have historical data you wish to keep You can afford a pause in processing of messages You don't use Schema Registry Follow these steps: Create a mirroring application that reads messages from on-prem and writes them to Aiven (The NAIS team has created AiviA for this) Stop producers Wait for mirroring to catch up Change consumers to read messages from Aiven Change producer to write messages to Aiven Delete mirroring application","title":"Simple mirroring"},{"location":"persistence/kafka/migrate_from_onprem/#advanced-mirroring","text":"This approach is more complicated and requires considerable effort to get set up and working correctly. If possible we want to avoid having to do this. This approach is suitable if: You have historical data you wish to keep You can't afford a pause in processing of messages Your messages are not idempotent You require migrating Schemas in sync with migrating messages Follow these steps: Contact us to discuss if you really need this Wait while we set up and configure MirrorMaker to handle the mirroring of your topic, including schemas and consumer offsets Change consumers to read messages from Aiven Change producer to write messages to Aiven Remove MirrorMaker configuration (or let us know that we can remove it)","title":"Advanced mirroring"},{"location":"persistence/kafka/offsets/","text":"Working with Kafka Offsets \u00a7 Retention and what it means \u00a7 On Aiven Kafka, we retain consumer offsets for a period of 7 days. This is the period recommended by Aiven and the default for Kafka. This is in contrast to On-Prem Kafka, where we have retained consumer offsets for several months. Due to how longer offset retention affects other parts of Kafka, we do not want to increase this period. When a consumer group stops consuming messages, its offsets will be retained for the mentioned period. How Kafka decides if a consumer group has stopped comes in two variations: Dynamically assigned partitions \u00a7 This is the normal operation when using current Kafka client libraries. In this case, Kafka assigns partitions to the consumers as they connect, and manages group membership. A consumer group is considered empty when there are no connected consumers with assigned partitions. Once the group is empty, Kafka retains the offsets for 7 days. Manually assigned partitions \u00a7 When using the assign API you are responsible for keeping track of consumers. In this scenario, Kafka uses the time of the last commit to determine offset retention. Offsets are kept for 7 days after the last commit. Warning This means that when using manual assignment on a topic with long periods of inactivity (more than 7 days between messages), you might lose offsets even if your consumer is running and committing offsets as it should. Do you even need offsets? \u00a7 Some scenarios don't actually need to track offsets, and can consider disabling the feature for a slight performance gain. In these situations, you can set enable.auto.commit=false , and simply not commit offsets. There are two main variations of this scenario: Always reading the entire topic from start to end. Set auto.offset.reset=earliest . Only caring about fresh messages arriving after the consumer connects. Set auto.offset.reset=latest . Autocommit: When/Why/Why not? \u00a7 When starting out with Kafka, it is common to use the autocommit feature available in the client libraries. This makes it easy to get started, and often provides good enough semantics. When you use autocommit, the client will automatically commit the last received offsets at a set interval, configured using auto.commit.interval.ms . The implementation ensures that you get \"at-least-once\" semantics, where the worst case scenario is to reprocess messages received in the interval between last commit and the consumer stopping. One downside with this mechanism is that you have little control over when offsets are committed. Autocommit is done before a poll to the server, which means that your consumer needs to ensure that has completed processing of a message before the next call to poll . If your consumer processes messages in other threads, you probably need to manage offsets explicitly and not rely on autocommit. Managing offsets explicitly \u00a7 The KafkaConsumer exposes two APIs for committing offsets. Asynchronous commits using commitAsync and synchronous commits using commitSync . From the Confluent documentation : Each call to the commit API results in an offset commit request being sent to the broker. Using the synchronous API, the consumer is blocked until that request returns successfully. This may reduce overall throughput since the consumer might otherwise be able to process records while that commit is pending. A second option is to use asynchronous commits. Instead of waiting for the request to complete, the consumer can send the request and return immediately by using asynchronous commits. In general, asynchronous commits should be considered less safe than synchronous commits. Saving offsets elsewhere \u00a7 The consumer application need not use Kafka's built-in offset storage, it can store offsets in a store of its own choosing. The primary use case for this is allowing the application to store both the offset and the results of the consumption in the same system in a way that both the results and offsets are stored atomically. Another use case is when you have consumers that receive messages very rarely, where consumer inactivity and other incidents might lead to lost offsets because of shorter retention. In some cases it might even be beneficial to store offsets in alternative storage even if your messages are not. This will avoid issues with offsets passing beyond the retention threshold, in case of recurring errors or networking issues. When storing offsets outside Kafka, your consumer needs to pay attention to rebalance events, to ensure correct offset management. In these cases it might be easier to also manage partition assignment explicitly . Before storing offsets outside Kafka, consult the Kafka documentation on the topic. What to do when you lose your offsets \u00a7 Shit happens, and you may experience lost offsets even if you've done everything right. In these cases, having a good plan for recovery can be crucial. Depending on your application, there are several paths to recovery, some more complicated than others. In the best of cases, you can simply start your consumer from either earliest or latest offsets and process normally. If you can accept reprocessing everything, set auto.offset.reset=earliest . If you can accept missing a few messages, set auto.offset.reset=latest . If neither of those are the case, your path becomes more complicated, and it is probably best to set auto.offset.reset=none . If you need to assess or manually handle the situation before continuing, setting auto.offset.reset to none will make your application fail immediately after offsets are lost. Trying to recover from lost offsets are considerably more complicated after your consumer has been doing the wrong thing for an hour. If you don't want to start at either end, but have a reasonable estimate of where your consumer stopped, you can use the seek API to jump to the wanted offset before starting your consumer. You can also update consumer offsets using the Kafka command-line tool kafka-consumer-groups.sh . Aiven has written a short article about its usage, that is a great place to start. In order to use it you need credentials giving you access to the topic, which you can get using the nais cli . For other strategies, post a message in #kafka on slack, and ask for help. Several teams have plans and tools for recovery that they can share. Getting estimates for last offset \u00a7 Finding a good estimate for where your last offset was can be tricky. One place to go is Prometheus. In our clusters, we have kafka-lag-exporter running. This tracks various offset-related metrics, one of which is the last seen offset for a consumer group. You can use this query to get offsets for a consumer group: max(kafka_consumergroup_group_offset{group=\"spedisjon-v1\"}) by (topic, partition)","title":"Working with Kafka Offsets"},{"location":"persistence/kafka/offsets/#working-with-kafka-offsets","text":"","title":"Working with Kafka Offsets"},{"location":"persistence/kafka/offsets/#retention-and-what-it-means","text":"On Aiven Kafka, we retain consumer offsets for a period of 7 days. This is the period recommended by Aiven and the default for Kafka. This is in contrast to On-Prem Kafka, where we have retained consumer offsets for several months. Due to how longer offset retention affects other parts of Kafka, we do not want to increase this period. When a consumer group stops consuming messages, its offsets will be retained for the mentioned period. How Kafka decides if a consumer group has stopped comes in two variations:","title":"Retention and what it means"},{"location":"persistence/kafka/offsets/#dynamically-assigned-partitions","text":"This is the normal operation when using current Kafka client libraries. In this case, Kafka assigns partitions to the consumers as they connect, and manages group membership. A consumer group is considered empty when there are no connected consumers with assigned partitions. Once the group is empty, Kafka retains the offsets for 7 days.","title":"Dynamically assigned partitions"},{"location":"persistence/kafka/offsets/#manually-assigned-partitions","text":"When using the assign API you are responsible for keeping track of consumers. In this scenario, Kafka uses the time of the last commit to determine offset retention. Offsets are kept for 7 days after the last commit. Warning This means that when using manual assignment on a topic with long periods of inactivity (more than 7 days between messages), you might lose offsets even if your consumer is running and committing offsets as it should.","title":"Manually assigned partitions"},{"location":"persistence/kafka/offsets/#do-you-even-need-offsets","text":"Some scenarios don't actually need to track offsets, and can consider disabling the feature for a slight performance gain. In these situations, you can set enable.auto.commit=false , and simply not commit offsets. There are two main variations of this scenario: Always reading the entire topic from start to end. Set auto.offset.reset=earliest . Only caring about fresh messages arriving after the consumer connects. Set auto.offset.reset=latest .","title":"Do you even need offsets?"},{"location":"persistence/kafka/offsets/#autocommit-whenwhywhy-not","text":"When starting out with Kafka, it is common to use the autocommit feature available in the client libraries. This makes it easy to get started, and often provides good enough semantics. When you use autocommit, the client will automatically commit the last received offsets at a set interval, configured using auto.commit.interval.ms . The implementation ensures that you get \"at-least-once\" semantics, where the worst case scenario is to reprocess messages received in the interval between last commit and the consumer stopping. One downside with this mechanism is that you have little control over when offsets are committed. Autocommit is done before a poll to the server, which means that your consumer needs to ensure that has completed processing of a message before the next call to poll . If your consumer processes messages in other threads, you probably need to manage offsets explicitly and not rely on autocommit.","title":"Autocommit: When/Why/Why not?"},{"location":"persistence/kafka/offsets/#managing-offsets-explicitly","text":"The KafkaConsumer exposes two APIs for committing offsets. Asynchronous commits using commitAsync and synchronous commits using commitSync . From the Confluent documentation : Each call to the commit API results in an offset commit request being sent to the broker. Using the synchronous API, the consumer is blocked until that request returns successfully. This may reduce overall throughput since the consumer might otherwise be able to process records while that commit is pending. A second option is to use asynchronous commits. Instead of waiting for the request to complete, the consumer can send the request and return immediately by using asynchronous commits. In general, asynchronous commits should be considered less safe than synchronous commits.","title":"Managing offsets explicitly"},{"location":"persistence/kafka/offsets/#saving-offsets-elsewhere","text":"The consumer application need not use Kafka's built-in offset storage, it can store offsets in a store of its own choosing. The primary use case for this is allowing the application to store both the offset and the results of the consumption in the same system in a way that both the results and offsets are stored atomically. Another use case is when you have consumers that receive messages very rarely, where consumer inactivity and other incidents might lead to lost offsets because of shorter retention. In some cases it might even be beneficial to store offsets in alternative storage even if your messages are not. This will avoid issues with offsets passing beyond the retention threshold, in case of recurring errors or networking issues. When storing offsets outside Kafka, your consumer needs to pay attention to rebalance events, to ensure correct offset management. In these cases it might be easier to also manage partition assignment explicitly . Before storing offsets outside Kafka, consult the Kafka documentation on the topic.","title":"Saving offsets elsewhere"},{"location":"persistence/kafka/offsets/#what-to-do-when-you-lose-your-offsets","text":"Shit happens, and you may experience lost offsets even if you've done everything right. In these cases, having a good plan for recovery can be crucial. Depending on your application, there are several paths to recovery, some more complicated than others. In the best of cases, you can simply start your consumer from either earliest or latest offsets and process normally. If you can accept reprocessing everything, set auto.offset.reset=earliest . If you can accept missing a few messages, set auto.offset.reset=latest . If neither of those are the case, your path becomes more complicated, and it is probably best to set auto.offset.reset=none . If you need to assess or manually handle the situation before continuing, setting auto.offset.reset to none will make your application fail immediately after offsets are lost. Trying to recover from lost offsets are considerably more complicated after your consumer has been doing the wrong thing for an hour. If you don't want to start at either end, but have a reasonable estimate of where your consumer stopped, you can use the seek API to jump to the wanted offset before starting your consumer. You can also update consumer offsets using the Kafka command-line tool kafka-consumer-groups.sh . Aiven has written a short article about its usage, that is a great place to start. In order to use it you need credentials giving you access to the topic, which you can get using the nais cli . For other strategies, post a message in #kafka on slack, and ask for help. Several teams have plans and tools for recovery that they can share.","title":"What to do when you lose your offsets"},{"location":"persistence/kafka/offsets/#getting-estimates-for-last-offset","text":"Finding a good estimate for where your last offset was can be tricky. One place to go is Prometheus. In our clusters, we have kafka-lag-exporter running. This tracks various offset-related metrics, one of which is the last seen offset for a consumer group. You can use this query to get offsets for a consumer group: max(kafka_consumergroup_group_offset{group=\"spedisjon-v1\"}) by (topic, partition)","title":"Getting estimates for last offset"},{"location":"security/antivirus/","text":"Anti-virus Scanning \u00a7 ClamAV \u00a7 This feature is installed on all nais clusters. clamAV runs in its own pod with a separate pod running the REST api applications use. There is a service set up so all applications will be able to talk to the REST api using http://clamav.nais.svc.nais.local/scan on-prem and http://clamav.clamav.svc.cluster.local on GCP. The REST api supports PUT or POST and can be tested using curl as well: # Examples using the on-prem url curl -v -X POST -H \"Content-Type: multipart/form-data\" -F \"file1=@/tmp/file_to_test\" http://clamav.nais.svc.nais.local/scan curl -v -X PUT --data-binary @/tmp/file_to_test http://clamav.nais.svc.nais.local/scan curl -v http://clamav.nais.svc.nais.local/scan?url=url_to_file See REST api documentation and clamAV documentation When using ClamAV on GCP, remember to add an outbound access policy : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : clamav namespace : clamav If you have any questions about clamAV please contact the nais team on the nais slack channel or contact @Sten.Ivar.R\u00f8kke . Examples \u00a7 Code example can be found here: foreldrepenger-api","title":"Anti-virus Scanning"},{"location":"security/antivirus/#anti-virus-scanning","text":"","title":"Anti-virus Scanning"},{"location":"security/antivirus/#clamav","text":"This feature is installed on all nais clusters. clamAV runs in its own pod with a separate pod running the REST api applications use. There is a service set up so all applications will be able to talk to the REST api using http://clamav.nais.svc.nais.local/scan on-prem and http://clamav.clamav.svc.cluster.local on GCP. The REST api supports PUT or POST and can be tested using curl as well: # Examples using the on-prem url curl -v -X POST -H \"Content-Type: multipart/form-data\" -F \"file1=@/tmp/file_to_test\" http://clamav.nais.svc.nais.local/scan curl -v -X PUT --data-binary @/tmp/file_to_test http://clamav.nais.svc.nais.local/scan curl -v http://clamav.nais.svc.nais.local/scan?url=url_to_file See REST api documentation and clamAV documentation When using ClamAV on GCP, remember to add an outbound access policy : apiVersion : \"nais.io/v1alpha1\" kind : \"Application\" metadata : name : app-a ... spec : ... accessPolicy : outbound : rules : - application : clamav namespace : clamav If you have any questions about clamAV please contact the nais team on the nais slack channel or contact @Sten.Ivar.R\u00f8kke .","title":"ClamAV"},{"location":"security/antivirus/#examples","text":"Code example can be found here: foreldrepenger-api","title":"Examples"},{"location":"security/auth/","text":"Authentication and Authorization - Overview \u00a7 Introduction to OAuth 2.0 / OpenID Connect \u00a7 OpenID Connect (OIDC) and OAuth 2.0 are the preferred specifications to provide end user authentication and ensure secure service-to-service communication for applications running on the platform. In short, OpenID Connect is used to delegate end user authentication to a third party (e.g. Azure AD ), while the OAuth 2.0 protocol can provide signed tokens ( JWT ) for service-to-service communication. NAV-specific Security Guide \u00a7 As OAuth 2.0, OpenID Connect, and the variety of \"flows\" within those specifications can be complex and \"large\", we aim to reduce the cognitive load on the common developer by providing a guide and blueprints for the most common scenarios in NAV. Info Please consult the NAV Security Guide (internal access required) for details on the usage of these specifications and protocols within NAV. This guide also includes details and examples on how to do AuthNZ against legacy apps that are not yet part of the security model discussed here. Citizen-facing applications \u00a7 See ID-porten and TokenX Employee-facing applications \u00a7 See Azure AD","title":"Auth Overview"},{"location":"security/auth/#authentication-and-authorization-overview","text":"","title":"Authentication and Authorization - Overview"},{"location":"security/auth/#introduction-to-oauth-20-openid-connect","text":"OpenID Connect (OIDC) and OAuth 2.0 are the preferred specifications to provide end user authentication and ensure secure service-to-service communication for applications running on the platform. In short, OpenID Connect is used to delegate end user authentication to a third party (e.g. Azure AD ), while the OAuth 2.0 protocol can provide signed tokens ( JWT ) for service-to-service communication.","title":"Introduction to OAuth 2.0 / OpenID Connect"},{"location":"security/auth/#nav-specific-security-guide","text":"As OAuth 2.0, OpenID Connect, and the variety of \"flows\" within those specifications can be complex and \"large\", we aim to reduce the cognitive load on the common developer by providing a guide and blueprints for the most common scenarios in NAV. Info Please consult the NAV Security Guide (internal access required) for details on the usage of these specifications and protocols within NAV. This guide also includes details and examples on how to do AuthNZ against legacy apps that are not yet part of the security model discussed here.","title":"NAV-specific Security Guide"},{"location":"security/auth/#citizen-facing-applications","text":"See ID-porten and TokenX","title":"Citizen-facing applications"},{"location":"security/auth/#employee-facing-applications","text":"See Azure AD","title":"Employee-facing applications"},{"location":"security/auth/tokenx/","text":"TokenX \u00a7 Status: Opt-In Open Beta This feature is only available in team namespaces . It currently only supports self-service/citizen facing applications - however other use-cases have been identified. Abstract \u00a7 Abstract What is TokenX? \u00a7 TokenX is the short term for OAuth 2.0 Token Exchange implemented in the context of Kubernetes. It consists of mainly 3 components: Tokendings - an OAuth 2.0 Authorization Server implementing the OAuth 2.0 Token Exchange specification Jwker - a Kubernetes operator responsible for registering applications as OAuth 2.0 clients in Tokendings Naiserator - a Kubernetes operator that handles the lifecycle of applications on the NAIS platform In short, TokenX is a OAuth 2.0 compliant add-on that enables and allows your application to maintain the zero trust networking principles (together with components such as LinkerD ). It does this by allowing applications to exchange and acquire properly scoped security tokens in order to securely communicate with each other. Interested readers may find more technical details in the Tokendings documentation . Why do I need TokenX? \u00a7 In a zero trust architecture, one cannot rely on traditional boundaries such as security zones and security gateways . Such security measures are no longer required for applications that leverage TokenX correctly as each application is self-contained within its own zone; requiring specific tokens in order to communicate with other applications. Using TokenX correctly throughout a call-chain also ensures that the identity of the original caller or subject (e.g. an end-user) is propagated while still maintaining proper scoping and security between each application. When do I need TokenX? \u00a7 There are primarily two distinct cases where one must use TokenX: You have a user facing app using ID-porten that should perform calls to another app on behalf of a user. You have an app receiving tokens issued from Tokendings and need to call another app while still propagating the original user context. Overview of flow Configuration \u00a7 Spec \u00a7 See the NAIS manifest . Getting Started \u00a7 nais.yaml spec : tokenx : enabled : true accessPolicy : inbound : rules : - application : app-2 - application : app-3 namespace : team-a - application : app-4 namespace : team-b cluster : prod-gcp Access Policies \u00a7 In order for other applications to acquire a token targeting your application, you must explicitly specify inbound access policies that authorizes these other applications. Thus, the access policies defines authorization on the application layer, and is enforced by Tokendings on token exchange operations. For example: spec : tokenx : enabled : true accessPolicy : inbound : rules : - application : app-1 - application : app-2 namespace : team-a - application : app-3 namespace : team-b cluster : prod-gcp The above configuration authorizes the following applications: application app-1 running in the same namespace and same cluster as your application application app-2 running in the namespace team-a in the same cluster application app-3 running in the namespace team-b in the cluster prod-gcp Usage \u00a7 Info See the NAV Security Guide for NAV-specific usage. Runtime Variables & Credentials \u00a7 Enabling TokenX will expose the following runtime environment variables and files (under the directory /var/run/secrets/nais.io/jwker ) for your application: TOKEN_X_WELL_KNOWN_URL \u00a7 Note The well-known URL of the OAuth 2.0 Token Exchange authorization server, in this case Tokendings. This URL contains the server metadata as defined in RFC8414 that your application may use. For example: issuer token_endpoint jwks_uri See OAuth 2.0 Authorization Server Metadata for more information about the contents of the response from the well-known url. TOKEN_X_CLIENT_ID \u00a7 Note Unique client_id that identifies your application. using the following naming scheme: <cluster>:<metadata.namespace>:<metadata.name> This value should be used in the client assertion when exchanging a token with Tokendings . TOKEN_X_PRIVATE_JWK \u00a7 Note Contains a JWK with the private RSA key for creating signed JWTs when authenticating to Tokendings with a signed client_assertion . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" } Client Authentication \u00a7 Your applications must authenticate itself with Tokendings when attempting to perform token exchanges. To do so, you must create a client assertion as per RFC7523 . In other words, you must create a JSON Web Token (JWT) that is signed by your application using the private key contained within TOKEN_X_PRIVATE_JWK . The assertion must contain the following claims: Claim Example Value Description sub dev-gcp:aura:app-a The subject of the token. Must be set to your application's own client_id . iss dev-gcp:aura:app-a The issuer of the token. Must be set to your application's own client_id . aud https://tokendings.dev-gcp.nais.io/token The audience of the token. Must be set to the token_endpoint of Tokendings . The value of this exists in the metadata found at the well-known endpoint . jti 83c580a6-b479-426d-876b-267aa9848e2f The JWT ID of the token. Used to uniquely identify a token. Set this to a UUID or similar. nbf 1597783152 nbf stands for not before . It identifies the time (seconds after Epoch) before which the JWT MUST NOT be accepted for processing. iat 1597783152 iat stands for issued at . It identifies the time (seconds after Epoch) in which the JWT was issued (or created). exp 1597783272 exp is the expiration time (seconds after Epoch) of the token. This must not be more than 120 seconds after nbf and iat . That is, the maximum lifetime of the token must be no greater than 120 seconds . Additionally, the headers of the assertion must contain the following parameters: Parameter Value Description kid 93ad09a5-70bc-4858-bd26-5ff4a0c5f73f The key identifier of the key used to sign the assertion. This identifier is available in the JWK found in TOKEN_X_PRIVATE_JWK . typ JWT Represents the type of this JWT. Set this to JWT . alg RS256 Represents the cryptographic algorithm used to secure the JWT. Set this to RS256 . An assertion should be unique and not be reused when authenticating with Tokendings in accordance with the security considerations in RFC7521 . That is, every request to Tokendings should contain a unique client assertion: Set the JWT ID ( jti ) claim to a unique value, such as an UUID. Set the JWT expiry ( exp ) claim so that the lifetime of the token is reasonably low. Tokendings allows a maximum lifetime of 120 seconds. A lifetime between 10-30 seconds should be fine for most situations. Example Client Assertion Values \u00a7 Header { \"kid\" : \"93ad09a5-70bc-4858-bd26-5ff4a0c5f73f\" , \"typ\" : \"JWT\" , \"alg\" : \"RS256\" } Payload { \"sub\" : \"prod-gcp:namespace-gcp:gcp-app\" , \"aud\" : \"https://tokendings.prod-gcp.nais.io/token\" , \"nbf\" : 1592508050 , \"iss\" : \"prod-gcp:namespace-gcp:gcp-app\" , \"exp\" : 1592508171 , \"iat\" : 1592508050 , \"jti\" : \"fd9717d3-6889-4b22-89b8-2626332abf14\" } Exchanging a token \u00a7 In order to acquire a token from Tokendings that is properly scoped to a given target application, you must exchange an existing subject token (i.e. a token that contains a subject, in this case a citizen end-user). Tokendings will then issue an access_token in JWT format, based on the parameters set in the token request. The token can then be used as a Bearer token in the Authorization header when calling your target API on behalf of the aforementioned subject. Prerequisites \u00a7 You have a subject token in the form of an access_token issued by one of the following providers: ID-porten Tokendings Loginservice (Remember that loginservice is a legacy system. TokenX accept their tokens to ease migration away from on-prem.) You have a client assertion that authenticates your application. Exchange Request \u00a7 The following denotes the required parameters needed to perform an exchange request. Parameter Value Comment grant_type urn:ietf:params:oauth:grant-type:token-exchange The identifier of the OAuth 2.0 grant to use, in this case the OAuth 2.0 Token Exchange grant. This grants allows applications to exchange one token for a new one containing much of the same information while still being correctly \"scoped\" in terms of OAuth. client_assertion_type urn:ietf:params:oauth:client-assertion-type:jwt-bearer Identifies the type of assertion the client/application will use to authenticate itself to Tokendings, in this case a JWT. client_assertion A serialized JWT identifying the calling app The client assertion ; a JWT signed by the calling client/application used to identify said client/application. subject_token_type urn:ietf:params:oauth:token-type:jwt Identifies the type of token that will be exchanged with a new one, in this case a JWT subject_token A serialized JWT, the token that should be exchanged The actual token (JWT) containing the signed-in user. Should be an access_token . audience The identifier of the app you wish to use the token for Identifies the intended audience for the resulting token, i.e. the target app you request a token for. This value shall be the client_id of the target app using the naming scheme <cluster>:<namespace>:<appname> e.g. prod-fss:namespace1:app1 The request should then sent to the token_endpoint of Tokendings, the value of which exists in the metadata found at the well-known endpoint . Example POST /token HTTP / 1.1 Host : tokendings.prod-gcp.nais.io Content-Type : application/x-www-form-urlencoded grant_type=urn:ietf:params:oauth:grant-type:token-exchange& client_assertion_type=urn:ietf:params:oauth:client-assertion-type:jwt-bearer& client_assertion=eY...............& subject_token_type=urn:ietf:params:oauth:token-type:jwt& subject_token=eY...............& audience=prod-fss:namespace1:app1 Info See frontend-dings for a complete example that illustrates: end-user authentication through ID-porten token exchange with the user's access_token calling a protected API using the exchanged token Exchange Response \u00a7 Tokendings will respond with a JSON object Example { \"access_token\" : \"eyJraWQiOi..............\" , \"issued_token_type\" : \"urn:ietf:params:oauth:token-type:access_token\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 299 } If performance is a concern, the token can be cached for reuse within the validity period indicated by the expires_in field. Token Validation \u00a7 If your app receives a token from another application, it is your responsibility to ensure this token is valid and intended for your application. Configure your app with the OAuth 2.0 Authorization Server Metadata from the well-known endpoint in order to retrieve issuer name and jwks_uri for public keys retrieval. Signature Verification \u00a7 The token should be signed with the RS256 algorithm (defined in JWT header). Tokens not matching this algorithm should be rejected. Verify that the signature is correct. The issuer's signing keys can be retrieved from the JWK Set (JWKS) at the jwks_uri . The kid attribute in the token header is thus a reference to a key contained within the JWK Set. The token signature should be verified against the public key in the matching JWK. Claims \u00a7 The following claims are by default provided in the issued token and should explicitly be validated: iss ( issuer ): The issuer of the token, the Tokendings issuer URI must match exactly . aud ( audience ): The intended audience for the token, must match your application's client_id . exp ( expiration time ): Expiration time, i.e. tokens received after this date must be rejected . nbf ( not before time ): The token cannot be used before this time, i.e. if the token is issued in the \"future\" (outside \"reasonable\" clock skew) it must be rejected . iat ( issued at time ): The time at which the token has been issued. Must be before exp . sub ( subject ): If applicable, used in user centric access control. This represents a unique identifier for the user. Other claims in the token are passed on verbatim from the original token issued by idp . The claim used for the national identity number ( f\u00f8dselsnummer ) varies from issuer to issuer. For instance: ID-porten use pid , loginservice use sub . The situation may be similar for other kinds of information with no standard claim name. TokenX does not try to unify this kind of information \u2014 claims are copied verbatim as described above. To extract such non-standard information from tokens, first use the idp claim to find the original token issuer. You can then map the original issuer's preferred claims to the claims in tokens issued by TokenX. Example Token (exchanged from ID-porten) \u00a7 The following example shows the claims of a token issued by Tokendings, where the exchanged subject token is issued by ID-porten : Example { \"at_hash\" : \"x6lQGCdbMX62p1VHeDsFBA\" , \"sub\" : \"HmjqfL7....\" , \"amr\" : [ \"BankID\" ], \"iss\" : \"https://tokendings.prod-gcp.nais.io\" , \"pid\" : \"12345678910\" , \"locale\" : \"nb\" , \"client_id\" : \"prod-gcp:team-a:app-a\" , \"sid\" : \"DASgLATSjYTp__ylaVbskHy66zWiplQrGDAYahvwk1k\" , \"aud\" : \"prod-fss:team-b:app-b\" , \"acr\" : \"Level4\" , \"nbf\" : 1597783152 , \"idp\" : \"https://oidc.difi.no/idporten-oidc-provider/\" , \"auth_time\" : 1611926877 , \"exp\" : 1597783452 , \"iat\" : 1597783152 , \"jti\" : \"97f580a6-b479-426d-876b-267aa9848e2e\" }","title":"TokenX"},{"location":"security/auth/tokenx/#tokenx","text":"Status: Opt-In Open Beta This feature is only available in team namespaces . It currently only supports self-service/citizen facing applications - however other use-cases have been identified.","title":"TokenX"},{"location":"security/auth/tokenx/#abstract","text":"Abstract","title":"Abstract"},{"location":"security/auth/tokenx/#what-is-tokenx","text":"TokenX is the short term for OAuth 2.0 Token Exchange implemented in the context of Kubernetes. It consists of mainly 3 components: Tokendings - an OAuth 2.0 Authorization Server implementing the OAuth 2.0 Token Exchange specification Jwker - a Kubernetes operator responsible for registering applications as OAuth 2.0 clients in Tokendings Naiserator - a Kubernetes operator that handles the lifecycle of applications on the NAIS platform In short, TokenX is a OAuth 2.0 compliant add-on that enables and allows your application to maintain the zero trust networking principles (together with components such as LinkerD ). It does this by allowing applications to exchange and acquire properly scoped security tokens in order to securely communicate with each other. Interested readers may find more technical details in the Tokendings documentation .","title":"What is TokenX?"},{"location":"security/auth/tokenx/#why-do-i-need-tokenx","text":"In a zero trust architecture, one cannot rely on traditional boundaries such as security zones and security gateways . Such security measures are no longer required for applications that leverage TokenX correctly as each application is self-contained within its own zone; requiring specific tokens in order to communicate with other applications. Using TokenX correctly throughout a call-chain also ensures that the identity of the original caller or subject (e.g. an end-user) is propagated while still maintaining proper scoping and security between each application.","title":"Why do I need TokenX?"},{"location":"security/auth/tokenx/#when-do-i-need-tokenx","text":"There are primarily two distinct cases where one must use TokenX: You have a user facing app using ID-porten that should perform calls to another app on behalf of a user. You have an app receiving tokens issued from Tokendings and need to call another app while still propagating the original user context. Overview of flow","title":"When do I need TokenX?"},{"location":"security/auth/tokenx/#configuration","text":"","title":"Configuration"},{"location":"security/auth/tokenx/#spec","text":"See the NAIS manifest .","title":"Spec"},{"location":"security/auth/tokenx/#getting-started","text":"nais.yaml spec : tokenx : enabled : true accessPolicy : inbound : rules : - application : app-2 - application : app-3 namespace : team-a - application : app-4 namespace : team-b cluster : prod-gcp","title":"Getting Started"},{"location":"security/auth/tokenx/#access-policies","text":"In order for other applications to acquire a token targeting your application, you must explicitly specify inbound access policies that authorizes these other applications. Thus, the access policies defines authorization on the application layer, and is enforced by Tokendings on token exchange operations. For example: spec : tokenx : enabled : true accessPolicy : inbound : rules : - application : app-1 - application : app-2 namespace : team-a - application : app-3 namespace : team-b cluster : prod-gcp The above configuration authorizes the following applications: application app-1 running in the same namespace and same cluster as your application application app-2 running in the namespace team-a in the same cluster application app-3 running in the namespace team-b in the cluster prod-gcp","title":"Access Policies"},{"location":"security/auth/tokenx/#usage","text":"Info See the NAV Security Guide for NAV-specific usage.","title":"Usage"},{"location":"security/auth/tokenx/#runtime-variables-credentials","text":"Enabling TokenX will expose the following runtime environment variables and files (under the directory /var/run/secrets/nais.io/jwker ) for your application:","title":"Runtime Variables &amp; Credentials"},{"location":"security/auth/tokenx/#token_x_well_known_url","text":"Note The well-known URL of the OAuth 2.0 Token Exchange authorization server, in this case Tokendings. This URL contains the server metadata as defined in RFC8414 that your application may use. For example: issuer token_endpoint jwks_uri See OAuth 2.0 Authorization Server Metadata for more information about the contents of the response from the well-known url.","title":"TOKEN_X_WELL_KNOWN_URL"},{"location":"security/auth/tokenx/#token_x_client_id","text":"Note Unique client_id that identifies your application. using the following naming scheme: <cluster>:<metadata.namespace>:<metadata.name> This value should be used in the client assertion when exchanging a token with Tokendings .","title":"TOKEN_X_CLIENT_ID"},{"location":"security/auth/tokenx/#token_x_private_jwk","text":"Note Contains a JWK with the private RSA key for creating signed JWTs when authenticating to Tokendings with a signed client_assertion . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" }","title":"TOKEN_X_PRIVATE_JWK"},{"location":"security/auth/tokenx/#client-authentication","text":"Your applications must authenticate itself with Tokendings when attempting to perform token exchanges. To do so, you must create a client assertion as per RFC7523 . In other words, you must create a JSON Web Token (JWT) that is signed by your application using the private key contained within TOKEN_X_PRIVATE_JWK . The assertion must contain the following claims: Claim Example Value Description sub dev-gcp:aura:app-a The subject of the token. Must be set to your application's own client_id . iss dev-gcp:aura:app-a The issuer of the token. Must be set to your application's own client_id . aud https://tokendings.dev-gcp.nais.io/token The audience of the token. Must be set to the token_endpoint of Tokendings . The value of this exists in the metadata found at the well-known endpoint . jti 83c580a6-b479-426d-876b-267aa9848e2f The JWT ID of the token. Used to uniquely identify a token. Set this to a UUID or similar. nbf 1597783152 nbf stands for not before . It identifies the time (seconds after Epoch) before which the JWT MUST NOT be accepted for processing. iat 1597783152 iat stands for issued at . It identifies the time (seconds after Epoch) in which the JWT was issued (or created). exp 1597783272 exp is the expiration time (seconds after Epoch) of the token. This must not be more than 120 seconds after nbf and iat . That is, the maximum lifetime of the token must be no greater than 120 seconds . Additionally, the headers of the assertion must contain the following parameters: Parameter Value Description kid 93ad09a5-70bc-4858-bd26-5ff4a0c5f73f The key identifier of the key used to sign the assertion. This identifier is available in the JWK found in TOKEN_X_PRIVATE_JWK . typ JWT Represents the type of this JWT. Set this to JWT . alg RS256 Represents the cryptographic algorithm used to secure the JWT. Set this to RS256 . An assertion should be unique and not be reused when authenticating with Tokendings in accordance with the security considerations in RFC7521 . That is, every request to Tokendings should contain a unique client assertion: Set the JWT ID ( jti ) claim to a unique value, such as an UUID. Set the JWT expiry ( exp ) claim so that the lifetime of the token is reasonably low. Tokendings allows a maximum lifetime of 120 seconds. A lifetime between 10-30 seconds should be fine for most situations.","title":"Client Authentication"},{"location":"security/auth/tokenx/#example-client-assertion-values","text":"Header { \"kid\" : \"93ad09a5-70bc-4858-bd26-5ff4a0c5f73f\" , \"typ\" : \"JWT\" , \"alg\" : \"RS256\" } Payload { \"sub\" : \"prod-gcp:namespace-gcp:gcp-app\" , \"aud\" : \"https://tokendings.prod-gcp.nais.io/token\" , \"nbf\" : 1592508050 , \"iss\" : \"prod-gcp:namespace-gcp:gcp-app\" , \"exp\" : 1592508171 , \"iat\" : 1592508050 , \"jti\" : \"fd9717d3-6889-4b22-89b8-2626332abf14\" }","title":"Example Client Assertion Values"},{"location":"security/auth/tokenx/#exchanging-a-token","text":"In order to acquire a token from Tokendings that is properly scoped to a given target application, you must exchange an existing subject token (i.e. a token that contains a subject, in this case a citizen end-user). Tokendings will then issue an access_token in JWT format, based on the parameters set in the token request. The token can then be used as a Bearer token in the Authorization header when calling your target API on behalf of the aforementioned subject.","title":"Exchanging a token"},{"location":"security/auth/tokenx/#prerequisites","text":"You have a subject token in the form of an access_token issued by one of the following providers: ID-porten Tokendings Loginservice (Remember that loginservice is a legacy system. TokenX accept their tokens to ease migration away from on-prem.) You have a client assertion that authenticates your application.","title":"Prerequisites"},{"location":"security/auth/tokenx/#exchange-request","text":"The following denotes the required parameters needed to perform an exchange request. Parameter Value Comment grant_type urn:ietf:params:oauth:grant-type:token-exchange The identifier of the OAuth 2.0 grant to use, in this case the OAuth 2.0 Token Exchange grant. This grants allows applications to exchange one token for a new one containing much of the same information while still being correctly \"scoped\" in terms of OAuth. client_assertion_type urn:ietf:params:oauth:client-assertion-type:jwt-bearer Identifies the type of assertion the client/application will use to authenticate itself to Tokendings, in this case a JWT. client_assertion A serialized JWT identifying the calling app The client assertion ; a JWT signed by the calling client/application used to identify said client/application. subject_token_type urn:ietf:params:oauth:token-type:jwt Identifies the type of token that will be exchanged with a new one, in this case a JWT subject_token A serialized JWT, the token that should be exchanged The actual token (JWT) containing the signed-in user. Should be an access_token . audience The identifier of the app you wish to use the token for Identifies the intended audience for the resulting token, i.e. the target app you request a token for. This value shall be the client_id of the target app using the naming scheme <cluster>:<namespace>:<appname> e.g. prod-fss:namespace1:app1 The request should then sent to the token_endpoint of Tokendings, the value of which exists in the metadata found at the well-known endpoint . Example POST /token HTTP / 1.1 Host : tokendings.prod-gcp.nais.io Content-Type : application/x-www-form-urlencoded grant_type=urn:ietf:params:oauth:grant-type:token-exchange& client_assertion_type=urn:ietf:params:oauth:client-assertion-type:jwt-bearer& client_assertion=eY...............& subject_token_type=urn:ietf:params:oauth:token-type:jwt& subject_token=eY...............& audience=prod-fss:namespace1:app1 Info See frontend-dings for a complete example that illustrates: end-user authentication through ID-porten token exchange with the user's access_token calling a protected API using the exchanged token","title":"Exchange Request"},{"location":"security/auth/tokenx/#exchange-response","text":"Tokendings will respond with a JSON object Example { \"access_token\" : \"eyJraWQiOi..............\" , \"issued_token_type\" : \"urn:ietf:params:oauth:token-type:access_token\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 299 } If performance is a concern, the token can be cached for reuse within the validity period indicated by the expires_in field.","title":"Exchange Response"},{"location":"security/auth/tokenx/#token-validation","text":"If your app receives a token from another application, it is your responsibility to ensure this token is valid and intended for your application. Configure your app with the OAuth 2.0 Authorization Server Metadata from the well-known endpoint in order to retrieve issuer name and jwks_uri for public keys retrieval.","title":"Token Validation"},{"location":"security/auth/tokenx/#signature-verification","text":"The token should be signed with the RS256 algorithm (defined in JWT header). Tokens not matching this algorithm should be rejected. Verify that the signature is correct. The issuer's signing keys can be retrieved from the JWK Set (JWKS) at the jwks_uri . The kid attribute in the token header is thus a reference to a key contained within the JWK Set. The token signature should be verified against the public key in the matching JWK.","title":"Signature Verification"},{"location":"security/auth/tokenx/#claims","text":"The following claims are by default provided in the issued token and should explicitly be validated: iss ( issuer ): The issuer of the token, the Tokendings issuer URI must match exactly . aud ( audience ): The intended audience for the token, must match your application's client_id . exp ( expiration time ): Expiration time, i.e. tokens received after this date must be rejected . nbf ( not before time ): The token cannot be used before this time, i.e. if the token is issued in the \"future\" (outside \"reasonable\" clock skew) it must be rejected . iat ( issued at time ): The time at which the token has been issued. Must be before exp . sub ( subject ): If applicable, used in user centric access control. This represents a unique identifier for the user. Other claims in the token are passed on verbatim from the original token issued by idp . The claim used for the national identity number ( f\u00f8dselsnummer ) varies from issuer to issuer. For instance: ID-porten use pid , loginservice use sub . The situation may be similar for other kinds of information with no standard claim name. TokenX does not try to unify this kind of information \u2014 claims are copied verbatim as described above. To extract such non-standard information from tokens, first use the idp claim to find the original token issuer. You can then map the original issuer's preferred claims to the claims in tokens issued by TokenX.","title":"Claims"},{"location":"security/auth/tokenx/#example-token-exchanged-from-id-porten","text":"The following example shows the claims of a token issued by Tokendings, where the exchanged subject token is issued by ID-porten : Example { \"at_hash\" : \"x6lQGCdbMX62p1VHeDsFBA\" , \"sub\" : \"HmjqfL7....\" , \"amr\" : [ \"BankID\" ], \"iss\" : \"https://tokendings.prod-gcp.nais.io\" , \"pid\" : \"12345678910\" , \"locale\" : \"nb\" , \"client_id\" : \"prod-gcp:team-a:app-a\" , \"sid\" : \"DASgLATSjYTp__ylaVbskHy66zWiplQrGDAYahvwk1k\" , \"aud\" : \"prod-fss:team-b:app-b\" , \"acr\" : \"Level4\" , \"nbf\" : 1597783152 , \"idp\" : \"https://oidc.difi.no/idporten-oidc-provider/\" , \"auth_time\" : 1611926877 , \"exp\" : 1597783452 , \"iat\" : 1597783152 , \"jti\" : \"97f580a6-b479-426d-876b-267aa9848e2e\" }","title":"Example Token (exchanged from ID-porten)"},{"location":"security/auth/azure-ad/","text":"Azure AD \u00a7 Warning This feature is only available in team namespaces Abstract The NAIS platform provides support for simple, declarative provisioning of an Azure AD client configured with sensible defaults. An Azure AD client allows your application to leverage Azure AD for authentication and authorization. The most common cases include: User (employees only) sign-in with SSO, using OpenID Connect with Authorization Code flow Request chains involving an end-user whose identity and permissions should be propagated through each service/web API, using the OAuth 2.0 On-Behalf-Of flow Daemon / server-side applications for server-to-server interactions without a user, using the OAuth 2.0 client credentials flow The feature described in configuration only provisions and configures an Azure AD client. Your application is responsible for using the client to implement the desired use case. If you need functionality to sign-in end-users with said client, we also provide a separate sidecar proxy that handles this. Info See the NAV Security Guide for NAV-specific usage of this client.","title":"Intro"},{"location":"security/auth/azure-ad/#azure-ad","text":"Warning This feature is only available in team namespaces Abstract The NAIS platform provides support for simple, declarative provisioning of an Azure AD client configured with sensible defaults. An Azure AD client allows your application to leverage Azure AD for authentication and authorization. The most common cases include: User (employees only) sign-in with SSO, using OpenID Connect with Authorization Code flow Request chains involving an end-user whose identity and permissions should be propagated through each service/web API, using the OAuth 2.0 On-Behalf-Of flow Daemon / server-side applications for server-to-server interactions without a user, using the OAuth 2.0 client credentials flow The feature described in configuration only provisions and configures an Azure AD client. Your application is responsible for using the client to implement the desired use case. If you need functionality to sign-in end-users with said client, we also provide a separate sidecar proxy that handles this. Info See the NAV Security Guide for NAV-specific usage of this client.","title":"Azure AD"},{"location":"security/auth/azure-ad/access-policy/","text":"Access Policy \u00a7 Pre-authorization \u00a7 For proper scoping of tokens when performing calls between clients, one must either: Request a token for a specific client ID using the client using the OAuth 2.0 client credentials flow (service-to-service calls). Exchange a token containing an end-user context using the OAuth 2.0 On-Behalf-Of flow (service-to-service calls on behalf of an end-user). Azure AD will enforce authorization for both flows. In other words, you must pre-authorize any consumer clients for your application. Clients that should receive and validate access tokens from other clients should pre-authorize said clients. This is declared by specifying spec.accessPolicy.inbound.rules[] : spec : accessPolicy : inbound : rules : - application : app-a - application : app-b namespace : other-namespace - application : app-c namespace : other-namespace cluster : other-cluster Danger These rules are eventually consistent , which means it might take a few minutes to propagate throughout Azure AD. Any client referred to must already exist in Azure AD in order to be assigned the access policy permissions. If you're pre-authorizing a client provisioned through aad-iac , ensure that you've read the legacy section. Clients defined in the Spec that do not exist in Azure AD at deploy time will be skipped. If a non-existing client is created at a later time, we'll attempt to automatically force a resynchronization . The above configuration will pre-authorize the Azure AD clients belonging to: application app-a running in the same namespace and same cluster as your application application app-b running in the namespace other-namespace in the same cluster application app-c running in the namespace other-namespace in the cluster other-cluster The default permissions will grant consumer clients the role access_as_application , which will only appear in tokens acquired with the client credentials flow (i.e. service-to-service requests). If you require more fine-grained access control, see Fine-Grained Access Control . Fine-Grained Access Control \u00a7 You may define custom permissions for your client in Azure AD. These can be granted to consumer clients individually as an extension of the access policy definitions described above. When granted to a consumer, the permissions will appear in their respective claims in tokens targeted to your application. Your application can then use these claims to implement custom authorization logic. Warning Custom permissions only apply in the context of your own application as an API provider. They are not global permissions. These permissions only appear in tokens when all the following conditions are met: The token is acquired by a consumer of your application. The consumer has been granted a custom permission in your access policy definition. The target audience is your application. Custom Scopes \u00a7 A scope only applies to tokens acquired using the OAuth 2.0 On-Behalf-Of flow (service-to-service calls on behalf of an end-user). Example configuration spec : accessPolicy : inbound : rules : - application : app-a namespace : other-namespace cluster : other-cluster permissions : scopes : - \"custom-scope\" The above configuration grants the application app-a the scope custom-scope . Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"custom-scope defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" } Info Any custom scopes granted will appear as a space separated string in the scp claim. Custom Roles \u00a7 A role only applies to tokens acquired using the using the OAuth 2.0 client credentials flow (service-to-service calls). Example configuration spec : accessPolicy : inbound : rules : - application : app-a namespace : other-namespace cluster : other-cluster permissions : roles : - \"custom-role\" The above configuration grants the application app-a the role custom-role . Example decoded client credentials token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957347 , \"nbf\" : 1624957347 , \"exp\" : 1624961247 , \"aio\" : \"E2Z...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"oid\" : \"933...\" , \"rh\" : \"0.AS...\" , \"roles\" : [ \"access_as_application\" , \"custom-role\" ], \"sub\" : \"933...\" , \"tid\" : \"623...\" , \"uti\" : \"kbG...\" , \"ver\" : \"2.0\" } Info Any custom roles granted will appear in the roles claim, which is an array of strings . Users \u00a7 By default, all users within the tenant are allowed to log in to your application. This behaviour is toggleable: spec : azure : application : enabled : true allowAllUsers : true Groups \u00a7 For some use cases, it is desirable to restrict access to smaller groups of users. This can be done by disallowing all users and explicitly declaring which groups are allowed to access the application: spec : azure : application : enabled : true allowAllUsers : false claims : groups : - id : \"<object ID of group in Azure AD>\" Warning Ensure that the object ID for the group is valid, and that the group actually exists in Azure AD. Non-existing groups will be skipped. Azure AD will now only allow sign-ins and token exchanges with the on-behalf-of flow if a given user is a direct member of at least one of the groups declared. This also controls the groups claim for a user token.","title":"Access Policy"},{"location":"security/auth/azure-ad/access-policy/#access-policy","text":"","title":"Access Policy"},{"location":"security/auth/azure-ad/access-policy/#pre-authorization","text":"For proper scoping of tokens when performing calls between clients, one must either: Request a token for a specific client ID using the client using the OAuth 2.0 client credentials flow (service-to-service calls). Exchange a token containing an end-user context using the OAuth 2.0 On-Behalf-Of flow (service-to-service calls on behalf of an end-user). Azure AD will enforce authorization for both flows. In other words, you must pre-authorize any consumer clients for your application. Clients that should receive and validate access tokens from other clients should pre-authorize said clients. This is declared by specifying spec.accessPolicy.inbound.rules[] : spec : accessPolicy : inbound : rules : - application : app-a - application : app-b namespace : other-namespace - application : app-c namespace : other-namespace cluster : other-cluster Danger These rules are eventually consistent , which means it might take a few minutes to propagate throughout Azure AD. Any client referred to must already exist in Azure AD in order to be assigned the access policy permissions. If you're pre-authorizing a client provisioned through aad-iac , ensure that you've read the legacy section. Clients defined in the Spec that do not exist in Azure AD at deploy time will be skipped. If a non-existing client is created at a later time, we'll attempt to automatically force a resynchronization . The above configuration will pre-authorize the Azure AD clients belonging to: application app-a running in the same namespace and same cluster as your application application app-b running in the namespace other-namespace in the same cluster application app-c running in the namespace other-namespace in the cluster other-cluster The default permissions will grant consumer clients the role access_as_application , which will only appear in tokens acquired with the client credentials flow (i.e. service-to-service requests). If you require more fine-grained access control, see Fine-Grained Access Control .","title":"Pre-authorization"},{"location":"security/auth/azure-ad/access-policy/#fine-grained-access-control","text":"You may define custom permissions for your client in Azure AD. These can be granted to consumer clients individually as an extension of the access policy definitions described above. When granted to a consumer, the permissions will appear in their respective claims in tokens targeted to your application. Your application can then use these claims to implement custom authorization logic. Warning Custom permissions only apply in the context of your own application as an API provider. They are not global permissions. These permissions only appear in tokens when all the following conditions are met: The token is acquired by a consumer of your application. The consumer has been granted a custom permission in your access policy definition. The target audience is your application.","title":"Fine-Grained Access Control"},{"location":"security/auth/azure-ad/access-policy/#custom-scopes","text":"A scope only applies to tokens acquired using the OAuth 2.0 On-Behalf-Of flow (service-to-service calls on behalf of an end-user). Example configuration spec : accessPolicy : inbound : rules : - application : app-a namespace : other-namespace cluster : other-cluster permissions : scopes : - \"custom-scope\" The above configuration grants the application app-a the scope custom-scope . Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"custom-scope defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" } Info Any custom scopes granted will appear as a space separated string in the scp claim.","title":"Custom Scopes"},{"location":"security/auth/azure-ad/access-policy/#custom-roles","text":"A role only applies to tokens acquired using the using the OAuth 2.0 client credentials flow (service-to-service calls). Example configuration spec : accessPolicy : inbound : rules : - application : app-a namespace : other-namespace cluster : other-cluster permissions : roles : - \"custom-role\" The above configuration grants the application app-a the role custom-role . Example decoded client credentials token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957347 , \"nbf\" : 1624957347 , \"exp\" : 1624961247 , \"aio\" : \"E2Z...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"oid\" : \"933...\" , \"rh\" : \"0.AS...\" , \"roles\" : [ \"access_as_application\" , \"custom-role\" ], \"sub\" : \"933...\" , \"tid\" : \"623...\" , \"uti\" : \"kbG...\" , \"ver\" : \"2.0\" } Info Any custom roles granted will appear in the roles claim, which is an array of strings .","title":"Custom Roles"},{"location":"security/auth/azure-ad/access-policy/#users","text":"By default, all users within the tenant are allowed to log in to your application. This behaviour is toggleable: spec : azure : application : enabled : true allowAllUsers : true","title":"Users"},{"location":"security/auth/azure-ad/access-policy/#groups","text":"For some use cases, it is desirable to restrict access to smaller groups of users. This can be done by disallowing all users and explicitly declaring which groups are allowed to access the application: spec : azure : application : enabled : true allowAllUsers : false claims : groups : - id : \"<object ID of group in Azure AD>\" Warning Ensure that the object ID for the group is valid, and that the group actually exists in Azure AD. Non-existing groups will be skipped. Azure AD will now only allow sign-ins and token exchanges with the on-behalf-of flow if a given user is a direct member of at least one of the groups declared. This also controls the groups claim for a user token.","title":"Groups"},{"location":"security/auth/azure-ad/concepts/","text":"Concepts \u00a7 The following describes a few core concepts in Azure AD referred to throughout this documentation. Tenants \u00a7 A tenant represents an organization in Azure AD. Each tenant will have their own set of applications, users and groups. In order to log in to a tenant, you must use an account specific to that tenant. NAV has two tenants in Azure AD: nav.no - available in all clusters, default tenant for production clusters trygdeetaten.no - only available in dev-* -clusters, default tenant for development clusters Warning If your use case requires you to use nav.no in the dev-* -clusters, then you must explicitly configure this . Note that you cannot interact with clients or applications across different tenants. The same application in different clusters will result in unique Azure AD clients, with each having their own client IDs and access policies. For instance, the following applications in the same nav.no tenant will result in separate, unique clients in Azure AD: app-a in dev-gcp app-a in prod-gcp Naming format \u00a7 An Azure AD client has an associated name within a tenant. NAIS uses this name for lookups and identification. All clients provisioned through NAIS will be registered in Azure AD using the following naming scheme: <cluster>:<namespace>:<app-name> Example dev-gcp:aura:nais-testapp Scopes \u00a7 A scope is a parameter that is set during authorization flows when requesting a token from Azure AD. It is used to indicate the intended audience (the expected target resource) for the requested token, which is found in the aud claim in the JWT returned from Azure AD. When consuming a downstream API that expects an Azure AD token, you must therefore set the correct scope to fetch a token that your API provider accepts. The scope has the following format: api://<cluster>.<namespace>.<app-name> For example: api://dev-gcp.aura.nais-testapp/.default Default scope \u00a7 The /.default scope is a static scope which indicates to Azure AD that your application is requesting all available scopes that have been granted to your application. For example, if your application has access to api://dev-gcp.aura.nais-testapp/defaultaccess and api://dev-gcp.aura.nais-testapp/read , requesting api://dev-gcp.aura.nais-testapp/.default will return a token that contains both of these scopes. If you want granularity, you may explicitly request the individual scopes instead as needed. Client ID \u00a7 An Azure AD client has its own ID that uniquely identifies the client within a tenant, and is used in authentication requests to Azure AD. Your application's Azure AD client ID is available at multiple locations: The environment variable AZURE_APP_CLIENT_ID , available inside your application at runtime In the Kubernetes resource - kubectl get azureapp <app-name> The Azure Portal . You may have to click on All applications if it does not show up in Owned applications . Search using the naming scheme mentioned earlier: <cluster>:<namespace>:<app> .","title":"Concepts"},{"location":"security/auth/azure-ad/concepts/#concepts","text":"The following describes a few core concepts in Azure AD referred to throughout this documentation.","title":"Concepts"},{"location":"security/auth/azure-ad/concepts/#tenants","text":"A tenant represents an organization in Azure AD. Each tenant will have their own set of applications, users and groups. In order to log in to a tenant, you must use an account specific to that tenant. NAV has two tenants in Azure AD: nav.no - available in all clusters, default tenant for production clusters trygdeetaten.no - only available in dev-* -clusters, default tenant for development clusters Warning If your use case requires you to use nav.no in the dev-* -clusters, then you must explicitly configure this . Note that you cannot interact with clients or applications across different tenants. The same application in different clusters will result in unique Azure AD clients, with each having their own client IDs and access policies. For instance, the following applications in the same nav.no tenant will result in separate, unique clients in Azure AD: app-a in dev-gcp app-a in prod-gcp","title":"Tenants"},{"location":"security/auth/azure-ad/concepts/#naming-format","text":"An Azure AD client has an associated name within a tenant. NAIS uses this name for lookups and identification. All clients provisioned through NAIS will be registered in Azure AD using the following naming scheme: <cluster>:<namespace>:<app-name> Example dev-gcp:aura:nais-testapp","title":"Naming format"},{"location":"security/auth/azure-ad/concepts/#scopes","text":"A scope is a parameter that is set during authorization flows when requesting a token from Azure AD. It is used to indicate the intended audience (the expected target resource) for the requested token, which is found in the aud claim in the JWT returned from Azure AD. When consuming a downstream API that expects an Azure AD token, you must therefore set the correct scope to fetch a token that your API provider accepts. The scope has the following format: api://<cluster>.<namespace>.<app-name> For example: api://dev-gcp.aura.nais-testapp/.default","title":"Scopes"},{"location":"security/auth/azure-ad/concepts/#default-scope","text":"The /.default scope is a static scope which indicates to Azure AD that your application is requesting all available scopes that have been granted to your application. For example, if your application has access to api://dev-gcp.aura.nais-testapp/defaultaccess and api://dev-gcp.aura.nais-testapp/read , requesting api://dev-gcp.aura.nais-testapp/.default will return a token that contains both of these scopes. If you want granularity, you may explicitly request the individual scopes instead as needed.","title":"Default scope"},{"location":"security/auth/azure-ad/concepts/#client-id","text":"An Azure AD client has its own ID that uniquely identifies the client within a tenant, and is used in authentication requests to Azure AD. Your application's Azure AD client ID is available at multiple locations: The environment variable AZURE_APP_CLIENT_ID , available inside your application at runtime In the Kubernetes resource - kubectl get azureapp <app-name> The Azure Portal . You may have to click on All applications if it does not show up in Owned applications . Search using the naming scheme mentioned earlier: <cluster>:<namespace>:<app> .","title":"Client ID"},{"location":"security/auth/azure-ad/configuration/","text":"Configuration \u00a7 Spec \u00a7 See the complete specification in the NAIS manifest . Getting started \u00a7 nais.yaml spec : azure : application : enabled : true # optional, enum of {trygdeetaten.no, nav.no} # defaults to trygdeetaten.no in dev-* clusters, nav.no in production tenant : nav.no # optional, generated defaults shown replyURLs : - \"https://my-app.dev.nav.no/oauth2/callback\" # optional claims : extra : - \"NAVident\" - \"azp_name\" groups : - id : \"<object ID of Azure AD group>\" # optional; if omitted: # - false if claims.groups is defined # - true otherwise allowAllUsers : true # optional, defaults shown singlePageApplication : false # optional, only relevant if your application should receive requests from consumers accessPolicy : inbound : rules : - application : app-a namespace : othernamespace cluster : dev-fss permissions : roles : - \"custom-role\" scopes : - \"custom-scope\" - application : app-b # required for on-premises only webproxy : true Accessing external hosts \u00a7 Azure AD is a third-party service outside of our clusters, which is not reachable by default like most third-party services. Google Cloud Platform (GCP) \u00a7 The following outbound external hosts are automatically added when enabling this feature: login.microsoftonline.com graph.microsoft.com You do not need to specify these explicitly. On-premises \u00a7 You must enable and use webproxy for external communication. Reply URLs \u00a7 A redirect URI, or reply URL, is the location that the authorization server will send the user to once the app has been successfully authorized, and granted an authorization code or access token. The code or token is contained in the redirect URI or reply token so it's important that you register the correct location as part of the app registration process. -- Microsoft's documentation on reply URLs Defaults \u00a7 If you have not specified any reply URLs, we will automatically generate a reply URL for each ingress specified using this formula: spec.ingresses[n] + \"/oauth2/callback\" Example In other words, this: spec : ingresses : - \"https://my.application.dev.nav.no\" - \"https://my.application.dev.nav.no/subpath\" azure : application : enabled : true will generate a spec equivalent to this: spec : ingresses : - \"https://my.application.dev.nav.no\" - \"https://my.application.dev.nav.no/subpath\" azure : application : enabled : true replyURLs : - \"https://my.application.dev.nav.no/oauth2/callback\" - \"https://my.application.dev.nav.no/subpath/oauth2/callback\" Overriding explicitly \u00a7 You may set reply URLs manually by specifying spec.azure.application.replyURLs[] : Example spec : azure : application : enabled : true replyURLs : - \"https://my.application.dev.nav.no/oauth2/callback\" - \"https://my.application.dev.nav.no/subpath/oauth2/callback\" Doing so will replace all of the default auto-generated reply URLs . Danger If you do override the reply URLs, make sure that you specify all the URLs that should be registered for the Azure AD client. Ensure that these URLs conform to the restrictions and limitations of reply URLs as specified by Microsoft. Tenants \u00a7 To explicitly target a specific tenant , add a spec.azure.application.tenant to your nais.yaml : spec : azure : application : enabled : true # enum of {trygdeetaten.no, nav.no} tenant : trygdeetaten.no Single-Page Application \u00a7 Azure AD supports the OAuth 2.0 Auth Code Flow with PKCE for logins from client-side/browser single-page-applications. However, the support for this must be explicitly enabled to avoid issues with CORS: spec : azure : application : enabled : true singlePageApplication : true Claims \u00a7 Groups \u00a7 The groups claim in user tokens is by default omitted due to potential issues with the token's size when used in cookies. Sometimes however, it is desirable to check for group membership for a given user's token. Start by defining all Azure AD group IDs that should appear in user tokens: spec : azure : application : enabled : true allowAllUsers : true claims : groups : - id : \"<object ID of group in Azure AD>\" Warning Ensure that you include the allowAllUsers field and set the value to your desired behaviour. If undefined, the default behaviour will only allow access to users in the defined groups . Warning Ensure that the object ID for the group is valid, and that the group actually exists in Azure AD. Non-existing groups will be skipped. Now all user tokens acquired for your application will include the groups claim. The claim will only contain groups that are both explicitly assigned to the application and which the user is a direct member of . Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" } If you wish to also restrict sign-ins and token exchanges with the on-behalf-of flow to users in these groups, see the groups section in access policy . Extra \u00a7 The Azure AD tenants in NAV have defined some non-standard claims that are available for inclusion in tokens. These are opt-in by default, and you may selectively include any (or all) of these as needed. NAVident \u00a7 The value of the NAVident claim maps to an internal identifier for the employees in NAV. This claim thus only applies in flows where a user is involved i.e., either the sign-in or on-behalf-of flows. spec : azure : application : enabled : true claims : extra : - NAVident Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" , \"NAVident\" : \"Z123456\" } azp_name \u00a7 The azp_name claim will return the name of the consumer application that requested the token. This claim applies to both the client credentials flow and the on-behalf-of flow. spec : azure : application : enabled : true claims : extra : - azp_name Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" , \"azp_name\" : \"dev-gcp:some-team:some-consumer\" } Access Policy \u00a7 See access policy .","title":"Configuration"},{"location":"security/auth/azure-ad/configuration/#configuration","text":"","title":"Configuration"},{"location":"security/auth/azure-ad/configuration/#spec","text":"See the complete specification in the NAIS manifest .","title":"Spec"},{"location":"security/auth/azure-ad/configuration/#getting-started","text":"nais.yaml spec : azure : application : enabled : true # optional, enum of {trygdeetaten.no, nav.no} # defaults to trygdeetaten.no in dev-* clusters, nav.no in production tenant : nav.no # optional, generated defaults shown replyURLs : - \"https://my-app.dev.nav.no/oauth2/callback\" # optional claims : extra : - \"NAVident\" - \"azp_name\" groups : - id : \"<object ID of Azure AD group>\" # optional; if omitted: # - false if claims.groups is defined # - true otherwise allowAllUsers : true # optional, defaults shown singlePageApplication : false # optional, only relevant if your application should receive requests from consumers accessPolicy : inbound : rules : - application : app-a namespace : othernamespace cluster : dev-fss permissions : roles : - \"custom-role\" scopes : - \"custom-scope\" - application : app-b # required for on-premises only webproxy : true","title":"Getting started"},{"location":"security/auth/azure-ad/configuration/#accessing-external-hosts","text":"Azure AD is a third-party service outside of our clusters, which is not reachable by default like most third-party services.","title":"Accessing external hosts"},{"location":"security/auth/azure-ad/configuration/#google-cloud-platform-gcp","text":"The following outbound external hosts are automatically added when enabling this feature: login.microsoftonline.com graph.microsoft.com You do not need to specify these explicitly.","title":"Google Cloud Platform (GCP)"},{"location":"security/auth/azure-ad/configuration/#on-premises","text":"You must enable and use webproxy for external communication.","title":"On-premises"},{"location":"security/auth/azure-ad/configuration/#reply-urls","text":"A redirect URI, or reply URL, is the location that the authorization server will send the user to once the app has been successfully authorized, and granted an authorization code or access token. The code or token is contained in the redirect URI or reply token so it's important that you register the correct location as part of the app registration process. -- Microsoft's documentation on reply URLs","title":"Reply URLs"},{"location":"security/auth/azure-ad/configuration/#defaults","text":"If you have not specified any reply URLs, we will automatically generate a reply URL for each ingress specified using this formula: spec.ingresses[n] + \"/oauth2/callback\" Example In other words, this: spec : ingresses : - \"https://my.application.dev.nav.no\" - \"https://my.application.dev.nav.no/subpath\" azure : application : enabled : true will generate a spec equivalent to this: spec : ingresses : - \"https://my.application.dev.nav.no\" - \"https://my.application.dev.nav.no/subpath\" azure : application : enabled : true replyURLs : - \"https://my.application.dev.nav.no/oauth2/callback\" - \"https://my.application.dev.nav.no/subpath/oauth2/callback\"","title":"Defaults"},{"location":"security/auth/azure-ad/configuration/#overriding-explicitly","text":"You may set reply URLs manually by specifying spec.azure.application.replyURLs[] : Example spec : azure : application : enabled : true replyURLs : - \"https://my.application.dev.nav.no/oauth2/callback\" - \"https://my.application.dev.nav.no/subpath/oauth2/callback\" Doing so will replace all of the default auto-generated reply URLs . Danger If you do override the reply URLs, make sure that you specify all the URLs that should be registered for the Azure AD client. Ensure that these URLs conform to the restrictions and limitations of reply URLs as specified by Microsoft.","title":"Overriding explicitly"},{"location":"security/auth/azure-ad/configuration/#tenants","text":"To explicitly target a specific tenant , add a spec.azure.application.tenant to your nais.yaml : spec : azure : application : enabled : true # enum of {trygdeetaten.no, nav.no} tenant : trygdeetaten.no","title":"Tenants"},{"location":"security/auth/azure-ad/configuration/#single-page-application","text":"Azure AD supports the OAuth 2.0 Auth Code Flow with PKCE for logins from client-side/browser single-page-applications. However, the support for this must be explicitly enabled to avoid issues with CORS: spec : azure : application : enabled : true singlePageApplication : true","title":"Single-Page Application"},{"location":"security/auth/azure-ad/configuration/#claims","text":"","title":"Claims"},{"location":"security/auth/azure-ad/configuration/#groups","text":"The groups claim in user tokens is by default omitted due to potential issues with the token's size when used in cookies. Sometimes however, it is desirable to check for group membership for a given user's token. Start by defining all Azure AD group IDs that should appear in user tokens: spec : azure : application : enabled : true allowAllUsers : true claims : groups : - id : \"<object ID of group in Azure AD>\" Warning Ensure that you include the allowAllUsers field and set the value to your desired behaviour. If undefined, the default behaviour will only allow access to users in the defined groups . Warning Ensure that the object ID for the group is valid, and that the group actually exists in Azure AD. Non-existing groups will be skipped. Now all user tokens acquired for your application will include the groups claim. The claim will only contain groups that are both explicitly assigned to the application and which the user is a direct member of . Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" } If you wish to also restrict sign-ins and token exchanges with the on-behalf-of flow to users in these groups, see the groups section in access policy .","title":"Groups"},{"location":"security/auth/azure-ad/configuration/#extra","text":"The Azure AD tenants in NAV have defined some non-standard claims that are available for inclusion in tokens. These are opt-in by default, and you may selectively include any (or all) of these as needed.","title":"Extra"},{"location":"security/auth/azure-ad/configuration/#navident","text":"The value of the NAVident claim maps to an internal identifier for the employees in NAV. This claim thus only applies in flows where a user is involved i.e., either the sign-in or on-behalf-of flows. spec : azure : application : enabled : true claims : extra : - NAVident Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" , \"NAVident\" : \"Z123456\" }","title":"NAVident"},{"location":"security/auth/azure-ad/configuration/#azp_name","text":"The azp_name claim will return the name of the consumer application that requested the token. This claim applies to both the client credentials flow and the on-behalf-of flow. spec : azure : application : enabled : true claims : extra : - azp_name Example decoded on-behalf-of token { \"aud\" : \"8a5...\" , \"iss\" : \"https://login.microsoftonline.com/.../v2.0\" , \"iat\" : 1624957183 , \"nbf\" : 1624957183 , \"exp\" : 1624961081 , \"aio\" : \"AXQ...\" , \"azp\" : \"e37...\" , \"azpacr\" : \"1\" , \"groups\" : [ \"2d7...\" ], \"name\" : \"Navnesen, Navn\" , \"oid\" : \"15c...\" , \"preferred_username\" : \"Navn.Navnesen@nav.no\" , \"rh\" : \"0.AS...\" , \"scp\" : \"defaultaccess\" , \"sub\" : \"6OC...\" , \"tid\" : \"623...\" , \"uti\" : \"i03...\" , \"ver\" : \"2.0\" , \"azp_name\" : \"dev-gcp:some-team:some-consumer\" }","title":"azp_name"},{"location":"security/auth/azure-ad/configuration/#access-policy","text":"See access policy .","title":"Access Policy"},{"location":"security/auth/azure-ad/faq-troubleshooting/","text":"FAQ / Troubleshooting \u00a7 First steps \u00a7 If something isn't quite right, these kubectl commands may be of help in diagnosing and reporting errors. To get a summary of the status of your Azure AD client: kubectl get azureapp <app> -owide For additional details: kubectl describe azureapp <app> Unassigned Pre-Authorized Apps \u00a7 Example You might see the following status message when running kubectl describe azureapp <app> : Status : ... Pre Authorized Apps : ... Unassigned : Access Policy Rule : Application : <other-application> Cluster : <cluster> Namespace : <namespace> Reason : WARNING : Application '<cluster>:<namespace>:<other-application>' was not found in the Azure AD tenant (<tenant>) and will _NOT_ be pre-authorized. Unassigned Count : 1 Solution / Answer Ensure that the application you're attempting to pre-authorize exists in Azure AD: Run kubectl get azureapp <other-application> and check that the Synchronized field is not empty. If you added the application to your access policy before it existed in Azure AD, try to resynchronize your own application: kubectl annotate azureapp <my-application> azure.nais.io/resync=true If all else fails, ask an adult in the #nais channel on Slack. \"Application Alice is not assigned to a role for the application Bob \" \u00a7 Example An application may receive the following 400 Bad Request response error when requesting a token from Azure AD: { \"error\" : \"invalid_grant\" , \"error_description\" : \"AADSTS501051: Application '<client ID>'(<cluster>:<namespace>:<alice>) is not assigned to a role for the application 'api://<cluster>.<namespace>.<bob>'(<cluster>:<namespace>:<bob>)\" , ... } Solution / Answer Ensure that Bob's access policy includes Alice. Run kubectl get azureapp bob to check the current count of assigned and unassigned applications for Bob. Run kubectl get azureapp bob -o json | jq '.status.preAuthorizedApps' to check the detailed statuses for all of Bob's desired pre-authorized applications. If Bob added Alice to its access policy before Alice existed in Azure AD, try to resynchronize Bob: kubectl annotate azureapp bob azure.nais.io/resync=true If all else fails, ask an adult in the #nais channel on Slack.","title":"FAQ / Troubleshooting"},{"location":"security/auth/azure-ad/faq-troubleshooting/#faq-troubleshooting","text":"","title":"FAQ / Troubleshooting"},{"location":"security/auth/azure-ad/faq-troubleshooting/#first-steps","text":"If something isn't quite right, these kubectl commands may be of help in diagnosing and reporting errors. To get a summary of the status of your Azure AD client: kubectl get azureapp <app> -owide For additional details: kubectl describe azureapp <app>","title":"First steps"},{"location":"security/auth/azure-ad/faq-troubleshooting/#unassigned-pre-authorized-apps","text":"Example You might see the following status message when running kubectl describe azureapp <app> : Status : ... Pre Authorized Apps : ... Unassigned : Access Policy Rule : Application : <other-application> Cluster : <cluster> Namespace : <namespace> Reason : WARNING : Application '<cluster>:<namespace>:<other-application>' was not found in the Azure AD tenant (<tenant>) and will _NOT_ be pre-authorized. Unassigned Count : 1 Solution / Answer Ensure that the application you're attempting to pre-authorize exists in Azure AD: Run kubectl get azureapp <other-application> and check that the Synchronized field is not empty. If you added the application to your access policy before it existed in Azure AD, try to resynchronize your own application: kubectl annotate azureapp <my-application> azure.nais.io/resync=true If all else fails, ask an adult in the #nais channel on Slack.","title":"Unassigned Pre-Authorized Apps"},{"location":"security/auth/azure-ad/faq-troubleshooting/#application-alice-is-not-assigned-to-a-role-for-the-application-bob","text":"Example An application may receive the following 400 Bad Request response error when requesting a token from Azure AD: { \"error\" : \"invalid_grant\" , \"error_description\" : \"AADSTS501051: Application '<client ID>'(<cluster>:<namespace>:<alice>) is not assigned to a role for the application 'api://<cluster>.<namespace>.<bob>'(<cluster>:<namespace>:<bob>)\" , ... } Solution / Answer Ensure that Bob's access policy includes Alice. Run kubectl get azureapp bob to check the current count of assigned and unassigned applications for Bob. Run kubectl get azureapp bob -o json | jq '.status.preAuthorizedApps' to check the detailed statuses for all of Bob's desired pre-authorized applications. If Bob added Alice to its access policy before Alice existed in Azure AD, try to resynchronize Bob: kubectl annotate azureapp bob azure.nais.io/resync=true If all else fails, ask an adult in the #nais channel on Slack.","title":"\"Application Alice is not assigned to a role for the application Bob\""},{"location":"security/auth/azure-ad/legacy/","text":"Legacy \u00a7 This section only applies if you have an existing Azure AD client registered in the IaC repository . Why migrate? \u00a7 Declarative provisioning, straight from your application's nais.yaml No longer dependent on manual user approvals in multiple IaC repositories No longer dependent on Vault Credentials are rotated regularly, completely transparent to the application. This ensures that credentials are fresh and lessens the impact in the case of exposure. The exact same feature is present in the GCP clusters, which simplifies migration . Pre-authorization \u00a7 Communication between legacy clients provisioned through aad-iac and clients provisioned through NAIS requires some additional configuration. Scenario 1 Allowing a NAIS client to access an aad-iac client \u00a7 Prerequisites: You have a legacy client registered in the aad-iac repository. You would like to pre-authorize client provisioned through NAIS. Steps: Refer to the NAIS client in aad-iac using its fully qualified name (see naming format ): <cluster>:<namespace>:<app-name> Example: See this example in aad-iac . Scenario 2 Allowing an aad-iac client to access a NAIS client \u00a7 Prerequisites: You have a client provisioned through NAIS. You would like to pre-authorize a legacy client registered in the aad-iac repository. Steps: The legacy client must follow the expected naming format . Follow step 1 and step 2 in the migration guide . Refer to the legacy client analogously to a NAIS application Example: See this example in aad-iac Pre-authorizing the legacy client in nais.yaml: spec : accessPolicy : inbound : rules : - application : dkif namespace : team-rocket cluster : dev-fss Migration guide - step by step \u00a7 The following describes the steps needed to migrate an existing legacy client where you wish to keep the existing client ID and configuration. If keeping the existing client ID and configuration is not important, it should be much easier to just provision new clients instead. Warning Be aware of the differences in tenants between the IaC repository and NAIS: nonprod -> trygdeetaten.no prod -> nav.no Step 1 - Rename your application in the Azure Portal The Display name of the application registered in the Azure Portal must match the expected format . Go to the Branding tab for your client in the Azure Portal . Update the Name . Step 2 - Update your application (and any dependants) in the IaC repository Ensure the name of the client registered in the IaC repository is updated to match the name set in step 1 . Ensure that any clients that has a reference to the previous name in their preauthorizedapplications is also updated. Step 3 - Deploy your NAIS application with Azure AD provisioning enabled See getting started . Step 4 - Delete your application from the IaC repository Verify that everything works after the migration Delete the application from the IaC repository in order to maintain a single source of truth","title":"Legacy"},{"location":"security/auth/azure-ad/legacy/#legacy","text":"This section only applies if you have an existing Azure AD client registered in the IaC repository .","title":"Legacy"},{"location":"security/auth/azure-ad/legacy/#why-migrate","text":"Declarative provisioning, straight from your application's nais.yaml No longer dependent on manual user approvals in multiple IaC repositories No longer dependent on Vault Credentials are rotated regularly, completely transparent to the application. This ensures that credentials are fresh and lessens the impact in the case of exposure. The exact same feature is present in the GCP clusters, which simplifies migration .","title":"Why migrate?"},{"location":"security/auth/azure-ad/legacy/#pre-authorization","text":"Communication between legacy clients provisioned through aad-iac and clients provisioned through NAIS requires some additional configuration. Scenario 1","title":"Pre-authorization"},{"location":"security/auth/azure-ad/legacy/#allowing-a-nais-client-to-access-an-aad-iac-client","text":"Prerequisites: You have a legacy client registered in the aad-iac repository. You would like to pre-authorize client provisioned through NAIS. Steps: Refer to the NAIS client in aad-iac using its fully qualified name (see naming format ): <cluster>:<namespace>:<app-name> Example: See this example in aad-iac . Scenario 2","title":"Allowing a NAIS client to access an aad-iac client"},{"location":"security/auth/azure-ad/legacy/#allowing-an-aad-iac-client-to-access-a-nais-client","text":"Prerequisites: You have a client provisioned through NAIS. You would like to pre-authorize a legacy client registered in the aad-iac repository. Steps: The legacy client must follow the expected naming format . Follow step 1 and step 2 in the migration guide . Refer to the legacy client analogously to a NAIS application Example: See this example in aad-iac Pre-authorizing the legacy client in nais.yaml: spec : accessPolicy : inbound : rules : - application : dkif namespace : team-rocket cluster : dev-fss","title":"Allowing an aad-iac client to access a NAIS client"},{"location":"security/auth/azure-ad/legacy/#migration-guide-step-by-step","text":"The following describes the steps needed to migrate an existing legacy client where you wish to keep the existing client ID and configuration. If keeping the existing client ID and configuration is not important, it should be much easier to just provision new clients instead. Warning Be aware of the differences in tenants between the IaC repository and NAIS: nonprod -> trygdeetaten.no prod -> nav.no Step 1 - Rename your application in the Azure Portal The Display name of the application registered in the Azure Portal must match the expected format . Go to the Branding tab for your client in the Azure Portal . Update the Name . Step 2 - Update your application (and any dependants) in the IaC repository Ensure the name of the client registered in the IaC repository is updated to match the name set in step 1 . Ensure that any clients that has a reference to the previous name in their preauthorizedapplications is also updated. Step 3 - Deploy your NAIS application with Azure AD provisioning enabled See getting started . Step 4 - Delete your application from the IaC repository Verify that everything works after the migration Delete the application from the IaC repository in order to maintain a single source of truth","title":"Migration guide - step by step"},{"location":"security/auth/azure-ad/operations/","text":"Operations \u00a7 Forcing resynchronization \u00a7 Synchronization to Azure AD only happens when at least one of three things happen: Any spec.azure.* or spec.accessPolicy.inbound.rules[] value has changed. A previously non-existing Azure AD app defined in spec.accessPolicy.inbound.rules[] has been created through NAIS. An annotation is applied to the resource: kubectl annotate azureapp <app> azure.nais.io/resync = true The annotation is removed after synchronization. It can then be re-applied to trigger new synchronizations. Forcing credential rotation \u00a7 Credential rotation happens automatically on a regular basis. However, if you need to trigger rotation manually you may do so by applying the following annotation: kubectl annotate azureapp <app> azure.nais.io/rotate = true You should then restart your pods so that the new credentials are re-injected: kubectl rollout restart deployment <app> Deletion \u00a7 The client is automatically deleted from Azure AD whenever the associated Application resource is deleted from Kubernetes. That is, if you were to run the following command: kubectl delete app <application> the AzureAdApplication resource will also be deleted from Kubernetes, and its associated Azure AD client is deleted. Consequently, the previous client ID used will no longer be available. If the application is re-deployed, the client ID will have a new and different value . Generally speaking, your application nor its consumers should not depend on or hard code the value of a client ID when acquiring tokens - see scopes . Prevent Deletion \u00a7 Sometimes you will want to prevent the deletion of the client, e.g. if your client has had a manual set up of additional Microsoft Graph permissions beyond the standard set. If you want to prevent deletion of the client, add the following annotation to your nais.yaml manifest: apiVersion : nais.io/v1alpha1 kind : Application metadata : name : myapplication namespace : myteam annotations : azure.nais.io/preserve : \"true\" If you decide that the client should be removed at a later point, remove the annotation from the spec.","title":"Operations"},{"location":"security/auth/azure-ad/operations/#operations","text":"","title":"Operations"},{"location":"security/auth/azure-ad/operations/#forcing-resynchronization","text":"Synchronization to Azure AD only happens when at least one of three things happen: Any spec.azure.* or spec.accessPolicy.inbound.rules[] value has changed. A previously non-existing Azure AD app defined in spec.accessPolicy.inbound.rules[] has been created through NAIS. An annotation is applied to the resource: kubectl annotate azureapp <app> azure.nais.io/resync = true The annotation is removed after synchronization. It can then be re-applied to trigger new synchronizations.","title":"Forcing resynchronization"},{"location":"security/auth/azure-ad/operations/#forcing-credential-rotation","text":"Credential rotation happens automatically on a regular basis. However, if you need to trigger rotation manually you may do so by applying the following annotation: kubectl annotate azureapp <app> azure.nais.io/rotate = true You should then restart your pods so that the new credentials are re-injected: kubectl rollout restart deployment <app>","title":"Forcing credential rotation"},{"location":"security/auth/azure-ad/operations/#deletion","text":"The client is automatically deleted from Azure AD whenever the associated Application resource is deleted from Kubernetes. That is, if you were to run the following command: kubectl delete app <application> the AzureAdApplication resource will also be deleted from Kubernetes, and its associated Azure AD client is deleted. Consequently, the previous client ID used will no longer be available. If the application is re-deployed, the client ID will have a new and different value . Generally speaking, your application nor its consumers should not depend on or hard code the value of a client ID when acquiring tokens - see scopes .","title":"Deletion"},{"location":"security/auth/azure-ad/operations/#prevent-deletion","text":"Sometimes you will want to prevent the deletion of the client, e.g. if your client has had a manual set up of additional Microsoft Graph permissions beyond the standard set. If you want to prevent deletion of the client, add the following annotation to your nais.yaml manifest: apiVersion : nais.io/v1alpha1 kind : Application metadata : name : myapplication namespace : myteam annotations : azure.nais.io/preserve : \"true\" If you decide that the client should be removed at a later point, remove the annotation from the spec.","title":"Prevent Deletion"},{"location":"security/auth/azure-ad/sidecar/","text":"Azure AD sidecar \u00a7 Status: Alpha This feature is only available in the GCP clusters . Experimental : this is a new feature. Use it in production, but be aware that bugs might arise. Report any issues to the #nais channel on Slack. Description \u00a7 A reverse proxy that automatically handles of Azure AD login, logout, and front-channel logout. Prerequisites Ensure that you first enable Azure AD for your application . Ensure that you also define an ingress for your application. All HTTP requests to the application will be intercepted by a sidecar (\" wonderwall \"). If the user does not have a valid local session with the sidecar, the request will be proxied as-is without modifications to the application container. In order to obtain a local session, the user must be redirected to the /oauth2/login endpoint, which performs the OpenID Connect Authorization Code Flow as specified by Microsoft . If the user successfully completed the login flow, a session is established with the sidecar. All requests that are forwarded to the application container will now contain an Authorization header with the user's access_token from Azure AD. Authorization: Bearer JWT_ACCESS_TOKEN Only the id_token acquired from this flow is validated and verified by the sidecar in accordance with the OpenID Connect specifications . Your application is responsible for validating the access_token . Spec \u00a7 Port Configuration The sidecar will occupy and use the ports 7564 and 7565 . Ensure that you do not bind to these ports from your application as they will be overridden. nais.yaml spec : azure : sidecar : enabled : true autoLogin : false errorPath : \"\" See the NAIS manifest . Endpoints \u00a7 The sidecar provides these endpoints under https://app.ingress : /oauth2/login redirects the user to Azure AD to perform the OpenID Connect Authorization Code Flow. /oauth2/callback handles callbacks from Azure AD as part of the OpenID Connect Authorization Code Flow. /oauth2/logout implements self-initiated logout . /oauth2/logout/frontchannel implements front-channel logout . Usage \u00a7 Overview \u00a7 The contract for usage of the sidecar is fairly simple. For any endpoint that requires authentication: Validate the Authorization header as specified in the responsibilities section . If the Authorization header is missing, redirect the user to the login endpoint . If the JWT access_token in the Authorization header is invalid or expired, redirect the user to the login endpoint . If you need to log out a user, redirect the user to the logout endpoint . Example See https://github.com/nais/wonderwalled for an example application that does this. Login \u00a7 Authenticate a user \u00a7 When you must authenticate a user, redirect to the user to: https://app.ingress/oauth2/login Redirect after authentication \u00a7 Redirects after successful authentication follow these rules in ascending priority: / (default). The URL set in the Referer header. The URL or relative path set in the query parameter redirect , e.g: https://app.ingress/oauth2/login?redirect=/some/path The host and scheme (if provided) are stripped from the redirect URL, which effectively only allows redirects to paths within your own ingress. Auto Login \u00a7 If you want all routes to your application to require an authenticated session, you can enable auto-login by setting the .spec.azure.sidecar.autoLogin field to true . This will make the sidecar automatically redirect any user to login when attempting to browse to any path for your application. You should still validate and check the Authorization header and the token within as specified in responsibilitites and guarantees . Logout \u00a7 When you must log a user out, redirect to the user to: https://app.ingress/oauth2/logout The user's session with the sidecar will be cleared, and the user will be redirected to Azure AD for global logout. Error Handling \u00a7 Authentication should generally not fail. However, in the event that it does happen; the sidecar automatically presents the end-users with a simple error page that allows the user to retry the authentication flow. If you wish to customize or handle these errors yourselves, set the .spec.azure.sidecar.errorPath to the absolute path within your ingress that should handle such requests: spec : azure : sidecar : errorPath : /login/error The sidecar will now redirect any errors to this path, along with the following query parameters: correlation_id - UUID that uniquely identifies the request, for tracing and log correlation. status_code - HTTP status code which indicates the type of error that occurred. Responsibilities and Guarantees \u00a7 The following describes the contract for usage of the sidecar. Sidecar \u00a7 The sidecar guarantees the following: The Authorization header is added to the original request if the user has a valid session. The Authorization header is removed from the original request if the user does not have a valid session. All HTTP requests to the /oauth2 endpoints defined above are owned by the sidecar and will never be forwarded to the application. The sidecar is safe to enable and use with multiple replicas of your application. The sidecar stores session data to a highly available Redis service on Aiven, and falls back to using cookies if unavailable. The sidecar does not : Automatically refresh the user's tokens. Secure your application's endpoints. Validate the user's access token set in the Authorization header. The token may be invalid or expired by the time your application receives it. Your application \u00a7 Your application should secure its own endpoints. That is, deny access to sensitive endpoints if the appropriate authentication is not supplied. Token Validation \u00a7 Your application should also validate the claims and signature for the Azure AD JWT access_token attached by the sidecar. That is, validate the standard claims such as iss , iat , exp , and aud . aud must be equal to your application's client ID in Azure AD.","title":"Sidecar"},{"location":"security/auth/azure-ad/sidecar/#azure-ad-sidecar","text":"Status: Alpha This feature is only available in the GCP clusters . Experimental : this is a new feature. Use it in production, but be aware that bugs might arise. Report any issues to the #nais channel on Slack.","title":"Azure AD sidecar"},{"location":"security/auth/azure-ad/sidecar/#description","text":"A reverse proxy that automatically handles of Azure AD login, logout, and front-channel logout. Prerequisites Ensure that you first enable Azure AD for your application . Ensure that you also define an ingress for your application. All HTTP requests to the application will be intercepted by a sidecar (\" wonderwall \"). If the user does not have a valid local session with the sidecar, the request will be proxied as-is without modifications to the application container. In order to obtain a local session, the user must be redirected to the /oauth2/login endpoint, which performs the OpenID Connect Authorization Code Flow as specified by Microsoft . If the user successfully completed the login flow, a session is established with the sidecar. All requests that are forwarded to the application container will now contain an Authorization header with the user's access_token from Azure AD. Authorization: Bearer JWT_ACCESS_TOKEN Only the id_token acquired from this flow is validated and verified by the sidecar in accordance with the OpenID Connect specifications . Your application is responsible for validating the access_token .","title":"Description"},{"location":"security/auth/azure-ad/sidecar/#spec","text":"Port Configuration The sidecar will occupy and use the ports 7564 and 7565 . Ensure that you do not bind to these ports from your application as they will be overridden. nais.yaml spec : azure : sidecar : enabled : true autoLogin : false errorPath : \"\" See the NAIS manifest .","title":"Spec"},{"location":"security/auth/azure-ad/sidecar/#endpoints","text":"The sidecar provides these endpoints under https://app.ingress : /oauth2/login redirects the user to Azure AD to perform the OpenID Connect Authorization Code Flow. /oauth2/callback handles callbacks from Azure AD as part of the OpenID Connect Authorization Code Flow. /oauth2/logout implements self-initiated logout . /oauth2/logout/frontchannel implements front-channel logout .","title":"Endpoints"},{"location":"security/auth/azure-ad/sidecar/#usage","text":"","title":"Usage"},{"location":"security/auth/azure-ad/sidecar/#overview","text":"The contract for usage of the sidecar is fairly simple. For any endpoint that requires authentication: Validate the Authorization header as specified in the responsibilities section . If the Authorization header is missing, redirect the user to the login endpoint . If the JWT access_token in the Authorization header is invalid or expired, redirect the user to the login endpoint . If you need to log out a user, redirect the user to the logout endpoint . Example See https://github.com/nais/wonderwalled for an example application that does this.","title":"Overview"},{"location":"security/auth/azure-ad/sidecar/#login","text":"","title":"Login"},{"location":"security/auth/azure-ad/sidecar/#authenticate-a-user","text":"When you must authenticate a user, redirect to the user to: https://app.ingress/oauth2/login","title":"Authenticate a user"},{"location":"security/auth/azure-ad/sidecar/#redirect-after-authentication","text":"Redirects after successful authentication follow these rules in ascending priority: / (default). The URL set in the Referer header. The URL or relative path set in the query parameter redirect , e.g: https://app.ingress/oauth2/login?redirect=/some/path The host and scheme (if provided) are stripped from the redirect URL, which effectively only allows redirects to paths within your own ingress.","title":"Redirect after authentication"},{"location":"security/auth/azure-ad/sidecar/#auto-login","text":"If you want all routes to your application to require an authenticated session, you can enable auto-login by setting the .spec.azure.sidecar.autoLogin field to true . This will make the sidecar automatically redirect any user to login when attempting to browse to any path for your application. You should still validate and check the Authorization header and the token within as specified in responsibilitites and guarantees .","title":"Auto Login"},{"location":"security/auth/azure-ad/sidecar/#logout","text":"When you must log a user out, redirect to the user to: https://app.ingress/oauth2/logout The user's session with the sidecar will be cleared, and the user will be redirected to Azure AD for global logout.","title":"Logout"},{"location":"security/auth/azure-ad/sidecar/#error-handling","text":"Authentication should generally not fail. However, in the event that it does happen; the sidecar automatically presents the end-users with a simple error page that allows the user to retry the authentication flow. If you wish to customize or handle these errors yourselves, set the .spec.azure.sidecar.errorPath to the absolute path within your ingress that should handle such requests: spec : azure : sidecar : errorPath : /login/error The sidecar will now redirect any errors to this path, along with the following query parameters: correlation_id - UUID that uniquely identifies the request, for tracing and log correlation. status_code - HTTP status code which indicates the type of error that occurred.","title":"Error Handling"},{"location":"security/auth/azure-ad/sidecar/#responsibilities-and-guarantees","text":"The following describes the contract for usage of the sidecar.","title":"Responsibilities and Guarantees"},{"location":"security/auth/azure-ad/sidecar/#sidecar","text":"The sidecar guarantees the following: The Authorization header is added to the original request if the user has a valid session. The Authorization header is removed from the original request if the user does not have a valid session. All HTTP requests to the /oauth2 endpoints defined above are owned by the sidecar and will never be forwarded to the application. The sidecar is safe to enable and use with multiple replicas of your application. The sidecar stores session data to a highly available Redis service on Aiven, and falls back to using cookies if unavailable. The sidecar does not : Automatically refresh the user's tokens. Secure your application's endpoints. Validate the user's access token set in the Authorization header. The token may be invalid or expired by the time your application receives it.","title":"Sidecar"},{"location":"security/auth/azure-ad/sidecar/#your-application","text":"Your application should secure its own endpoints. That is, deny access to sensitive endpoints if the appropriate authentication is not supplied.","title":"Your application"},{"location":"security/auth/azure-ad/sidecar/#token-validation","text":"Your application should also validate the claims and signature for the Azure AD JWT access_token attached by the sidecar. That is, validate the standard claims such as iss , iat , exp , and aud . aud must be equal to your application's client ID in Azure AD.","title":"Token Validation"},{"location":"security/auth/azure-ad/usage/","text":"Usage \u00a7 Info See the NAV Security Guide for NAV-specific usage. Runtime Variables & Credentials \u00a7 The following environment variables and files (under the directory /var/run/secrets/nais.io/azure ) are available at runtime: AZURE_APP_CLIENT_ID \u00a7 Note Azure AD client ID. Unique ID for the application in Azure AD Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f AZURE_APP_CLIENT_SECRET \u00a7 Note Azure AD client secret, i.e. password for authenticating the application to Azure AD Example value: b5S0Bgg1OF17Ptpy4_uvUg-m.I~KU_.5RR AZURE_APP_JWKS \u00a7 Note A JWK Set as defined in RFC7517 section 5 . This will always contain a single key, i.e. AZURE_APP_JWK - the newest key registered. Example value: { \"keys\" : [ { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" , \"x5c\" : [ \"MIID8jCC...\" ], \"x5t\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"x5t#S256\" : \"AH2gbUvjZYmSQXZ6-YIRxM2YYrLiZYW8NywowyGcxp0\" } ] } AZURE_APP_JWK \u00a7 Note Private JWK as defined in RFC7517 , i.e. a JWK with the private RSA key for creating signed JWTs when authenticating to Azure AD with a certificate . Example value: { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" , \"x5c\" : [ \"MIID8jCC...\" ], \"x5t\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"x5t#S256\" : \"AH2gbUvjZYmSQXZ6-YIRxM2YYrLiZYW8NywowyGcxp0\" } AZURE_APP_PRE_AUTHORIZED_APPS \u00a7 Note A JSON string. List of names and client IDs for the valid (i.e. those that exist in Azure AD) applications defined in spec.accessPolicy.inbound.rules[] Example value: [ { \"name\" : \"dev-gcp:othernamespace:app-a\" , \"clientId\" : \"381ce452-1d49-49df-9e7e-990ef0328d6c\" }, { \"name\" : \"dev-gcp:aura:app-b\" , \"clientId\" : \"048eb0e8-e18a-473a-a87d-dfede7c65d84\" } ] AZURE_APP_TENANT_ID \u00a7 Note The Azure AD tenant ID for which the Azure AD client resides in. Example value: 77678b69-1daf-47b6-9072-771d270ac800 AZURE_APP_WELL_KNOWN_URL \u00a7 Note The well-known URL to the metadata discovery document for the specific tenant in which the Azure AD client resides in. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/v2.0/.well-known/openid-configuration AZURE_OPENID_CONFIG_ISSUER \u00a7 Note issuer from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/v2.0 AZURE_OPENID_CONFIG_JWKS_URI \u00a7 Note jwks_uri from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/discovery/v2.0/keys AZURE_OPENID_CONFIG_TOKEN_ENDPOINT \u00a7 Note token_endpoint from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/oauth2/v2.0/token","title":"Usage"},{"location":"security/auth/azure-ad/usage/#usage","text":"Info See the NAV Security Guide for NAV-specific usage.","title":"Usage"},{"location":"security/auth/azure-ad/usage/#runtime-variables-credentials","text":"The following environment variables and files (under the directory /var/run/secrets/nais.io/azure ) are available at runtime:","title":"Runtime Variables &amp; Credentials"},{"location":"security/auth/azure-ad/usage/#azure_app_client_id","text":"Note Azure AD client ID. Unique ID for the application in Azure AD Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f","title":"AZURE_APP_CLIENT_ID"},{"location":"security/auth/azure-ad/usage/#azure_app_client_secret","text":"Note Azure AD client secret, i.e. password for authenticating the application to Azure AD Example value: b5S0Bgg1OF17Ptpy4_uvUg-m.I~KU_.5RR","title":"AZURE_APP_CLIENT_SECRET"},{"location":"security/auth/azure-ad/usage/#azure_app_jwks","text":"Note A JWK Set as defined in RFC7517 section 5 . This will always contain a single key, i.e. AZURE_APP_JWK - the newest key registered. Example value: { \"keys\" : [ { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" , \"x5c\" : [ \"MIID8jCC...\" ], \"x5t\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"x5t#S256\" : \"AH2gbUvjZYmSQXZ6-YIRxM2YYrLiZYW8NywowyGcxp0\" } ] }","title":"AZURE_APP_JWKS"},{"location":"security/auth/azure-ad/usage/#azure_app_jwk","text":"Note Private JWK as defined in RFC7517 , i.e. a JWK with the private RSA key for creating signed JWTs when authenticating to Azure AD with a certificate . Example value: { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" , \"x5c\" : [ \"MIID8jCC...\" ], \"x5t\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"x5t#S256\" : \"AH2gbUvjZYmSQXZ6-YIRxM2YYrLiZYW8NywowyGcxp0\" }","title":"AZURE_APP_JWK"},{"location":"security/auth/azure-ad/usage/#azure_app_pre_authorized_apps","text":"Note A JSON string. List of names and client IDs for the valid (i.e. those that exist in Azure AD) applications defined in spec.accessPolicy.inbound.rules[] Example value: [ { \"name\" : \"dev-gcp:othernamespace:app-a\" , \"clientId\" : \"381ce452-1d49-49df-9e7e-990ef0328d6c\" }, { \"name\" : \"dev-gcp:aura:app-b\" , \"clientId\" : \"048eb0e8-e18a-473a-a87d-dfede7c65d84\" } ]","title":"AZURE_APP_PRE_AUTHORIZED_APPS"},{"location":"security/auth/azure-ad/usage/#azure_app_tenant_id","text":"Note The Azure AD tenant ID for which the Azure AD client resides in. Example value: 77678b69-1daf-47b6-9072-771d270ac800","title":"AZURE_APP_TENANT_ID"},{"location":"security/auth/azure-ad/usage/#azure_app_well_known_url","text":"Note The well-known URL to the metadata discovery document for the specific tenant in which the Azure AD client resides in. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/v2.0/.well-known/openid-configuration","title":"AZURE_APP_WELL_KNOWN_URL"},{"location":"security/auth/azure-ad/usage/#azure_openid_config_issuer","text":"Note issuer from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/v2.0","title":"AZURE_OPENID_CONFIG_ISSUER"},{"location":"security/auth/azure-ad/usage/#azure_openid_config_jwks_uri","text":"Note jwks_uri from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/discovery/v2.0/keys","title":"AZURE_OPENID_CONFIG_JWKS_URI"},{"location":"security/auth/azure-ad/usage/#azure_openid_config_token_endpoint","text":"Note token_endpoint from the metadata discovery document. Example value: https://login.microsoftonline.com/77678b69-1daf-47b6-9072-771d270ac800/oauth2/v2.0/token","title":"AZURE_OPENID_CONFIG_TOKEN_ENDPOINT"},{"location":"security/auth/idporten/","text":"ID-porten \u00a7 Status: Opt-In Open Beta This feature is only available in team namespaces Forthcoming changes ID-porten is currently undergoing some changes . These changes will roll out in the coming months. TL;DR: new URL and issuer, PKCE is required for Authorization Code Flow and the contents of the sub claim will likely change. Abstract \u00a7 Abstract ID-porten is a common log-in system used for logging into Norwegian public e-services for citizens. The NAIS platform provides support for simple, declarative provisioning of an ID-porten client with sensible defaults that your application may use to integrate with ID-porten. An ID-porten client allows your application to leverage ID-porten for authentication of citizen end-users, providing sign-in capabilities with single sign-on (SSO). To achieve this, your application must implement OpenID Connect with the Authorization Code flow. This is also a critical first step in request chains involving an end-user whose identity and permissions should be propagated through each service/web API when accessing services in NAV using the OAuth 2.0 Token Exchange protocol. See the TokenX documentation for details. Info See the NAV Security Guide for NAV-specific usage of this client. Warning Please ensure that you have read the ID-porten Integration guide . Configuration \u00a7 Getting Started \u00a7 nais.yaml spec : idporten : enabled : true # optional, default shown clientURI : \"https://nav.no\" # optional, default shown redirectPath : \"/oauth2/callback\" # optional, default shown frontchannelLogoutPath : \"/oauth2/logout\" # optional, defaults shown postLogoutRedirectURIs : - \"https://nav.no\" # optional, in seconds - defaults shown (1 hour) accessTokenLifetime : 3600 # optional, in seconds - defaults shown (2 hours) sessionLifetime : 7200 # required for on-premises only webproxy : true Spec \u00a7 See the NAIS manifest . Access Policies \u00a7 ID-porten is a third-party service outside of our clusters, which is not reachable by default like most third-party services. Google Cloud Platform (GCP) \u00a7 The following outbound external hosts are automatically added when enabling this feature: oidc-ver2.difi.no in development oidc.difi.no in production You do not need to specify these explicitly. On-premises \u00a7 You must enable and use webproxy for external communication. Ingresses \u00a7 Danger For security reasons you may only specify one ingress when this feature is enabled. Redirect URI \u00a7 The redirect URI is the fully qualified URI that ID-porten redirects back to after a successful authorization request. NAIS will automatically infer the complete redirect URI to be registered at ID-porten using the scheme: spec.ingresses[0] + spec.idporten.redirectPath where spec.idporten.redirectPath has a default value of /oauth2/callback . E.g. https://my.application.ingress/oauth2/callback If you wish to use a different path than the default, you may do so by manually specifying spec.idporten.redirectPath . Logout URIs \u00a7 Warning When integrating with ID-porten, you are required to correctly implement proper logout functionality. Refer to the documentation at DigDir for further details. Self-initiated Logout \u00a7 When logout is initiated from your client , you must redirect the given user to ID-porten's endsession -endpoint. ID-porten will then log the user out from all other services connected to the same single sign-on session. If the optional parameters id_token_hint and post_logout_redirect_uri are set in the request, ID-porten will redirect the user to the specified URI given that the URI is registered for the client. Front-channel Logout \u00a7 Front-channel logouts are logouts initiated by other ID-porten clients. Your application will receive a GET request from ID-porten at frontchannel_logout_uri . This request includes two parameters: iss which denotes the issuer for the Identity Provider sid which denotes the user's associated session ID at ID-porten which is set in the sid claim in the user's id_token In short, when receiving such a request you are obligated to clear the local session for your application for the given user's sid so that the user is properly logged out across all services in the circle-of-trust. Your application's frontchannel_logout_uri is by default automatically inferred by NAIS and registered at ID-porten using the following scheme: spec.ingresses[0] + spec.idporten.frontchannelLogoutPath where spec.idporten.frontchannelLogoutPath has a default value of /oauth2/logout . E.g. https://my.application.ingress/oauth2/logout If you wish to use a different path than the default, you may do so by manually specifying spec.idporten.frontchannelLogoutPath . Usage \u00a7 Info See the NAV Security Guide for NAV-specific usage. Runtime Variables & Credentials \u00a7 The following environment variables and files (under the directory /var/run/secrets/nais.io/idporten ) are available at runtime: IDPORTEN_CLIENT_ID \u00a7 Note ID-porten client ID. Unique ID for the application in ID-porten. Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f IDPORTEN_CLIENT_JWK \u00a7 Note Private JWK containing the private RSA key for creating signed JWTs when authenticating to ID-porten with a JWT grant . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"alg\" : \"RS256\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" } IDPORTEN_REDIRECT_URI \u00a7 Note The redirect URI registered for the client at ID-porten. This must be a valid URI for the application where the user is redirected back to after successful authentication and authorization. Example value: https://my.application.dev.nav.no/callback IDPORTEN_WELL_KNOWN_URL \u00a7 Note The well-known URL for the OIDC metadata discovery document for ID-porten. Example value: https://oidc-ver2.difi.no/idporten-oidc-provider/.well-known/openid-configuration Test Users for Logins \u00a7 ID-porten maintains a public list of test users found here . Permanently deleting a client \u00a7 Warning Permanent deletes are irreversible. Only do this if you are certain that you wish to completely remove the client from DigDir. When an IDPortenClient resource is deleted from a Kubernetes cluster, the client is not deleted from DigDir. Info The Application resource owns the IDPortenClient resource, deletion of the former will thus trigger a deletion of the latter. If the IDPortenClient resource is recreated, the client will thus retain the same client ID. If you want to completely delete the client from DigDir, you must add the following annotation to the IDPortenClient resource: kubectl annotate idportenclient <app> digdir.nais.io/delete = true When this annotation is in place, deleting the IDPortenClient resource from Kubernetes will trigger removal of the client from DigDir.","title":"Intro"},{"location":"security/auth/idporten/#id-porten","text":"Status: Opt-In Open Beta This feature is only available in team namespaces Forthcoming changes ID-porten is currently undergoing some changes . These changes will roll out in the coming months. TL;DR: new URL and issuer, PKCE is required for Authorization Code Flow and the contents of the sub claim will likely change.","title":"ID-porten"},{"location":"security/auth/idporten/#abstract","text":"Abstract ID-porten is a common log-in system used for logging into Norwegian public e-services for citizens. The NAIS platform provides support for simple, declarative provisioning of an ID-porten client with sensible defaults that your application may use to integrate with ID-porten. An ID-porten client allows your application to leverage ID-porten for authentication of citizen end-users, providing sign-in capabilities with single sign-on (SSO). To achieve this, your application must implement OpenID Connect with the Authorization Code flow. This is also a critical first step in request chains involving an end-user whose identity and permissions should be propagated through each service/web API when accessing services in NAV using the OAuth 2.0 Token Exchange protocol. See the TokenX documentation for details. Info See the NAV Security Guide for NAV-specific usage of this client. Warning Please ensure that you have read the ID-porten Integration guide .","title":"Abstract"},{"location":"security/auth/idporten/#configuration","text":"","title":"Configuration"},{"location":"security/auth/idporten/#getting-started","text":"nais.yaml spec : idporten : enabled : true # optional, default shown clientURI : \"https://nav.no\" # optional, default shown redirectPath : \"/oauth2/callback\" # optional, default shown frontchannelLogoutPath : \"/oauth2/logout\" # optional, defaults shown postLogoutRedirectURIs : - \"https://nav.no\" # optional, in seconds - defaults shown (1 hour) accessTokenLifetime : 3600 # optional, in seconds - defaults shown (2 hours) sessionLifetime : 7200 # required for on-premises only webproxy : true","title":"Getting Started"},{"location":"security/auth/idporten/#spec","text":"See the NAIS manifest .","title":"Spec"},{"location":"security/auth/idporten/#access-policies","text":"ID-porten is a third-party service outside of our clusters, which is not reachable by default like most third-party services.","title":"Access Policies"},{"location":"security/auth/idporten/#google-cloud-platform-gcp","text":"The following outbound external hosts are automatically added when enabling this feature: oidc-ver2.difi.no in development oidc.difi.no in production You do not need to specify these explicitly.","title":"Google Cloud Platform (GCP)"},{"location":"security/auth/idporten/#on-premises","text":"You must enable and use webproxy for external communication.","title":"On-premises"},{"location":"security/auth/idporten/#ingresses","text":"Danger For security reasons you may only specify one ingress when this feature is enabled.","title":"Ingresses"},{"location":"security/auth/idporten/#redirect-uri","text":"The redirect URI is the fully qualified URI that ID-porten redirects back to after a successful authorization request. NAIS will automatically infer the complete redirect URI to be registered at ID-porten using the scheme: spec.ingresses[0] + spec.idporten.redirectPath where spec.idporten.redirectPath has a default value of /oauth2/callback . E.g. https://my.application.ingress/oauth2/callback If you wish to use a different path than the default, you may do so by manually specifying spec.idporten.redirectPath .","title":"Redirect URI"},{"location":"security/auth/idporten/#logout-uris","text":"Warning When integrating with ID-porten, you are required to correctly implement proper logout functionality. Refer to the documentation at DigDir for further details.","title":"Logout URIs"},{"location":"security/auth/idporten/#self-initiated-logout","text":"When logout is initiated from your client , you must redirect the given user to ID-porten's endsession -endpoint. ID-porten will then log the user out from all other services connected to the same single sign-on session. If the optional parameters id_token_hint and post_logout_redirect_uri are set in the request, ID-porten will redirect the user to the specified URI given that the URI is registered for the client.","title":"Self-initiated Logout"},{"location":"security/auth/idporten/#front-channel-logout","text":"Front-channel logouts are logouts initiated by other ID-porten clients. Your application will receive a GET request from ID-porten at frontchannel_logout_uri . This request includes two parameters: iss which denotes the issuer for the Identity Provider sid which denotes the user's associated session ID at ID-porten which is set in the sid claim in the user's id_token In short, when receiving such a request you are obligated to clear the local session for your application for the given user's sid so that the user is properly logged out across all services in the circle-of-trust. Your application's frontchannel_logout_uri is by default automatically inferred by NAIS and registered at ID-porten using the following scheme: spec.ingresses[0] + spec.idporten.frontchannelLogoutPath where spec.idporten.frontchannelLogoutPath has a default value of /oauth2/logout . E.g. https://my.application.ingress/oauth2/logout If you wish to use a different path than the default, you may do so by manually specifying spec.idporten.frontchannelLogoutPath .","title":"Front-channel Logout"},{"location":"security/auth/idporten/#usage","text":"Info See the NAV Security Guide for NAV-specific usage.","title":"Usage"},{"location":"security/auth/idporten/#runtime-variables-credentials","text":"The following environment variables and files (under the directory /var/run/secrets/nais.io/idporten ) are available at runtime:","title":"Runtime Variables &amp; Credentials"},{"location":"security/auth/idporten/#idporten_client_id","text":"Note ID-porten client ID. Unique ID for the application in ID-porten. Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f","title":"IDPORTEN_CLIENT_ID"},{"location":"security/auth/idporten/#idporten_client_jwk","text":"Note Private JWK containing the private RSA key for creating signed JWTs when authenticating to ID-porten with a JWT grant . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"alg\" : \"RS256\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" }","title":"IDPORTEN_CLIENT_JWK"},{"location":"security/auth/idporten/#idporten_redirect_uri","text":"Note The redirect URI registered for the client at ID-porten. This must be a valid URI for the application where the user is redirected back to after successful authentication and authorization. Example value: https://my.application.dev.nav.no/callback","title":"IDPORTEN_REDIRECT_URI"},{"location":"security/auth/idporten/#idporten_well_known_url","text":"Note The well-known URL for the OIDC metadata discovery document for ID-porten. Example value: https://oidc-ver2.difi.no/idporten-oidc-provider/.well-known/openid-configuration","title":"IDPORTEN_WELL_KNOWN_URL"},{"location":"security/auth/idporten/#test-users-for-logins","text":"ID-porten maintains a public list of test users found here .","title":"Test Users for Logins"},{"location":"security/auth/idporten/#permanently-deleting-a-client","text":"Warning Permanent deletes are irreversible. Only do this if you are certain that you wish to completely remove the client from DigDir. When an IDPortenClient resource is deleted from a Kubernetes cluster, the client is not deleted from DigDir. Info The Application resource owns the IDPortenClient resource, deletion of the former will thus trigger a deletion of the latter. If the IDPortenClient resource is recreated, the client will thus retain the same client ID. If you want to completely delete the client from DigDir, you must add the following annotation to the IDPortenClient resource: kubectl annotate idportenclient <app> digdir.nais.io/delete = true When this annotation is in place, deleting the IDPortenClient resource from Kubernetes will trigger removal of the client from DigDir.","title":"Permanently deleting a client"},{"location":"security/auth/idporten/sidecar/","text":"ID-porten sidecar \u00a7 Status: Beta This feature is only available in the GCP clusters . Experimental : users report that this component is working, but it needs a broader audience to be battle-tested properly. Report any issues to the #nais channel on Slack. Description \u00a7 A reverse proxy that automatically handles of ID-porten login, logout, and front-channel logout. Prerequisite Ensure that you first enable ID-porten for your application . All HTTP requests to the application will be intercepted by a sidecar (\" wonderwall \"). If the user does not have a valid local session with the sidecar, the request will be proxied as-is without modifications to the application container. In order to obtain a local session, the user must be redirected to the /oauth2/login endpoint, which performs the OpenID Connect Authorization Code Flow as specified by ID-porten . If the user successfully completed the login flow, a session is established with the sidecar. All requests that are forwarded to the application container will now contain an Authorization header with the user's access_token from ID-porten. Authorization: Bearer JWT_ACCESS_TOKEN Only the id_token acquired from this flow is validated and verified by the sidecar in accordance with the OpenID Connect specifications . Your application is responsible for validating the access_token . Spec \u00a7 Port Configuration The sidecar will occupy and use the ports 7564 and 7565 . Ensure that you do not bind to these ports from your application as they will be overridden. nais.yaml spec : idporten : sidecar : enabled : true level : Level4 locale : nb autoLogin : false errorPath : \"\" See the NAIS manifest . Endpoints \u00a7 The sidecar provides these endpoints under https://app.ingress : /oauth2/login redirects the user to ID-porten to perform the OpenID Connect Authorization Code Flow. /oauth2/callback handles callbacks from ID-porten as part of the OpenID Connect Authorization Code Flow. /oauth2/logout implements self-initiated logout . /oauth2/logout/frontchannel implements front-channel logout . Usage \u00a7 Overview \u00a7 The contract for usage of the sidecar is fairly simple. For any endpoint that requires authentication: Validate the Authorization header as specified in the responsibilities section . If the Authorization header is missing, redirect the user to the login endpoint . If the JWT access_token in the Authorization header is invalid or expired, redirect the user to the login endpoint . If you need to log out a user, redirect the user to the logout endpoint . Example See https://github.com/nais/wonderwalled for an example application that does this. Login \u00a7 Authenticate a user \u00a7 When you must authenticate a user, redirect to the user to: https://app.ingress/oauth2/login Redirect after authentication \u00a7 Redirects after successful authentication follow these rules in ascending priority: / (default). The URL set in the Referer header. The URL or relative path set in the query parameter redirect , e.g: https://app.ingress/oauth2/login?redirect=/some/path The host and scheme (if provided) are stripped from the redirect URL, which effectively only allows redirects to paths within your own ingress. Security Levels \u00a7 ID-porten supports different levels of security when authenticating users. This is sent by the sidecar as the acr_values parameter to the /authorize endpoint . Valid values are Level3 or Level4 . You can set a default value for all requests by specifying spec.idporten.sidecar.level . If unspecified, the sidecar will use Level4 as the default value. For runtime control of the value, set the query parameter level when redirecting the user to login: https://app.ingress/oauth2/login?level=Level4 Locales \u00a7 ID-porten supports a few different locales for the user interface during authentication. This is sent by the sidecar as the ui_locales parameter to the /authorize endpoint . Valid values shown below: Value Description nb Norwegian Bokm\u00e5l nn Norwegian Nynorsk en English se S\u00e1mi You can set a default value for all requests by specifying spec.idporten.sidecar.locale . If unspecified, the sidecar will use nb as the default value. For runtime control of the value, set the query parameter locale when redirecting the user to login: https://app.ingress/oauth2/login?locale=en Auto Login \u00a7 If you want all routes to your application to require an authenticated session, you can enable auto-login by setting the .spec.idporten.sidecar.autoLogin field to true . This will make the sidecar automatically redirect any user to login when attempting to browse to any path for your application. You should still validate and check the Authorization header and the token within as specified in responsibilitites and guarantees . Logout \u00a7 When you must log a user out, redirect to the user to: https://app.ingress/oauth2/logout The user's session with the sidecar will be cleared, and the user will be redirected to ID-porten for global logout. Redirect after logout \u00a7 After the user is successfully logged out at ID-porten, the user may be redirected to another URI. By default, the sidecar will indicate to ID-porten that the user should be redirected to https://www.nav.no . You may configure or override this in two ways: Default for all users: If defined, the first entry in .spec.idporten.postLogoutRedirectURIs will be used. Runtime per-user: Set the post_logout_redirect_uri parameter when redirecting the user to logout: https://app.ingress/oauth2/logout?post_logout_redirect_uri=https://example.nav.no Note that ID-porten requires the exact redirect URI to be pre-registered. That is, the complete URI should be listed under .spec.idporten.postLogoutRedirectURIs . Otherwise, the user will not be redirected to the given URI. Error Handling \u00a7 Authentication should generally not fail. However, in the event that it does happen; the sidecar automatically presents the end-users with a simple error page that allows the user to retry the authentication flow. If you wish to customize or handle these errors yourselves, set the .spec.idporten.sidecar.errorPath to the absolute path within your ingress that should handle such requests: spec : idporten : sidecar : errorPath : /login/error The sidecar will now redirect any errors to this path, along with the following query parameters: correlation_id - UUID that uniquely identifies the request, for tracing and log correlation. status_code - HTTP status code which indicates the type of error that occurred. Responsibilities and Guarantees \u00a7 The following describes the contract for usage of the sidecar. Sidecar \u00a7 The sidecar guarantees the following: The Authorization header is added to the original request if the user has a valid session. The Authorization header is removed from the original request if the user does not have a valid session. All HTTP requests to the /oauth2 endpoints defined above are owned by the sidecar and will never be forwarded to the application. The sidecar is safe to enable and use with multiple replicas of your application. The sidecar stores session data to a highly available Redis service on Aiven, and falls back to using cookies if unavailable. The sidecar does not : Automatically refresh the user's tokens. Secure your application's endpoints. Validate the user's access token set in the Authorization header. The token may be invalid or expired by the time your application receives it. Your application \u00a7 Your application should secure its own endpoints. That is, deny access to sensitive endpoints if the appropriate authentication is not supplied. Token Validation \u00a7 Your application should also validate the claims and signature for the ID-porten JWT access_token attached by the sidecar. That is, validate the standard claims such as iss , iat , exp . Note that the aud claim is not set for ID-porten access tokens. You should instead validate that the client_id claim has a value equal to your own ID-porten client ID . Validate that the acr claim exists and that the set level matches the minimum security level for your endpoints: If your endpoint(s) accepts a minimum of Level3 authentication, you must also accept Level4 . The inverse should be rejected. That is, applications expecting Level4 authentication should NOT accept tokens with acr=Level3 .","title":"Sidecar"},{"location":"security/auth/idporten/sidecar/#id-porten-sidecar","text":"Status: Beta This feature is only available in the GCP clusters . Experimental : users report that this component is working, but it needs a broader audience to be battle-tested properly. Report any issues to the #nais channel on Slack.","title":"ID-porten sidecar"},{"location":"security/auth/idporten/sidecar/#description","text":"A reverse proxy that automatically handles of ID-porten login, logout, and front-channel logout. Prerequisite Ensure that you first enable ID-porten for your application . All HTTP requests to the application will be intercepted by a sidecar (\" wonderwall \"). If the user does not have a valid local session with the sidecar, the request will be proxied as-is without modifications to the application container. In order to obtain a local session, the user must be redirected to the /oauth2/login endpoint, which performs the OpenID Connect Authorization Code Flow as specified by ID-porten . If the user successfully completed the login flow, a session is established with the sidecar. All requests that are forwarded to the application container will now contain an Authorization header with the user's access_token from ID-porten. Authorization: Bearer JWT_ACCESS_TOKEN Only the id_token acquired from this flow is validated and verified by the sidecar in accordance with the OpenID Connect specifications . Your application is responsible for validating the access_token .","title":"Description"},{"location":"security/auth/idporten/sidecar/#spec","text":"Port Configuration The sidecar will occupy and use the ports 7564 and 7565 . Ensure that you do not bind to these ports from your application as they will be overridden. nais.yaml spec : idporten : sidecar : enabled : true level : Level4 locale : nb autoLogin : false errorPath : \"\" See the NAIS manifest .","title":"Spec"},{"location":"security/auth/idporten/sidecar/#endpoints","text":"The sidecar provides these endpoints under https://app.ingress : /oauth2/login redirects the user to ID-porten to perform the OpenID Connect Authorization Code Flow. /oauth2/callback handles callbacks from ID-porten as part of the OpenID Connect Authorization Code Flow. /oauth2/logout implements self-initiated logout . /oauth2/logout/frontchannel implements front-channel logout .","title":"Endpoints"},{"location":"security/auth/idporten/sidecar/#usage","text":"","title":"Usage"},{"location":"security/auth/idporten/sidecar/#overview","text":"The contract for usage of the sidecar is fairly simple. For any endpoint that requires authentication: Validate the Authorization header as specified in the responsibilities section . If the Authorization header is missing, redirect the user to the login endpoint . If the JWT access_token in the Authorization header is invalid or expired, redirect the user to the login endpoint . If you need to log out a user, redirect the user to the logout endpoint . Example See https://github.com/nais/wonderwalled for an example application that does this.","title":"Overview"},{"location":"security/auth/idporten/sidecar/#login","text":"","title":"Login"},{"location":"security/auth/idporten/sidecar/#authenticate-a-user","text":"When you must authenticate a user, redirect to the user to: https://app.ingress/oauth2/login","title":"Authenticate a user"},{"location":"security/auth/idporten/sidecar/#redirect-after-authentication","text":"Redirects after successful authentication follow these rules in ascending priority: / (default). The URL set in the Referer header. The URL or relative path set in the query parameter redirect , e.g: https://app.ingress/oauth2/login?redirect=/some/path The host and scheme (if provided) are stripped from the redirect URL, which effectively only allows redirects to paths within your own ingress.","title":"Redirect after authentication"},{"location":"security/auth/idporten/sidecar/#security-levels","text":"ID-porten supports different levels of security when authenticating users. This is sent by the sidecar as the acr_values parameter to the /authorize endpoint . Valid values are Level3 or Level4 . You can set a default value for all requests by specifying spec.idporten.sidecar.level . If unspecified, the sidecar will use Level4 as the default value. For runtime control of the value, set the query parameter level when redirecting the user to login: https://app.ingress/oauth2/login?level=Level4","title":"Security Levels"},{"location":"security/auth/idporten/sidecar/#locales","text":"ID-porten supports a few different locales for the user interface during authentication. This is sent by the sidecar as the ui_locales parameter to the /authorize endpoint . Valid values shown below: Value Description nb Norwegian Bokm\u00e5l nn Norwegian Nynorsk en English se S\u00e1mi You can set a default value for all requests by specifying spec.idporten.sidecar.locale . If unspecified, the sidecar will use nb as the default value. For runtime control of the value, set the query parameter locale when redirecting the user to login: https://app.ingress/oauth2/login?locale=en","title":"Locales"},{"location":"security/auth/idporten/sidecar/#auto-login","text":"If you want all routes to your application to require an authenticated session, you can enable auto-login by setting the .spec.idporten.sidecar.autoLogin field to true . This will make the sidecar automatically redirect any user to login when attempting to browse to any path for your application. You should still validate and check the Authorization header and the token within as specified in responsibilitites and guarantees .","title":"Auto Login"},{"location":"security/auth/idporten/sidecar/#logout","text":"When you must log a user out, redirect to the user to: https://app.ingress/oauth2/logout The user's session with the sidecar will be cleared, and the user will be redirected to ID-porten for global logout.","title":"Logout"},{"location":"security/auth/idporten/sidecar/#redirect-after-logout","text":"After the user is successfully logged out at ID-porten, the user may be redirected to another URI. By default, the sidecar will indicate to ID-porten that the user should be redirected to https://www.nav.no . You may configure or override this in two ways: Default for all users: If defined, the first entry in .spec.idporten.postLogoutRedirectURIs will be used. Runtime per-user: Set the post_logout_redirect_uri parameter when redirecting the user to logout: https://app.ingress/oauth2/logout?post_logout_redirect_uri=https://example.nav.no Note that ID-porten requires the exact redirect URI to be pre-registered. That is, the complete URI should be listed under .spec.idporten.postLogoutRedirectURIs . Otherwise, the user will not be redirected to the given URI.","title":"Redirect after logout"},{"location":"security/auth/idporten/sidecar/#error-handling","text":"Authentication should generally not fail. However, in the event that it does happen; the sidecar automatically presents the end-users with a simple error page that allows the user to retry the authentication flow. If you wish to customize or handle these errors yourselves, set the .spec.idporten.sidecar.errorPath to the absolute path within your ingress that should handle such requests: spec : idporten : sidecar : errorPath : /login/error The sidecar will now redirect any errors to this path, along with the following query parameters: correlation_id - UUID that uniquely identifies the request, for tracing and log correlation. status_code - HTTP status code which indicates the type of error that occurred.","title":"Error Handling"},{"location":"security/auth/idporten/sidecar/#responsibilities-and-guarantees","text":"The following describes the contract for usage of the sidecar.","title":"Responsibilities and Guarantees"},{"location":"security/auth/idporten/sidecar/#sidecar","text":"The sidecar guarantees the following: The Authorization header is added to the original request if the user has a valid session. The Authorization header is removed from the original request if the user does not have a valid session. All HTTP requests to the /oauth2 endpoints defined above are owned by the sidecar and will never be forwarded to the application. The sidecar is safe to enable and use with multiple replicas of your application. The sidecar stores session data to a highly available Redis service on Aiven, and falls back to using cookies if unavailable. The sidecar does not : Automatically refresh the user's tokens. Secure your application's endpoints. Validate the user's access token set in the Authorization header. The token may be invalid or expired by the time your application receives it.","title":"Sidecar"},{"location":"security/auth/idporten/sidecar/#your-application","text":"Your application should secure its own endpoints. That is, deny access to sensitive endpoints if the appropriate authentication is not supplied.","title":"Your application"},{"location":"security/auth/idporten/sidecar/#token-validation","text":"Your application should also validate the claims and signature for the ID-porten JWT access_token attached by the sidecar. That is, validate the standard claims such as iss , iat , exp . Note that the aud claim is not set for ID-porten access tokens. You should instead validate that the client_id claim has a value equal to your own ID-porten client ID . Validate that the acr claim exists and that the set level matches the minimum security level for your endpoints: If your endpoint(s) accepts a minimum of Level3 authentication, you must also accept Level4 . The inverse should be rejected. That is, applications expecting Level4 authentication should NOT accept tokens with acr=Level3 .","title":"Token Validation"},{"location":"security/auth/maskinporten/","text":"Maskinporten \u00a7 Status: Opt-In Open Beta This feature is only available in team namespaces Abstract \u00a7 Abstract Maskinporten is a service provided by DigDir that allows API providers - in this case, external agencies - to securely enforce server-to-server authorization of their exposed APIs using OAuth 2.0 JWT grants . It allows API providers to model access policies by using scopes based on the organization numbers of the consumers. The NAIS platform provides support for declarative registration of Maskinporten resources. These cover two distinct use cases: For API consumers : - a client that your application may use to integrate with Maskinporten, and in turn consume services and APIs served by external agencies For API providers : - user-defined scopes within Maskinporten that are exposed to and consumable by other organizations that are granted access. Client \u00a7 If you want to consume an external API, you'll need a client . Scopes \u00a7 If you want to expose an API to external consumsers, you'll need to define scopes .","title":"Intro"},{"location":"security/auth/maskinporten/#maskinporten","text":"Status: Opt-In Open Beta This feature is only available in team namespaces","title":"Maskinporten"},{"location":"security/auth/maskinporten/#abstract","text":"Abstract Maskinporten is a service provided by DigDir that allows API providers - in this case, external agencies - to securely enforce server-to-server authorization of their exposed APIs using OAuth 2.0 JWT grants . It allows API providers to model access policies by using scopes based on the organization numbers of the consumers. The NAIS platform provides support for declarative registration of Maskinporten resources. These cover two distinct use cases: For API consumers : - a client that your application may use to integrate with Maskinporten, and in turn consume services and APIs served by external agencies For API providers : - user-defined scopes within Maskinporten that are exposed to and consumable by other organizations that are granted access.","title":"Abstract"},{"location":"security/auth/maskinporten/#client","text":"If you want to consume an external API, you'll need a client .","title":"Client"},{"location":"security/auth/maskinporten/#scopes","text":"If you want to expose an API to external consumsers, you'll need to define scopes .","title":"Scopes"},{"location":"security/auth/maskinporten/client/","text":"Client \u00a7 The NAIS platform provides support for simple declarative provisioning of a client that your application may use to integrate with Maskinporten, and in turn consume services and APIs served by external agencies. The client allows your application to leverage Maskinporten for authentication and authorization when performing service-to-service requests to external agencies. To achieve this, your application must: implement JWT grants request tokens from the /token -endpoint with the above JWT grants When a client requests a token from Maskinporten: Maskinporten validates the validity of the JWT and its signature ( Runtime JWK Secret used to sign the JWT). If the client has access to the requested list of scopes , an access_token will be returned to the client. The token can be used for authentication to the intended external service. Danger Make sure that the relevant service providers have pre-registered NAV 's organization (number: 889640782 ) as a valid consumer of any scopes that you define. Provisioning of client will fail otherwise. NAV\u00b4s pre-registered scopes can be found with proper access rights in Digdir selvbetjening . Getting Started \u00a7 Access Policies \u00a7 Maskinporten is a third-party service outside of our clusters, which is not reachable by default like most third-party services. Google Cloud Platform (GCP) \u00a7 The following outbound external hosts are automatically added when enabling this feature: ver2.maskinporten.no in development maskinporten.no in production You do not need to specify these explicitly. On-premises \u00a7 You must enable and use webproxy for external communication. Spec \u00a7 See the NAIS manifest . Configuration \u00a7 nais.yaml spec : maskinporten : enabled : true scopes : consumes : - name : \"skatt:some.scope\" - name : \"nav:some/other/scope\" # required for on-premises only webproxy : true Usage \u00a7 Runtime Variables and Credentials \u00a7 The following environment variables and files (under the directory /var/run/secrets/nais.io/maskinporten ) are available at runtime: MASKINPORTEN_CLIENT_ID \u00a7 Note Maskinporten client ID. Unique ID for the application in Maskinporten. Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f MASKINPORTEN_SCOPES \u00a7 Note The scopes registered for the client at Maskinporten as a whitepace-separated string. See JWT grants for more information. Example value: nav:first/scope nav:another/scope MASKINPORTEN_WELL_KNOWN_URL \u00a7 Note The well-known URL for the OIDC metadata discovery document for Maskinporten. Example value: https://ver2.maskinporten.no/.well-known/oauth-authorization-server MASKINPORTEN_CLIENT_JWK \u00a7 Note Private JWK containing the private RSA key for creating signed JWTs when using the JWT grants . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"alg\" : \"RS256\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" } Consuming an API \u00a7 Refer to the documentation at DigDir . You may skip any step involving client registration as this is automatically handled when enabling this feature . Legacy \u00a7 Info This section only applies if you have an existing client registered at the IaC repository Migration guide to keep existing Maskinporten client (NAIS application only) \u00a7 The following describes the steps needed to migrate a client registered in IaC repository . Step 1 - Update your client description in the IaC repository \u00a7 Ensure the description of the client registered in the IaC repository follows the naming scheme: <cluster>:<metadata.namespace>:<metadata.name> Step 3 - Deploy your NAIS application with Maskinporten provisioning enabled \u00a7 See configuration . Step 4 - Delete your application from the IaC repository \u00a7 Verify that everything works after the migration Delete the application from the IaC repository in order to maintain a single source of truth. Permanently deleting a client \u00a7 Warning Permanent deletes are irreversible. Only do this if you are certain that you wish to completely remove the client from DigDir and deactivates exposed scopes and granted access for consumers wil be removed. When a MaskinportenClient resource gets deleted from a Kubernetes cluster, the client will not be deleted from DigDir. Info The Application resource owns the MaskinportenClient resource, deletion of the former will thus trigger a deletion of the latter. If the MaskinportenClient resource is recreated, the client will thus retain the same client ID. If you want to completely delete the client from DigDir, you must add the following annotation to the MaskinportenClient resource: kubectl annotate maskinportenclient <app> digdir.nais.io/delete = true When this annotation is in place, deleting the MaskinportenClient resource from Kubernetes will trigger removal of the client and inactivating of registered scopes from DigDir.","title":"Client"},{"location":"security/auth/maskinporten/client/#client","text":"The NAIS platform provides support for simple declarative provisioning of a client that your application may use to integrate with Maskinporten, and in turn consume services and APIs served by external agencies. The client allows your application to leverage Maskinporten for authentication and authorization when performing service-to-service requests to external agencies. To achieve this, your application must: implement JWT grants request tokens from the /token -endpoint with the above JWT grants When a client requests a token from Maskinporten: Maskinporten validates the validity of the JWT and its signature ( Runtime JWK Secret used to sign the JWT). If the client has access to the requested list of scopes , an access_token will be returned to the client. The token can be used for authentication to the intended external service. Danger Make sure that the relevant service providers have pre-registered NAV 's organization (number: 889640782 ) as a valid consumer of any scopes that you define. Provisioning of client will fail otherwise. NAV\u00b4s pre-registered scopes can be found with proper access rights in Digdir selvbetjening .","title":"Client"},{"location":"security/auth/maskinporten/client/#getting-started","text":"","title":"Getting Started"},{"location":"security/auth/maskinporten/client/#access-policies","text":"Maskinporten is a third-party service outside of our clusters, which is not reachable by default like most third-party services.","title":"Access Policies"},{"location":"security/auth/maskinporten/client/#google-cloud-platform-gcp","text":"The following outbound external hosts are automatically added when enabling this feature: ver2.maskinporten.no in development maskinporten.no in production You do not need to specify these explicitly.","title":"Google Cloud Platform (GCP)"},{"location":"security/auth/maskinporten/client/#on-premises","text":"You must enable and use webproxy for external communication.","title":"On-premises"},{"location":"security/auth/maskinporten/client/#spec","text":"See the NAIS manifest .","title":"Spec"},{"location":"security/auth/maskinporten/client/#configuration","text":"nais.yaml spec : maskinporten : enabled : true scopes : consumes : - name : \"skatt:some.scope\" - name : \"nav:some/other/scope\" # required for on-premises only webproxy : true","title":"Configuration"},{"location":"security/auth/maskinporten/client/#usage","text":"","title":"Usage"},{"location":"security/auth/maskinporten/client/#runtime-variables-and-credentials","text":"The following environment variables and files (under the directory /var/run/secrets/nais.io/maskinporten ) are available at runtime:","title":"Runtime Variables and Credentials"},{"location":"security/auth/maskinporten/client/#maskinporten_client_id","text":"Note Maskinporten client ID. Unique ID for the application in Maskinporten. Example value: e89006c5-7193-4ca3-8e26-d0990d9d981f","title":"MASKINPORTEN_CLIENT_ID"},{"location":"security/auth/maskinporten/client/#maskinporten_scopes","text":"Note The scopes registered for the client at Maskinporten as a whitepace-separated string. See JWT grants for more information. Example value: nav:first/scope nav:another/scope","title":"MASKINPORTEN_SCOPES"},{"location":"security/auth/maskinporten/client/#maskinporten_well_known_url","text":"Note The well-known URL for the OIDC metadata discovery document for Maskinporten. Example value: https://ver2.maskinporten.no/.well-known/oauth-authorization-server","title":"MASKINPORTEN_WELL_KNOWN_URL"},{"location":"security/auth/maskinporten/client/#maskinporten_client_jwk","text":"Note Private JWK containing the private RSA key for creating signed JWTs when using the JWT grants . { \"use\" : \"sig\" , \"kty\" : \"RSA\" , \"kid\" : \"jXDxKRE6a4jogcc4HgkDq3uVgQ0\" , \"alg\" : \"RS256\" , \"n\" : \"xQ3chFsz...\" , \"e\" : \"AQAB\" , \"d\" : \"C0BVXQFQ...\" , \"p\" : \"9TGEF_Vk...\" , \"q\" : \"zb0yTkgqO...\" , \"dp\" : \"7YcKcCtJ...\" , \"dq\" : \"sXxLHp9A...\" , \"qi\" : \"QCW5VQjO...\" }","title":"MASKINPORTEN_CLIENT_JWK"},{"location":"security/auth/maskinporten/client/#consuming-an-api","text":"Refer to the documentation at DigDir . You may skip any step involving client registration as this is automatically handled when enabling this feature .","title":"Consuming an API"},{"location":"security/auth/maskinporten/client/#legacy","text":"Info This section only applies if you have an existing client registered at the IaC repository","title":"Legacy"},{"location":"security/auth/maskinporten/client/#migration-guide-to-keep-existing-maskinporten-client-nais-application-only","text":"The following describes the steps needed to migrate a client registered in IaC repository .","title":"Migration guide to keep existing Maskinporten client (NAIS application only)"},{"location":"security/auth/maskinporten/client/#step-1-update-your-client-description-in-the-iac-repository","text":"Ensure the description of the client registered in the IaC repository follows the naming scheme: <cluster>:<metadata.namespace>:<metadata.name>","title":"Step 1 - Update your client description in the IaC repository"},{"location":"security/auth/maskinporten/client/#step-3-deploy-your-nais-application-with-maskinporten-provisioning-enabled","text":"See configuration .","title":"Step 3 - Deploy your NAIS application with Maskinporten provisioning enabled"},{"location":"security/auth/maskinporten/client/#step-4-delete-your-application-from-the-iac-repository","text":"Verify that everything works after the migration Delete the application from the IaC repository in order to maintain a single source of truth.","title":"Step 4 - Delete your application from the IaC repository"},{"location":"security/auth/maskinporten/client/#permanently-deleting-a-client","text":"Warning Permanent deletes are irreversible. Only do this if you are certain that you wish to completely remove the client from DigDir and deactivates exposed scopes and granted access for consumers wil be removed. When a MaskinportenClient resource gets deleted from a Kubernetes cluster, the client will not be deleted from DigDir. Info The Application resource owns the MaskinportenClient resource, deletion of the former will thus trigger a deletion of the latter. If the MaskinportenClient resource is recreated, the client will thus retain the same client ID. If you want to completely delete the client from DigDir, you must add the following annotation to the MaskinportenClient resource: kubectl annotate maskinportenclient <app> digdir.nais.io/delete = true When this annotation is in place, deleting the MaskinportenClient resource from Kubernetes will trigger removal of the client and inactivating of registered scopes from DigDir.","title":"Permanently deleting a client"},{"location":"security/auth/maskinporten/scopes/","text":"Scopes \u00a7 A scope in Maskinporten terminology is equivalent to a distinct API. As an API provider, you will: define scopes to be registered in Maskinporten grant access to other organizations for your defined scopes The notion of scope is loosely defined to allow semantic freedom in terms of API providers' own definition and granularity of access and authorization. An external consumer that has been granted access to your scopes may then acquire an access_token with their own Maskinporten client, which they will need to acquire from DigDir on their own. Our NAIS clients are registered as part of the NAV organization and may only be used by NAV. Spec \u00a7 See the NAIS manifest . Configuration \u00a7 nais.yaml spec : maskinporten : enabled : true scopes : exposes : - name : \"some.scope.read\" enabled : true product : \"arbeid\" allowedIntegrations : - maskinporten atMaxAge : 120 consumers : - orgno : \"123456789\" # required for on-premises only webproxy : true Scope Naming Format \u00a7 All scopes within Maskinporten are defined using the following syntax: scope := <prefix>:<subscope> Example scope scope := nav:trygdeopplysninger Prefix \u00a7 The prefix for all scopes provisioned through NAIS will always be nav . Subscope \u00a7 A subscope should describe the resource to be exposed as accurately as possible (e.g. trygdeopplysninger or helseopplysninger ). The subscope may also be postfixed to separate between access levels, for instance read and/or write access (e.g. nav:trygdeopplysninger.write ). Absence of a postfix should generally be treated as strictly read access. All subscopes for NAIS clients will have the following form: subscope := <product><separator><name> where separator is: / if and only if name contains / . : otherwise. Subscope example For the example configuration above where product := arbeid name := some.scope.read the subscope will be the following: subscope := arbeid:some.scope.read which results in the scope: scope := nav:arbeid:some.scope.read Subscope example with different separator If the name instead contains the / character, e.g: name := some/scope.read and the product is the same as before: product := arbeid the resulting subscope will be: subscope := arbeid/some/scope.read which results in the scope: scope := nav:arbeid/some/scope.read Audience Restrictions \u00a7 If there are multiple APIs that are protected by the same scope , one might be susceptible to replay attacks. One way to mitigate this is to require that tokens contain an aud claim with a unique value for each unique API. The API should reject requests with tokens that do not have this claim and expected value. This ensures that an access_token may only be used for a specific API. The value of this must be defined by the API provider out-of-band from Maskinporten. It is thus the API provider's responsibility to inform any consumers of this expected value so that they can modify their requests accordingly. See DigDir's documentation on audience-restricted tokens for more information. Legacy \u00a7 Info This section only applies if you have an existing scope registered at the IaC repository or use the scope in on-prem clusters today. Application do not use the IaC repo and is migrating from on-prem to gcp \u00a7 The scope is assigned your application in your current cluster; <cluster>:<metadata.namespace>:<metadata.name>.<subscope> so when migrating from on-prem nais to gcp, the scope belongs to that cluster. If you already migrated your app, there is no reason to panic, scope still exists and works as before, but you are not able to make changes eg. add/remove consumers until cluster is updated. Right now the cluster need to be changed manually so please take contact in channel #nais. Migration guide to keep existing Maskinporten scope (IaC -> nais.yml) (NAIS application only) \u00a7 The following describes the steps needed to migrate a scope registered in IaC repository . Step 1 - Update your scope description in the IaC repository \u00a7 Ensure the description of the scope registered in the IaC repository follows the naming scheme: <cluster>:<metadata.namespace>:<metadata.name>.<subscope> Step 3 - Deploy your NAIS application with Maskinporten provisioning enabled. \u00a7 Ensure exposed scopes enabled and name matches the already exposed subscope See configuration . Step 4 - Delete your scope from the IaC repository \u00a7 Verify that everything works after the migration Delete the scope from the IaC repository in order to maintain a single source of truth.","title":"Scopes"},{"location":"security/auth/maskinporten/scopes/#scopes","text":"A scope in Maskinporten terminology is equivalent to a distinct API. As an API provider, you will: define scopes to be registered in Maskinporten grant access to other organizations for your defined scopes The notion of scope is loosely defined to allow semantic freedom in terms of API providers' own definition and granularity of access and authorization. An external consumer that has been granted access to your scopes may then acquire an access_token with their own Maskinporten client, which they will need to acquire from DigDir on their own. Our NAIS clients are registered as part of the NAV organization and may only be used by NAV.","title":"Scopes"},{"location":"security/auth/maskinporten/scopes/#spec","text":"See the NAIS manifest .","title":"Spec"},{"location":"security/auth/maskinporten/scopes/#configuration","text":"nais.yaml spec : maskinporten : enabled : true scopes : exposes : - name : \"some.scope.read\" enabled : true product : \"arbeid\" allowedIntegrations : - maskinporten atMaxAge : 120 consumers : - orgno : \"123456789\" # required for on-premises only webproxy : true","title":"Configuration"},{"location":"security/auth/maskinporten/scopes/#scope-naming-format","text":"All scopes within Maskinporten are defined using the following syntax: scope := <prefix>:<subscope> Example scope scope := nav:trygdeopplysninger","title":"Scope Naming Format"},{"location":"security/auth/maskinporten/scopes/#prefix","text":"The prefix for all scopes provisioned through NAIS will always be nav .","title":"Prefix"},{"location":"security/auth/maskinporten/scopes/#subscope","text":"A subscope should describe the resource to be exposed as accurately as possible (e.g. trygdeopplysninger or helseopplysninger ). The subscope may also be postfixed to separate between access levels, for instance read and/or write access (e.g. nav:trygdeopplysninger.write ). Absence of a postfix should generally be treated as strictly read access. All subscopes for NAIS clients will have the following form: subscope := <product><separator><name> where separator is: / if and only if name contains / . : otherwise. Subscope example For the example configuration above where product := arbeid name := some.scope.read the subscope will be the following: subscope := arbeid:some.scope.read which results in the scope: scope := nav:arbeid:some.scope.read Subscope example with different separator If the name instead contains the / character, e.g: name := some/scope.read and the product is the same as before: product := arbeid the resulting subscope will be: subscope := arbeid/some/scope.read which results in the scope: scope := nav:arbeid/some/scope.read","title":"Subscope"},{"location":"security/auth/maskinporten/scopes/#audience-restrictions","text":"If there are multiple APIs that are protected by the same scope , one might be susceptible to replay attacks. One way to mitigate this is to require that tokens contain an aud claim with a unique value for each unique API. The API should reject requests with tokens that do not have this claim and expected value. This ensures that an access_token may only be used for a specific API. The value of this must be defined by the API provider out-of-band from Maskinporten. It is thus the API provider's responsibility to inform any consumers of this expected value so that they can modify their requests accordingly. See DigDir's documentation on audience-restricted tokens for more information.","title":"Audience Restrictions"},{"location":"security/auth/maskinporten/scopes/#legacy","text":"Info This section only applies if you have an existing scope registered at the IaC repository or use the scope in on-prem clusters today.","title":"Legacy"},{"location":"security/auth/maskinporten/scopes/#application-do-not-use-the-iac-repo-and-is-migrating-from-on-prem-to-gcp","text":"The scope is assigned your application in your current cluster; <cluster>:<metadata.namespace>:<metadata.name>.<subscope> so when migrating from on-prem nais to gcp, the scope belongs to that cluster. If you already migrated your app, there is no reason to panic, scope still exists and works as before, but you are not able to make changes eg. add/remove consumers until cluster is updated. Right now the cluster need to be changed manually so please take contact in channel #nais.","title":"Application do not use the IaC repo and is migrating from on-prem to gcp"},{"location":"security/auth/maskinporten/scopes/#migration-guide-to-keep-existing-maskinporten-scope-iac-naisyml-nais-application-only","text":"The following describes the steps needed to migrate a scope registered in IaC repository .","title":"Migration guide to keep existing Maskinporten scope (IaC -&gt; nais.yml) (NAIS application only)"},{"location":"security/auth/maskinporten/scopes/#step-1-update-your-scope-description-in-the-iac-repository","text":"Ensure the description of the scope registered in the IaC repository follows the naming scheme: <cluster>:<metadata.namespace>:<metadata.name>.<subscope>","title":"Step 1 - Update your scope description in the IaC repository"},{"location":"security/auth/maskinporten/scopes/#step-3-deploy-your-nais-application-with-maskinporten-provisioning-enabled","text":"Ensure exposed scopes enabled and name matches the already exposed subscope See configuration .","title":"Step 3 - Deploy your NAIS application with Maskinporten provisioning enabled."},{"location":"security/auth/maskinporten/scopes/#step-4-delete-your-scope-from-the-iac-repository","text":"Verify that everything works after the migration Delete the scope from the IaC repository in order to maintain a single source of truth.","title":"Step 4 - Delete your scope from the IaC repository"},{"location":"security/secrets/","text":"Secrets \u00a7 NAIS provides multiple integrations for secrets management services: Native Kubernetes Secrets - available in all clusters in team namespaces Google Secrets Manager - only available in GCP Vault - only available on-premises See the respective pages for more information.","title":"Overview"},{"location":"security/secrets/#secrets","text":"NAIS provides multiple integrations for secrets management services: Native Kubernetes Secrets - available in all clusters in team namespaces Google Secrets Manager - only available in GCP Vault - only available on-premises See the respective pages for more information.","title":"Secrets"},{"location":"security/secrets/google-secrets-manager/","text":"Google Secrets Manager \u00a7 Warning Google Secrets Manager integration with Kubernetes is currently available as an OPEN BETA. Please report any issues to the #nais channel on Slack. You may store secrets in Google Secrets Manager as an alternative to the other offered solutions. As a supplement to Kubernetes Secrets , we also offer one-way synchronization of secrets from Google Secrets Manager to Kubernetes Secrets that you may mount into your applications in the GCP clusters. Getting started \u00a7 Tip See the examples for a complete illustration of the process. Step 1: Create a Secret in Google Secret Manager \u00a7 Check Start at the GCP Console page. Click on the Create Secret button. Secret Name Restrictions In order to synchronize the secret to Kubernetes, ensure that the secret name adheres to the following restrictions: Maximum length of 63 characters. May only contain letters, numbers and hyphens ( - ). Must be lowercase. Must start with a lowercase letter or number. Must end with a lowercase letter or number. All secrets must exist in the region europe-north1 . This option is found when you click manually manage locations for this secret . Unfortunately, we cannot enforce a default value here. Fill in the Secret value . Step 2: Import Secret to Kubernetes \u00a7 Only available in GCP Importing a secret to Kubernetes is only possible in the GCP clusters . One-way Synchronization The secret in Google Secret Manager is the single source of truth. Modifications to the secret in Kubernetes will NOT be synchronized back to Google Secret Manager. Any modifications done in Kubernetes will be overwritten by the actual secret found in Google Secret Manager. Check Enable synchronization \u00a7 Label your Secret with sync=true to enable synchronization to NAIS: Info Synchronization only occurs when new secret versions are created. If the secret already existed without this label, you must create a new secret version to effectuate the sync. The latest secret version in Google Secret Manager will be copied into your Kubernetes namespace as a Kubernetes Secret . Secret Naming \u00a7 The name of the secret in Kubernetes will match the name of the secret in Google Secret Manager. Naming collisions If a secret with the same name already exists in Kubernetes, the secret will not be imported. Step 2a: Secrets formatted as environment variables \u00a7 Check If your secret contains a list of environment variables: Then additionally add the label env=true to your secret in Google Secret Manager: This tells the synchronization mechanism to parse the secret as environment variables. Step 3: Using the Kubernetes Secret in your applications \u00a7 Check Now that the Secret exists in your Kubernetes namespace, your application may refer to and use it. There are two ways of mounting/exposing a Kubernetes Secret to your application: as files , or as environment variables Files \u00a7 spec : # secret will be available in the file named \"secret\" # in the directory /var/run/secrets/my-secret/ filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-secret Environment variables \u00a7 spec : # secret will be made available as environment variables envFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager Examples \u00a7 Example secret with single value format \u00a7 This method is generally used when you need a binary file as a secret mounted to the file system of your pod. If you need environment variables, see the other example . Secret in Google Secret Manager (click to expand) Imported Secret in Kubernetes (click to expand) apiVersion : v1 data : secret : c29tZS1zZWNyZXQtdmFsdWU= # key=\"secret\" # value=\"some-secret-value\", base64 encoded kind : Secret metadata : annotations : hunter2.nais.io/last-modified : \"2021-03-25T08:04:19Z\" hunter2.nais.io/last-modified-by : user@nav.no hunter2.nais.io/secret-version : \"1\" creationTimestamp : \"2021-03-25T08:04:25Z\" labels : nais.io/created-by : hunter2 name : my-google-secret namespace : my-team type : Opaque Secret values when mounted to application (click to expand) spec : # secret will be available in the file named \"secret\" in the directory /var/run/secrets/my-google-secret/ # e.g. /var/run/secrets/my-google-secret/secret # note that the name \"secret\" is hard-coded and not configurable; the key used in the example below _must_ be \"secret\". filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-google-secret Example with environment variable format \u00a7 Secret in Google Secret Manager (click to expand) Imported Secret in Kubernetes (click to expand) apiVersion : v1 data : SOME_ENV : c29tZS1zZWNyZXQtdmFsdWU= # key=\"SOME_ENV\" # value=\"some-secret-value\", base64 encoded SOME_OTHER_ENV : c29tZS1vdGhlci1zZWNyZXQ= # key=\"SOME_OTHER_ENV\" # value=\"some-other-secret\", base64 encoded kind : Secret metadata : annotations : hunter2.nais.io/last-modified : \"2021-03-25T08:24:58Z\" hunter2.nais.io/last-modified-by : user@nav.no hunter2.nais.io/secret-version : \"2\" creationTimestamp : \"2021-03-25T08:04:25Z\" labels : nais.io/created-by : hunter2 name : my-google-secret namespace : my-team type : Opaque Secret values when mounted to application (click to expand) spec : # secret will be available as files in the directory /var/run/secrets/my-google-secret/ # e.g. /var/run/secrets/my-google-secret/SOME_ENV and /var/run/secrets/my-google-secret/SOME_OTHER_ENV filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-google-secret # secret will be made available as environment variables # SOME_ENV=some-secret-value # SOME_OTHER_ENV=some-other-secret envFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager","title":"Google Secrets Manager"},{"location":"security/secrets/google-secrets-manager/#google-secrets-manager","text":"Warning Google Secrets Manager integration with Kubernetes is currently available as an OPEN BETA. Please report any issues to the #nais channel on Slack. You may store secrets in Google Secrets Manager as an alternative to the other offered solutions. As a supplement to Kubernetes Secrets , we also offer one-way synchronization of secrets from Google Secrets Manager to Kubernetes Secrets that you may mount into your applications in the GCP clusters.","title":"Google Secrets Manager"},{"location":"security/secrets/google-secrets-manager/#getting-started","text":"Tip See the examples for a complete illustration of the process.","title":"Getting started"},{"location":"security/secrets/google-secrets-manager/#step-1-create-a-secret-in-google-secret-manager","text":"Check Start at the GCP Console page. Click on the Create Secret button. Secret Name Restrictions In order to synchronize the secret to Kubernetes, ensure that the secret name adheres to the following restrictions: Maximum length of 63 characters. May only contain letters, numbers and hyphens ( - ). Must be lowercase. Must start with a lowercase letter or number. Must end with a lowercase letter or number. All secrets must exist in the region europe-north1 . This option is found when you click manually manage locations for this secret . Unfortunately, we cannot enforce a default value here. Fill in the Secret value .","title":"Step 1: Create a Secret in Google Secret Manager"},{"location":"security/secrets/google-secrets-manager/#step-2-import-secret-to-kubernetes","text":"Only available in GCP Importing a secret to Kubernetes is only possible in the GCP clusters . One-way Synchronization The secret in Google Secret Manager is the single source of truth. Modifications to the secret in Kubernetes will NOT be synchronized back to Google Secret Manager. Any modifications done in Kubernetes will be overwritten by the actual secret found in Google Secret Manager. Check","title":"Step 2: Import Secret to Kubernetes"},{"location":"security/secrets/google-secrets-manager/#enable-synchronization","text":"Label your Secret with sync=true to enable synchronization to NAIS: Info Synchronization only occurs when new secret versions are created. If the secret already existed without this label, you must create a new secret version to effectuate the sync. The latest secret version in Google Secret Manager will be copied into your Kubernetes namespace as a Kubernetes Secret .","title":"Enable synchronization"},{"location":"security/secrets/google-secrets-manager/#secret-naming","text":"The name of the secret in Kubernetes will match the name of the secret in Google Secret Manager. Naming collisions If a secret with the same name already exists in Kubernetes, the secret will not be imported.","title":"Secret Naming"},{"location":"security/secrets/google-secrets-manager/#step-2a-secrets-formatted-as-environment-variables","text":"Check If your secret contains a list of environment variables: Then additionally add the label env=true to your secret in Google Secret Manager: This tells the synchronization mechanism to parse the secret as environment variables.","title":"Step 2a: Secrets formatted as environment variables"},{"location":"security/secrets/google-secrets-manager/#step-3-using-the-kubernetes-secret-in-your-applications","text":"Check Now that the Secret exists in your Kubernetes namespace, your application may refer to and use it. There are two ways of mounting/exposing a Kubernetes Secret to your application: as files , or as environment variables","title":"Step 3: Using the Kubernetes Secret in your applications"},{"location":"security/secrets/google-secrets-manager/#files","text":"spec : # secret will be available in the file named \"secret\" # in the directory /var/run/secrets/my-secret/ filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-secret","title":"Files"},{"location":"security/secrets/google-secrets-manager/#environment-variables","text":"spec : # secret will be made available as environment variables envFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager","title":"Environment variables"},{"location":"security/secrets/google-secrets-manager/#examples","text":"","title":"Examples"},{"location":"security/secrets/google-secrets-manager/#example-secret-with-single-value-format","text":"This method is generally used when you need a binary file as a secret mounted to the file system of your pod. If you need environment variables, see the other example . Secret in Google Secret Manager (click to expand) Imported Secret in Kubernetes (click to expand) apiVersion : v1 data : secret : c29tZS1zZWNyZXQtdmFsdWU= # key=\"secret\" # value=\"some-secret-value\", base64 encoded kind : Secret metadata : annotations : hunter2.nais.io/last-modified : \"2021-03-25T08:04:19Z\" hunter2.nais.io/last-modified-by : user@nav.no hunter2.nais.io/secret-version : \"1\" creationTimestamp : \"2021-03-25T08:04:25Z\" labels : nais.io/created-by : hunter2 name : my-google-secret namespace : my-team type : Opaque Secret values when mounted to application (click to expand) spec : # secret will be available in the file named \"secret\" in the directory /var/run/secrets/my-google-secret/ # e.g. /var/run/secrets/my-google-secret/secret # note that the name \"secret\" is hard-coded and not configurable; the key used in the example below _must_ be \"secret\". filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-google-secret","title":"Example secret with single value format"},{"location":"security/secrets/google-secrets-manager/#example-with-environment-variable-format","text":"Secret in Google Secret Manager (click to expand) Imported Secret in Kubernetes (click to expand) apiVersion : v1 data : SOME_ENV : c29tZS1zZWNyZXQtdmFsdWU= # key=\"SOME_ENV\" # value=\"some-secret-value\", base64 encoded SOME_OTHER_ENV : c29tZS1vdGhlci1zZWNyZXQ= # key=\"SOME_OTHER_ENV\" # value=\"some-other-secret\", base64 encoded kind : Secret metadata : annotations : hunter2.nais.io/last-modified : \"2021-03-25T08:24:58Z\" hunter2.nais.io/last-modified-by : user@nav.no hunter2.nais.io/secret-version : \"2\" creationTimestamp : \"2021-03-25T08:04:25Z\" labels : nais.io/created-by : hunter2 name : my-google-secret namespace : my-team type : Opaque Secret values when mounted to application (click to expand) spec : # secret will be available as files in the directory /var/run/secrets/my-google-secret/ # e.g. /var/run/secrets/my-google-secret/SOME_ENV and /var/run/secrets/my-google-secret/SOME_OTHER_ENV filesFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager mountPath : /var/run/secrets/my-google-secret # secret will be made available as environment variables # SOME_ENV=some-secret-value # SOME_OTHER_ENV=some-other-secret envFrom : - secret : my-google-secret # value is the secret name in Google Secret Manager","title":"Example with environment variable format"},{"location":"security/secrets/kubernetes-secrets/","text":"Kubernetes Secrets \u00a7 Integration with Google Cloud Platform We also offer an optional integration with Google Secret Manager as a simplified supplement to using Kubernetes Secrets directly. When running an application in a team namespace, Kubernetes Secrets can be used directly. To get started using this, simply create the secrets . A secret can be either key-value pairs or files, and can be exposed to the application as environment variables or files. Microsoft has a good YouTube video that explains the concepts of Kubernetes secrets. Example \u00a7 Creating a secret $ kubectl create secret generic my-secret --from-literal = key1 = supersecret secret/my-secret created Info The kubectl plugin kubectl-modify-secret is recommended if you need to modify the secret contents after creation. Exposing my-secret as environment variables to the application by referring to it in nais.yaml spec : envFrom : - secret : my-secret That's it! When the application is running, the environment variable key1 will have the value supersecret . Alternatively, if the secret should have their contents mounted into the containers as files: spec : filesFrom : - secret : my-secret The secret is then exposed under the path specified by spec.filesFrom[].mountPath (default /var/run/secrets ). For this example it is available at /var/run/secrets/key1 . See the official Kubernetes documentation or by running kubectl create secret generic --help for more details on creating and managing your secrets.","title":"Kubernetes Secrets"},{"location":"security/secrets/kubernetes-secrets/#kubernetes-secrets","text":"Integration with Google Cloud Platform We also offer an optional integration with Google Secret Manager as a simplified supplement to using Kubernetes Secrets directly. When running an application in a team namespace, Kubernetes Secrets can be used directly. To get started using this, simply create the secrets . A secret can be either key-value pairs or files, and can be exposed to the application as environment variables or files. Microsoft has a good YouTube video that explains the concepts of Kubernetes secrets.","title":"Kubernetes Secrets"},{"location":"security/secrets/kubernetes-secrets/#example","text":"Creating a secret $ kubectl create secret generic my-secret --from-literal = key1 = supersecret secret/my-secret created Info The kubectl plugin kubectl-modify-secret is recommended if you need to modify the secret contents after creation. Exposing my-secret as environment variables to the application by referring to it in nais.yaml spec : envFrom : - secret : my-secret That's it! When the application is running, the environment variable key1 will have the value supersecret . Alternatively, if the secret should have their contents mounted into the containers as files: spec : filesFrom : - secret : my-secret The secret is then exposed under the path specified by spec.filesFrom[].mountPath (default /var/run/secrets ). For this example it is available at /var/run/secrets/key1 . See the official Kubernetes documentation or by running kubectl create secret generic --help for more details on creating and managing your secrets.","title":"Example"},{"location":"security/secrets/vault/","text":"Vault \u00a7 Vault is used for managing secrets on-premises. It is not available in GCP. Refer to the Vault documentation for more details.","title":"Vault"},{"location":"security/secrets/vault/#vault","text":"Vault is used for managing secrets on-premises. It is not available in GCP. Refer to the Vault documentation for more details.","title":"Vault"},{"location":"theme-dev/","text":"Accessible documentation project \u00a7 There is need for WCAG (Web Content Accessibility Guidelines) compilant documentation, but currently there is no well defined solution. At the same time, this documentation has been created using MkDocs, a static page generator. MkDocs uses themes that determines how the pages looks and behave. Therefore themes are also a way to alter how the page will be. However currently there is no theme that focuses on WCAG compilance. Therefore the goal of this project is to create such. Prototype of WCAG compliant Mkdocs theme is developed. It is based on exisitng MkDocs theme ( Cinder theme for MkDocs ) that has been modified. Documentation for this project \u00a7 Relevant documentation for this project exist under section \"theme-dev\" (this/current section). Beside this page, it does consist of some additional pages such as: User guide - contains relevant information for users of this project's prototype. Developer guide - contains relevant information regardless developing this projects's prototype. Sandbox - is testing area for this project. Note this documentation has nothing to do with the orginal nais documentation. Scope of this documentation is only the modification of the theme. Next to do in this project \u00a7 Here is a list that touches on topics that currently are missing, but are needed. Order of items in this list does reflect our order of execution, but one should note that some items may have higher piority than others. Note If you are about to further develop this theme, make sure to read the developer guide and make yourself familiar with our recomendations. Our developer guide current solution needs to be tested further - No proper testing beside simple checks such as Windows narrator, contrast checker and/or manuall testing with keyboard has been peformed. This is mostly due to difficulty/trickyness of proper WCAG testing. Therefore it would be nice to actually test it so one can make sure that current solution works as intended. text alternatives for non-text content - Currently not all non-text elementes does have text alternatives. Some images/figues does not have any alternative text specified, see Introduction to Nais . WCAG 1.1.1 - Non-text content specifies rule that non-text elements should have a text alternative. This either has to be done manualy by the content author or develop a solution that will specify these automatically. markdown of main content needs to be WCAG compliant - For the most part, the main content is WCAG compliant as it is rendered using standard html tags. However in case of custom elements such as admonitions, it is not given that these elements fullfill these guidelines. Note that modifying the way main content is rendered will require developing MkDocs plugins. mechanisms to make textual content accessible needs to be developed/implemented - WCAG 3.1 - Readable section focuses on text content where diverse sophisticated mechanisms needs to be implemented. Because these will require marking text sections on markdown level, creating new syntaxes in markup language and rendering them accordingly will probably be required. Therefore creating MkDocs plugins will be necessary. mechanism for localization, it should be possible to change/translate gui to other languages - WCAG 3.1.1 - Language of page section states that human language of a page needs to be defined. Currently this functionality is implemented through html lang tag, see user guide - HTML lang attribute . However if content language other than english is desired, no mechanism to translate/change text labels exist. Labels such as \"next page, search\" will stay in english even if content language is different. This is against WCAG and therefore function to change/translate labels is needed. More information about theme localization in MkDocs: Supporting theme Localization/Translation in MkDocs shortcuts functionality needs to comply WCAG or otherwise be removed - WCAG 2.1.4 - Character Key Shortcuts section specifies a rule that shortcut functionality needs to obey. Current implementation does not obey this rule which makes it against WCAG. Other solution is to remove shorcuts, but one has to make sure it is a better choice. headers links/permament links These links should be described (i.e aria label) as what they are/ what they do. Currently the narrator will just read them as link/the symbol (#, \u00a7, etc.). This however will require modifing/creating MkDocs plugin as they are embeded in main content. Learn more about what the symbol of permament links should be and use it. Different websites uses different ones such as; \"#\", \"\u00a7\", \"\u00b6\" which is not ideal. Maybe custom symbol should be created? some figures/images are not \"visible\" to everyone - Images of figures such as for example mind maps, tables or/and flow charts can not be explained equally good just by an alternative text. Therefore one should create such content to be interactive. Technicaly there is no such guideline, the closes one is WCAG 1.4.9 - images of text however we would imagine this can be a problem. Therefore one could research this topic further. + considerations from our recomentations section","title":"About"},{"location":"theme-dev/#accessible-documentation-project","text":"There is need for WCAG (Web Content Accessibility Guidelines) compilant documentation, but currently there is no well defined solution. At the same time, this documentation has been created using MkDocs, a static page generator. MkDocs uses themes that determines how the pages looks and behave. Therefore themes are also a way to alter how the page will be. However currently there is no theme that focuses on WCAG compilance. Therefore the goal of this project is to create such. Prototype of WCAG compliant Mkdocs theme is developed. It is based on exisitng MkDocs theme ( Cinder theme for MkDocs ) that has been modified.","title":"Accessible documentation project"},{"location":"theme-dev/#documentation-for-this-project","text":"Relevant documentation for this project exist under section \"theme-dev\" (this/current section). Beside this page, it does consist of some additional pages such as: User guide - contains relevant information for users of this project's prototype. Developer guide - contains relevant information regardless developing this projects's prototype. Sandbox - is testing area for this project. Note this documentation has nothing to do with the orginal nais documentation. Scope of this documentation is only the modification of the theme.","title":"Documentation for this project"},{"location":"theme-dev/#next-to-do-in-this-project","text":"Here is a list that touches on topics that currently are missing, but are needed. Order of items in this list does reflect our order of execution, but one should note that some items may have higher piority than others. Note If you are about to further develop this theme, make sure to read the developer guide and make yourself familiar with our recomendations. Our developer guide current solution needs to be tested further - No proper testing beside simple checks such as Windows narrator, contrast checker and/or manuall testing with keyboard has been peformed. This is mostly due to difficulty/trickyness of proper WCAG testing. Therefore it would be nice to actually test it so one can make sure that current solution works as intended. text alternatives for non-text content - Currently not all non-text elementes does have text alternatives. Some images/figues does not have any alternative text specified, see Introduction to Nais . WCAG 1.1.1 - Non-text content specifies rule that non-text elements should have a text alternative. This either has to be done manualy by the content author or develop a solution that will specify these automatically. markdown of main content needs to be WCAG compliant - For the most part, the main content is WCAG compliant as it is rendered using standard html tags. However in case of custom elements such as admonitions, it is not given that these elements fullfill these guidelines. Note that modifying the way main content is rendered will require developing MkDocs plugins. mechanisms to make textual content accessible needs to be developed/implemented - WCAG 3.1 - Readable section focuses on text content where diverse sophisticated mechanisms needs to be implemented. Because these will require marking text sections on markdown level, creating new syntaxes in markup language and rendering them accordingly will probably be required. Therefore creating MkDocs plugins will be necessary. mechanism for localization, it should be possible to change/translate gui to other languages - WCAG 3.1.1 - Language of page section states that human language of a page needs to be defined. Currently this functionality is implemented through html lang tag, see user guide - HTML lang attribute . However if content language other than english is desired, no mechanism to translate/change text labels exist. Labels such as \"next page, search\" will stay in english even if content language is different. This is against WCAG and therefore function to change/translate labels is needed. More information about theme localization in MkDocs: Supporting theme Localization/Translation in MkDocs shortcuts functionality needs to comply WCAG or otherwise be removed - WCAG 2.1.4 - Character Key Shortcuts section specifies a rule that shortcut functionality needs to obey. Current implementation does not obey this rule which makes it against WCAG. Other solution is to remove shorcuts, but one has to make sure it is a better choice. headers links/permament links These links should be described (i.e aria label) as what they are/ what they do. Currently the narrator will just read them as link/the symbol (#, \u00a7, etc.). This however will require modifing/creating MkDocs plugin as they are embeded in main content. Learn more about what the symbol of permament links should be and use it. Different websites uses different ones such as; \"#\", \"\u00a7\", \"\u00b6\" which is not ideal. Maybe custom symbol should be created? some figures/images are not \"visible\" to everyone - Images of figures such as for example mind maps, tables or/and flow charts can not be explained equally good just by an alternative text. Therefore one should create such content to be interactive. Technicaly there is no such guideline, the closes one is WCAG 1.4.9 - images of text however we would imagine this can be a problem. Therefore one could research this topic further. + considerations from our recomentations section","title":"Next to do in this project"},{"location":"theme-dev/dev-guide/","text":"Developer guide \u00a7 This page is everyone that happens to continue our work. It does contain information that we would like you to know. Read it carefully as it may save you some trouble. Source code \u00a7 Information about soucre code and source files. Css structure \u00a7 Css for this theme is divided into 3 \"levels\", basicly there are 3 different stylesheet where one can provide changes. Those are: cinder base/highlight bootstrap cinder.css - \"local\" changes that controlls cinder looks. So this is where i.e color of headers is set. base.css - this is where extension to bootstrap are made, or definisions of sets i.e alerts styles highlight.css - this is file that contains styles for hljs (code snippets syntax highlighter). In terms of level, this one is equal with base. bootstrap.css - bootstrap file that provides backbone. It has been this way since orginal cinder theme. You can alter it as you like, but we did not see the reason to do it. img directory \u00a7 There are some images of grid, they looks useless, but removing them causes MkDocs to fail. One could investigate on what is the deal with them. They are not acutally used by the theme itself. Local version notifier \u00a7 During our development sprint, we found need for indication of local version. This is because we used back to back local version (served by mkdocs serve command) and version hosted through github repo. To avoid confusion between these two (which were identical) we decided to mark one of them which in this case was the local version. What it does is basicly printing a message (in this case on navbar) only when the address of the page is a local address. It does print a message that consist of information \"This is local version\" and a number which is nothing else than UNIX epoch. The reason behind the number is to be able to distinguish between reloads and because it looks cooler \ud83d\ude0e. Another functionality is the ability for the message to be removed when clicked. This becomes helpful when the message is in the way and have to be deleted. This notifier however has been excluded from the final version source code to avoid unessecary code that could lead to problems. If this is something you thing is cool and want it back, here it is: It consist of two parts, one jinja if-statement and one javascript code. jinja if-statement: <!-- technically you can put this if-statement wherever you want. We recomend though to put this on navbar, if using bootstrap place it inside <header><nav>...</nav></header> in 'header.html' --> {% if '127.0.0.1' in page.canonical_url %} <!-- '127.0.0.1' defines the local address on which mkdocs is serving the local version, if it is different for you, then change it --> < div id = \"local-version-notice\" class = \"text-warning\" role = \"alert\" > message </ div > {% endif %} javascript: //put this in javascript where it will be executed automaticly, i.e jquery $(document).ready(function() {} const $local_version_notice = $ ( '#local-version-notice' ); $ ( '#local-version-notice' ). text ( \"This is local version nr. \" + Date . now ()); $local_version_notice . on ( \"click\" , function () { this . remove (); }); Known bugs \u00a7 Things that we found not working, but you can maybe fix headers defined as <summary> inside <details> when hidden will show up on table of content, but they are inaccessible meaning that page will not scroll to them when directed to by table of content or permalink tabbed sections are not supported by cinder theme, but are used by this documentation. However this is more incompatibility issure rather than bug and therefore it should not be considered as one. Recomendations \u00a7 Insight into currently problematic areas and what we recomend to do about them. Use of position sticky \u00a7 Generally, position sticky rule is tricky. It make it easier to make content stay while scrolling, but it also creates tricky behaviour. Basicly if content A is nested in content B and content A and B is longer than viewport, then in order to scroll through whole content A, you have to scroll through whole content B. This result in situation where the end of content A will be shown at the bottom of content B. We wouldn't say this behaviour is very accessibility friendly, but in this case this is the best solution we have. We cannot use postion fixed as this goes against bootstrap (you have to redo breakpoints). At the same time, we assume that having aside content scroll with the page will be more frustating than position sticky. Use of position sticky is dictated by use of bootstrap (or maybe you find a way). Therefore this is also reason why switching off of bootstrap should be considered. Bootstrap \u00a7 Bootstrap is usefull as styling of a webpage is faster, but it is also problematic. The main problem with bootstrap is that it is constraining. Creating new elements/layouts that are not native for bootstrap becomes nightmare as you effectively go against bootstrap and have to overrite its rules. At the same time, bootstrap does not have implementation of everything that one can imagine. Therefore you are forced to create them yourself, but you have limited controll on what you can do and can not. In this project, most problematic elements are side menu and layout breakpoints. Bootstrap does not have implementation of endless nested side menu that would be WCAG compilant. Grid system is also problematic as the column breakpoints are not optimal. They occur in positions that makes some elements too narrow or too wide. Limitations of bootstrap also impacts implemetation choices for some functions. This is mainly search functionality, it had to become a modal that is opened by a button instead of a search input with a dropdown where results are printed. Not optimal position sticky rule also had to be used instead of position fixed rule. We believe that in order for this project to become better, a migration from bootstrap to custom css is needed. This way one could create layout and style that is more suitable for WCAG. This however will be more time consuming compared to use of bootstrap.","title":"Developer guide"},{"location":"theme-dev/dev-guide/#developer-guide","text":"This page is everyone that happens to continue our work. It does contain information that we would like you to know. Read it carefully as it may save you some trouble.","title":"Developer guide"},{"location":"theme-dev/dev-guide/#source-code","text":"Information about soucre code and source files.","title":"Source code"},{"location":"theme-dev/dev-guide/#css-structure","text":"Css for this theme is divided into 3 \"levels\", basicly there are 3 different stylesheet where one can provide changes. Those are: cinder base/highlight bootstrap cinder.css - \"local\" changes that controlls cinder looks. So this is where i.e color of headers is set. base.css - this is where extension to bootstrap are made, or definisions of sets i.e alerts styles highlight.css - this is file that contains styles for hljs (code snippets syntax highlighter). In terms of level, this one is equal with base. bootstrap.css - bootstrap file that provides backbone. It has been this way since orginal cinder theme. You can alter it as you like, but we did not see the reason to do it.","title":"Css structure"},{"location":"theme-dev/dev-guide/#img-directory","text":"There are some images of grid, they looks useless, but removing them causes MkDocs to fail. One could investigate on what is the deal with them. They are not acutally used by the theme itself.","title":"img directory"},{"location":"theme-dev/dev-guide/#local-version-notifier","text":"During our development sprint, we found need for indication of local version. This is because we used back to back local version (served by mkdocs serve command) and version hosted through github repo. To avoid confusion between these two (which were identical) we decided to mark one of them which in this case was the local version. What it does is basicly printing a message (in this case on navbar) only when the address of the page is a local address. It does print a message that consist of information \"This is local version\" and a number which is nothing else than UNIX epoch. The reason behind the number is to be able to distinguish between reloads and because it looks cooler \ud83d\ude0e. Another functionality is the ability for the message to be removed when clicked. This becomes helpful when the message is in the way and have to be deleted. This notifier however has been excluded from the final version source code to avoid unessecary code that could lead to problems. If this is something you thing is cool and want it back, here it is: It consist of two parts, one jinja if-statement and one javascript code. jinja if-statement: <!-- technically you can put this if-statement wherever you want. We recomend though to put this on navbar, if using bootstrap place it inside <header><nav>...</nav></header> in 'header.html' --> {% if '127.0.0.1' in page.canonical_url %} <!-- '127.0.0.1' defines the local address on which mkdocs is serving the local version, if it is different for you, then change it --> < div id = \"local-version-notice\" class = \"text-warning\" role = \"alert\" > message </ div > {% endif %} javascript: //put this in javascript where it will be executed automaticly, i.e jquery $(document).ready(function() {} const $local_version_notice = $ ( '#local-version-notice' ); $ ( '#local-version-notice' ). text ( \"This is local version nr. \" + Date . now ()); $local_version_notice . on ( \"click\" , function () { this . remove (); });","title":"Local version notifier"},{"location":"theme-dev/dev-guide/#known-bugs","text":"Things that we found not working, but you can maybe fix headers defined as <summary> inside <details> when hidden will show up on table of content, but they are inaccessible meaning that page will not scroll to them when directed to by table of content or permalink tabbed sections are not supported by cinder theme, but are used by this documentation. However this is more incompatibility issure rather than bug and therefore it should not be considered as one.","title":"Known bugs"},{"location":"theme-dev/dev-guide/#recomendations","text":"Insight into currently problematic areas and what we recomend to do about them.","title":"Recomendations"},{"location":"theme-dev/dev-guide/#use-of-position-sticky","text":"Generally, position sticky rule is tricky. It make it easier to make content stay while scrolling, but it also creates tricky behaviour. Basicly if content A is nested in content B and content A and B is longer than viewport, then in order to scroll through whole content A, you have to scroll through whole content B. This result in situation where the end of content A will be shown at the bottom of content B. We wouldn't say this behaviour is very accessibility friendly, but in this case this is the best solution we have. We cannot use postion fixed as this goes against bootstrap (you have to redo breakpoints). At the same time, we assume that having aside content scroll with the page will be more frustating than position sticky. Use of position sticky is dictated by use of bootstrap (or maybe you find a way). Therefore this is also reason why switching off of bootstrap should be considered.","title":"Use of position sticky"},{"location":"theme-dev/dev-guide/#bootstrap","text":"Bootstrap is usefull as styling of a webpage is faster, but it is also problematic. The main problem with bootstrap is that it is constraining. Creating new elements/layouts that are not native for bootstrap becomes nightmare as you effectively go against bootstrap and have to overrite its rules. At the same time, bootstrap does not have implementation of everything that one can imagine. Therefore you are forced to create them yourself, but you have limited controll on what you can do and can not. In this project, most problematic elements are side menu and layout breakpoints. Bootstrap does not have implementation of endless nested side menu that would be WCAG compilant. Grid system is also problematic as the column breakpoints are not optimal. They occur in positions that makes some elements too narrow or too wide. Limitations of bootstrap also impacts implemetation choices for some functions. This is mainly search functionality, it had to become a modal that is opened by a button instead of a search input with a dropdown where results are printed. Not optimal position sticky rule also had to be used instead of position fixed rule. We believe that in order for this project to become better, a migration from bootstrap to custom css is needed. This way one could create layout and style that is more suitable for WCAG. This however will be more time consuming compared to use of bootstrap.","title":"Bootstrap"},{"location":"theme-dev/sandbox/","text":"Sandbox \u00a7 Here you can test stuff. Danger for example, this is a danger admonition","title":"Testing sandbox"},{"location":"theme-dev/sandbox/#sandbox","text":"Here you can test stuff. Danger for example, this is a danger admonition","title":"Sandbox"},{"location":"theme-dev/user-guide/","text":"User guide \u00a7 The theme you are currently seeing is modified version of Cinder theme for MkDocs. Because it is modified, the orginal documentation is not fully relevant. Therefore any changes that needs to be documented, will be published here. Warning Changes to this theme was not tested extensively, therefore there is possiblity that functionalites described here will not behave as expected. About plugin support \u00a7 Cinder theme does not support all plugins and so does this modified theme. Adding support for plugins is not scope of this project. Therefore one must be satisified with limited list of supported plugins. Favicon \u00a7 In order to use custom favicon use following mkdocs.yml file entry: theme: favicon: path/to/icon If no favicon is specified in config file, default icon located under cinder/img/favicon.ico will be used instead. It is also possible to substitute this file to achieve the same goal. Nav title section \u00a7 Nav title section is the section that shows logo and title of the page on header. There are few customization options regarding this seciton we have implemented. Logo \u00a7 In order to use custom logo on navigation bar use following mkdocs.yml file entry: theme: logo: path/to/logo If no logo is specified in config file, only site title will be shown. Site title \u00a7 This is text that will show up next to the logo. It is possible (and advised) to use custom title for this element. This is because this element should be descriptive and tell the user what the pages are about. Therefore it is possible to customize this text by site_title attribute in mkdocs config file. Specifing custom title: theme: site_title: Documentation for my precious project. If no site_title is specified in config file, site_name attribute will be used instead. Repo links \u00a7 Repo links has been changed, now link to repo is separate from edit links. Before, edit links has been overwritten by link to repo if they were specified. Now it is possible to have both, link to repo and edit link Repo link \u00a7 Links to repo will show up on navigation bar. Appropriate icon is choosen based on keyword in repo url. This means that if repo_url in mkdocs.yml file contains 'github' , then github icon will be shown beside. Currently only 'github' , 'bitbucket' and 'gitlab' keywords are supported. If repo_url does not contain any of these keywords, then no icon will be shown. Link text is determined by repo_name attribute in mkdocs.yml . If this attribute is not specified, then default keyword will be shown i.e 'GitHub'. Edit link \u00a7 Edit link will be shown on top of content and it is controlled by edit_uri in mkdocs.yml ( about edit_uri in MkDocs ). This link text is agnostic to repo provider, which means that it will stay the same regardless if edit link is refering to Github, Gitlab etc. HTML lang attribute \u00a7 It is possbile to specify the html lang attribute (that defnies the content language) in mkdocs.yml file. This is done by following entry: theme: locale: fr //set the language to french Note This functionality is implemented according to mkdocs guide. More info about mkdocs locales refer to its documentation. localization in MkDocs themes Position Sticky \u00a7 Aside sections uses position: sticky; css rule to keep them attached in place. This rule however is not fully supported on all browser, therefore they will most likely scroll with the page content (this has not been tested). Such solution is best compromise between functionality and clean code. hljs (syntax highlighter) \u00a7 \"hljs\" is syntax highlighter for code snippets and it is used by this theme. Color scheme \u00a7 There are dozen of different color schemes created for hljs that one can choose. On how to change color, refer to cinder's official documentation ( Cinder theme - Syntax Highlighting Color Scheme ). However, the default color scheme for this theme is a modified \"StackOverflow Light\" theme for hljs. The colors has been adjusted so the contrast ratio is above 7.1. This is in order to meet WCAG requirements related to contrast. Therefore one should note that choosing different color scheme may not compile with WCAG regulations. Language detection \u00a7 MkDocs allow to define language for code blocks using the standart makrdown syntax. This is done by writing given language after first 3 backticks, like so: ```java public static void... ``` Language definition for code snippets in MkDocs However this won't work for hljs as language definition is done via class name in <code> tags. Language definition in hljs In result hljs will still use auto detection even if language is defined in markdown. Therefore it is advised not to specify language in markdown as there is no use for it. Removed functions \u00a7 There are functions that exist in cinder, but has been removed in this theme. Page meta-data \u00a7 MkDocs supports meta-data for markdown files. You can read more about it on their official documentation. page-metadata in MkDocs . Cinder theme uses this feature to add support for printing out page sources. This feature however has been removed in this theme due to its unpopularity. There is lack of information regardles on how this feature should be and therefore it has been removed. Scroll-spy & table of content \u00a7 Cinder does inherent MkDocs functionality of scroll spy and table of content. This allows to indicate current element in viewport on table of content. However, this functionality has been removed as the nature of this functionality makes it difficult to render correct. (if there is more table of content items in viewport, then it is difficut to determite which one is the \"active\" one). Therefore to avoid confusion, it has been removed. Keyboard Shortcuts \u00a7 As the default MkDocs, cinder theme have support for user defined keyboard shortcuts. Information about these shortcuts can be found on cinder theme documentation keyboard shortcuts in cinder . This functionality has not been adapted to comply with WCAG. Therefore it is advised not to use this functionality if WCAG compliance is a priority.","title":"User guide"},{"location":"theme-dev/user-guide/#user-guide","text":"The theme you are currently seeing is modified version of Cinder theme for MkDocs. Because it is modified, the orginal documentation is not fully relevant. Therefore any changes that needs to be documented, will be published here. Warning Changes to this theme was not tested extensively, therefore there is possiblity that functionalites described here will not behave as expected.","title":"User guide"},{"location":"theme-dev/user-guide/#about-plugin-support","text":"Cinder theme does not support all plugins and so does this modified theme. Adding support for plugins is not scope of this project. Therefore one must be satisified with limited list of supported plugins.","title":"About plugin support"},{"location":"theme-dev/user-guide/#favicon","text":"In order to use custom favicon use following mkdocs.yml file entry: theme: favicon: path/to/icon If no favicon is specified in config file, default icon located under cinder/img/favicon.ico will be used instead. It is also possible to substitute this file to achieve the same goal.","title":"Favicon"},{"location":"theme-dev/user-guide/#nav-title-section","text":"Nav title section is the section that shows logo and title of the page on header. There are few customization options regarding this seciton we have implemented.","title":"Nav title section"},{"location":"theme-dev/user-guide/#logo","text":"In order to use custom logo on navigation bar use following mkdocs.yml file entry: theme: logo: path/to/logo If no logo is specified in config file, only site title will be shown.","title":"Logo"},{"location":"theme-dev/user-guide/#site-title","text":"This is text that will show up next to the logo. It is possible (and advised) to use custom title for this element. This is because this element should be descriptive and tell the user what the pages are about. Therefore it is possible to customize this text by site_title attribute in mkdocs config file. Specifing custom title: theme: site_title: Documentation for my precious project. If no site_title is specified in config file, site_name attribute will be used instead.","title":"Site title"},{"location":"theme-dev/user-guide/#repo-links","text":"Repo links has been changed, now link to repo is separate from edit links. Before, edit links has been overwritten by link to repo if they were specified. Now it is possible to have both, link to repo and edit link","title":"Repo links"},{"location":"theme-dev/user-guide/#repo-link","text":"Links to repo will show up on navigation bar. Appropriate icon is choosen based on keyword in repo url. This means that if repo_url in mkdocs.yml file contains 'github' , then github icon will be shown beside. Currently only 'github' , 'bitbucket' and 'gitlab' keywords are supported. If repo_url does not contain any of these keywords, then no icon will be shown. Link text is determined by repo_name attribute in mkdocs.yml . If this attribute is not specified, then default keyword will be shown i.e 'GitHub'.","title":"Repo link"},{"location":"theme-dev/user-guide/#edit-link","text":"Edit link will be shown on top of content and it is controlled by edit_uri in mkdocs.yml ( about edit_uri in MkDocs ). This link text is agnostic to repo provider, which means that it will stay the same regardless if edit link is refering to Github, Gitlab etc.","title":"Edit link"},{"location":"theme-dev/user-guide/#html-lang-attribute","text":"It is possbile to specify the html lang attribute (that defnies the content language) in mkdocs.yml file. This is done by following entry: theme: locale: fr //set the language to french Note This functionality is implemented according to mkdocs guide. More info about mkdocs locales refer to its documentation. localization in MkDocs themes","title":"HTML lang attribute"},{"location":"theme-dev/user-guide/#position-sticky","text":"Aside sections uses position: sticky; css rule to keep them attached in place. This rule however is not fully supported on all browser, therefore they will most likely scroll with the page content (this has not been tested). Such solution is best compromise between functionality and clean code.","title":"Position Sticky"},{"location":"theme-dev/user-guide/#hljs-syntax-highlighter","text":"\"hljs\" is syntax highlighter for code snippets and it is used by this theme.","title":"hljs (syntax highlighter)"},{"location":"theme-dev/user-guide/#color-scheme","text":"There are dozen of different color schemes created for hljs that one can choose. On how to change color, refer to cinder's official documentation ( Cinder theme - Syntax Highlighting Color Scheme ). However, the default color scheme for this theme is a modified \"StackOverflow Light\" theme for hljs. The colors has been adjusted so the contrast ratio is above 7.1. This is in order to meet WCAG requirements related to contrast. Therefore one should note that choosing different color scheme may not compile with WCAG regulations.","title":"Color scheme"},{"location":"theme-dev/user-guide/#language-detection","text":"MkDocs allow to define language for code blocks using the standart makrdown syntax. This is done by writing given language after first 3 backticks, like so: ```java public static void... ``` Language definition for code snippets in MkDocs However this won't work for hljs as language definition is done via class name in <code> tags. Language definition in hljs In result hljs will still use auto detection even if language is defined in markdown. Therefore it is advised not to specify language in markdown as there is no use for it.","title":"Language detection"},{"location":"theme-dev/user-guide/#removed-functions","text":"There are functions that exist in cinder, but has been removed in this theme.","title":"Removed functions"},{"location":"theme-dev/user-guide/#page-meta-data","text":"MkDocs supports meta-data for markdown files. You can read more about it on their official documentation. page-metadata in MkDocs . Cinder theme uses this feature to add support for printing out page sources. This feature however has been removed in this theme due to its unpopularity. There is lack of information regardles on how this feature should be and therefore it has been removed.","title":"Page meta-data"},{"location":"theme-dev/user-guide/#scroll-spy-table-of-content","text":"Cinder does inherent MkDocs functionality of scroll spy and table of content. This allows to indicate current element in viewport on table of content. However, this functionality has been removed as the nature of this functionality makes it difficult to render correct. (if there is more table of content items in viewport, then it is difficut to determite which one is the \"active\" one). Therefore to avoid confusion, it has been removed.","title":"Scroll-spy &amp; table of content"},{"location":"theme-dev/user-guide/#keyboard-shortcuts","text":"As the default MkDocs, cinder theme have support for user defined keyboard shortcuts. Information about these shortcuts can be found on cinder theme documentation keyboard shortcuts in cinder . This functionality has not been adapted to comply with WCAG. Therefore it is advised not to use this functionality if WCAG compliance is a priority.","title":"Keyboard Shortcuts"}]}